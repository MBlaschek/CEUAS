{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "solar-amazon",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Check if lat/lon change significantly in the files, causing the non-true-orphans problem \"\"\"\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "#import dash  # (version 1.12.0) pip install dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import os,sys\n",
    "import json\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "import geopandas as gpd\n",
    "    \n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "continuous-foster",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lat_lon(d):\n",
    "    \n",
    "    odb_dir = '/raid60/scratch/leo/scratch/era5/odbs/'\n",
    "    \n",
    "    \n",
    "    qs='select distinct lat, lon'  #distinct,count(date) \n",
    "\n",
    "    \n",
    "    res = { 'single' : { 'lat':[], 'lon':[], 'file':[] } ,   \n",
    "            'multi'  : { 'lat':[], 'lon':[], 'file':[] } }\n",
    "    \n",
    "\n",
    "    datasets = { '1759' : 'era5.1759.conv.*' ,\n",
    "                 '1761' : 'era5.1761.conv.*' ,\n",
    "                 '1'    : 'era5.conv._*'     ,\n",
    "                 '2'    : 'era5.conv._*'     ,\n",
    "                 '3188' : 'era5.3188.conv.'   }\n",
    "    \n",
    "    flist=glob.glob(odb_dir+'/'+ d + '/' + datasets[d])[:20]\n",
    "\n",
    "\n",
    "    tot = len(flist)\n",
    "    for fn in flist:\n",
    "        #fn = odb_dir + '/' + f\n",
    "        print(fn)\n",
    "        \n",
    "        try:\n",
    "            rdata=subprocess.check_output([\"odb\",\"sql\",\"-q\",qs,\"-i\",fn,'--no_alignment'],stderr=subprocess.DEVNULL)\n",
    "            data = rdata.decode('utf-8').split('\\n')\n",
    "            #print(rdata)\n",
    "            #print(data)\n",
    "            #print(data[1:-1])\n",
    "\n",
    "            chunk = data[1:-1]\n",
    "\n",
    "            #print(chunk)\n",
    "\n",
    "            if  len(chunk) ==1:\n",
    "                res['single']['lat'].append(float(chunk[0].split('\\t')[0]))\n",
    "                res['single']['lon'].append(float(chunk[0].split('\\t')[1]))\n",
    "                res['single']['file'].append(fn)\n",
    "            else:\n",
    "\n",
    "                lat1, lat2 = float(chunk[0].split('\\t')[0]) , float(chunk[-1].split('\\t')[0])\n",
    "                lon1, lon2 = float(chunk[0].split('\\t')[1]) , float(chunk[-1].split('\\t')[1])\n",
    "\n",
    "                if (abs(lat1-lat2) < 0.5 and abs(lon1-lon2) < 0.5):\n",
    "                        res['single']['lat'].append(lat1)\n",
    "                        res['single']['lon'].append(lon1)\n",
    "                        res['single']['file'].append(fn)\n",
    "\n",
    "                else:\n",
    "                        res['multi']['lat'].append([lat1,lat2])\n",
    "                        res['multi']['lon'].append([lon1,lon2])\n",
    "                        res['multi']['file'].append(fn)\n",
    "                        out = open(d+'_changing_coords_lat1lat2_lon1lon2-ALLFILES.dat', 'a')\n",
    "                        lat1,lat2,lon1,lon2 = str(lat1), str(lat2), str(lon1), str(lon2)\n",
    "                        out.write(fn + '\\t' + lat1 + '\\t' + lat2 + '\\t' + lon1 + '\\t' + lon2 + '\\n' )\n",
    "        except:\n",
    "            print(\"*** Fail: \", fn )\n",
    "            pass\n",
    "    return res, tot\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "final-printer",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_fn(fn):\n",
    "        res = { 'single' : { 'lat':[], 'lon':[], 'file':[] } ,   \n",
    "                'multi'  : { 'lat':[], 'lon':[], 'file':[] } }\n",
    "        qs='select distinct lat, lon'  #distinct,count(date) \n",
    "\n",
    "        try:\n",
    "            rdata=subprocess.check_output([\"odb\",\"sql\",\"-q\",qs,\"-i\",fn,'--no_alignment'],stderr=subprocess.DEVNULL)\n",
    "            data = rdata.decode('utf-8').split('\\n')\n",
    "            #print(rdata)\n",
    "            #print(data)\n",
    "            #print(data[1:-1])\n",
    "\n",
    "            chunk = data[1:-1]\n",
    "\n",
    "            #print(chunk)\n",
    "\n",
    "            if  len(chunk) ==1:\n",
    "                res['single']['lat'].append(float(chunk[0].split('\\t')[0]))\n",
    "                res['single']['lon'].append(float(chunk[0].split('\\t')[1]))\n",
    "                res['single']['file'].append(fn)\n",
    "            else:\n",
    "\n",
    "                lat1, lat2 = float(chunk[0].split('\\t')[0]) , float(chunk[-1].split('\\t')[0])\n",
    "                lon1, lon2 = float(chunk[0].split('\\t')[1]) , float(chunk[-1].split('\\t')[1])\n",
    "\n",
    "                if (abs(lat1-lat2) < 0.5 and abs(lon1-lon2) < 0.5):\n",
    "                        res['single']['lat'].append(lat1)\n",
    "                        res['single']['lon'].append(lon1)\n",
    "                        res['single']['file'].append(fn)\n",
    "\n",
    "                else:\n",
    "                        res['multi']['lat'].append([lat1,lat2])\n",
    "                        res['multi']['lon'].append([lon1,lon2])\n",
    "                        res['multi']['file'].append(fn)\n",
    "                        out = open(d+'_changing_coords_lat1lat2_lon1lon2.dat', 'a')\n",
    "                        lat1,lat2,lon1,lon2 = str(lat1), str(lat2), str(lon1), str(lon2)\n",
    "                        out.write(fn + '\\t' + lat1 + '\\t' + lat2 + '\\t' + lon1 + '\\t' + lon2 + '\\n' )\n",
    "            print('*** DONE ' , fn )\n",
    "        except:\n",
    "            print(\"*** Fail: \", fn )\n",
    "            pass\n",
    "        return res \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "israeli-physiology",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot(dataset, res):\n",
    "    \n",
    "    tot = len(res['single']['lat'])\n",
    "    if 'lat' in res['multi'].keys():\n",
    "        tot = tot + len(res['multi']['lat'])\n",
    "        \n",
    "    \"\"\" Getting the WMO regions json file \"\"\"\n",
    "    WMO_json = 'WMO_regions.json'\n",
    "    if not os.path.isfile(WMO_json):\n",
    "        os.system( 'wget https://cpdb.wmo.int/js/json/WMO_regions.json --no-check-certificate ')\n",
    "\n",
    "    WMO =  gpd.read_file('WMO_regions.json')\n",
    "\n",
    "\n",
    "    fs = 13\n",
    "    plt.xlim([-180.,180.])\n",
    "    plt.ylim([-90.,90.])\n",
    "        #clb=f.colorbar(c,ax=ax,orientation='horizontal')                                                                                                                                                                                                                                     \n",
    "        #clb.ax.set_title('{:6.4f}'.format(np.mean(ds[k]).values/43200)+' '+units)                                                                                                                                                                                                            \n",
    "\n",
    "    \"\"\" Loading from geopandas built-in methods \"\"\"\n",
    "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    world = world.plot(figsize=(10,12))\n",
    "    WMO.plot( ax=world,  facecolor=\"none\", edgecolor=\"lightgray\", lw = 0.5)\n",
    "\n",
    "\n",
    "    plotto = plt.scatter( res['single']['lon'], res['single']['lat'] ,  \n",
    "                         s = 3, color = 'red', \n",
    "                         label = 'Correct lat/lon '+ str(len(res['single']['lon'])) )\n",
    "    #plotto = plt.scatter( res['single']['lon'], res['single']['lat'] ,  s = 0.7, color = 'red', label = 'Orphans ' )\n",
    "\n",
    "    if 'lon' in res['multi'].keys(): # I have at least one \n",
    "        for lon,lat,num in zip(res['multi']['lon'],res['multi']['lat'], range(len(res['multi']['lat']))):\n",
    "            if num == 1:\n",
    "                plt.plot( [lon[0],lon[1]], [lat[0],lat[1]], \n",
    "                         color = 'lime', \n",
    "                         label = 'Changing lat/lon ' + str(len(res['multi']['lon'])))\n",
    "                #plt.plot( [lon[0],lon[1]], [lat[0],lat[1]], color = 'blue' , label = 'Changing coords ' )\n",
    "\n",
    "                plt.scatter( [lon[0],lon[1]], [lat[0],lat[1]], \n",
    "                            color = 'blue' ,s = 40, )\n",
    "\n",
    "            else:\n",
    "                plt.plot( [lon[0],lon[1]], [lat[0],lat[1]], color = 'lime')\n",
    "                plt.scatter( [lon[0],lon[1]], [lat[0],lat[1]], \n",
    "                            color = 'blue', s = 40,)\n",
    "\n",
    "    #cbar = plt.colorbar(fraction=0.03, pad=0.03) # pad moves bar to left-right, fractions is the length of the bar\n",
    "    #cbar.set_label('Number of Records ')\n",
    "\n",
    "    if dataset == 'rda':\n",
    "        ds = 'NCAR'\n",
    "    elif dataset == 'ai_bfr':\n",
    "        ds = 'BUFR'\n",
    "    else:\n",
    "        ds = 'ERA5 ' + dataset \n",
    "    plt.title (str(tot) + ' stations of the dataset ' + ds , fontsize = fs)\n",
    "\n",
    "    plt.legend(fontsize = 12, loc = 'lower left')\n",
    "    \n",
    "    #plt.legend(loc = 'lower left', fontsize = 6 ,ncol = 2)\n",
    "    plt.savefig('Plots/all_stations_map_' + dataset + '.png', dpi= 250,   bbox_inches = 'tight' )\n",
    "    plt.show()\n",
    "        #plt.close()\n",
    "    print('Done +++' , d )\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "announced-council",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for d in ['1','2','1759','1761','3188']:\n",
    "#    a = get_orphan(d)\n",
    "\n",
    "#for d in ['1','2','1759','1761','3188']:\n",
    "#for d in ['1','2','1759','1761','3188']:\n",
    "#for d in ['1759','1761','2']:\n",
    "#        res, tot = get_orphan(d)\n",
    "        \n",
    "#res, tot = get_lat_lon_multi('2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-notice",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50  *** Files \n",
      "*** Fail:  /raid60/scratch/leo/scratch/era5/odbs//1759/era5.1759.conv.1909*** Fail: \n",
      " /raid60/scratch/leo/scratch/era5/odbs//1759/era5.1759.conv.1903\n",
      "*** Fail: *** Fail: *** Fail: *** Fail: *** Fail: *** Fail: *** Fail: *** Fail:     *** Fail:  *** Fail:  /raid60/scratch/leo/scratch/era5/odbs//1759/era5.1759.conv.1918  *** Fail: /raid60/scratch/leo/scratch/era5/odbs//1759/era5.1759.conv.1917/raid60/scratch/leo/scratch/era5/odbs//1759/era5.1759.conv.1914*** Fail: /raid60/scratch/leo/scratch/era5/odbs//1759/era5.1759.conv.1911*** Fail: /raid60/scratch/leo/scratch/era5/odbs//1759/era5.1759.conv.1908\n",
      "/raid60/scratch/leo/scratch/era5/odbs//1759/era5.1759.conv.1905 *** Fail:  /raid60/scratch/leo/scratch/era5/odbs//1759/era5.1759.conv.1907\n",
      "/raid60/scratch/leo/scratch/era5/odbs//1759/era5.1759.conv.1912\n",
      " \n",
      " \n",
      "*** DONE \n",
      " /raid60/scratch/leo/scratch/era5/odbs//1759/era5.1759.conv.1906 /raid60/scratch/leo/scratch/era5/odbs//1759/era5.1759.conv.1900\n",
      "*** Fail: \n",
      "/raid60/scratch/leo/scratch/era5/odbs//1759/era5.1759.conv.1915/raid60/scratch/leo/scratch/era5/odbs//1759/era5.1759.conv.1904*** Fail: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "for d in ['1759','1761','2', '1', '3188']:\n",
    "        res, tot = get_lat_lon(d)\n",
    "        \n",
    "        print('single: ', len(res['single']['file']) , '\\n')\n",
    "        print('multi: ', len(res['multi']['file']) , '\\n')\n",
    "        print(' *** tot:' , tot,  ' *** ')\n",
    "        plot(d, res, tot)\n",
    "\"\"\"\n",
    "\n",
    "# looping over datasets \n",
    "odb_dir = '/raid60/scratch/leo/scratch/era5/odbs/'\n",
    "\n",
    "datasets = { '1759' : 'era5.1759.conv.*' ,\n",
    "             '1761' : 'era5.1761.conv.*' ,\n",
    "             '1'    : 'era5.conv._*'     ,\n",
    "             '2'    : 'era5.conv._*'     ,\n",
    "             '3188' : 'era5.3188.conv.'   }\n",
    "\n",
    "for d in ['1759','1761','3188']:\n",
    "\n",
    "    flist = glob.glob(odb_dir+'/'+ d + '/' + datasets[d])[:50]\n",
    "    flist = [f for f in flist if '.gz' not in f and '.nc' not in f and '0000' not in f and '9999' not in f ]\n",
    "    #print(flist)\n",
    "    print(len(flist) , ' *** Files ')\n",
    "    tot = len(flist)\n",
    "\n",
    "    p=Pool(20)\n",
    "    func=partial(run_fn)\n",
    "    all_res=list(p.map(func,flist))\n",
    "\n",
    "    RES = {}\n",
    "    for k in ['single', 'multi']:\n",
    "        #print(all_res)\n",
    "        res = [d[k] for d in all_res]\n",
    "        dic = {}\n",
    "        for kk in res[0].keys():\n",
    "            try:\n",
    "                dic[kk] = tuple(r[kk][0] for r in res )\n",
    "            except:\n",
    "                print('fail', fn)\n",
    "                pass\n",
    "        RES[k] = dic\n",
    "\n",
    "    print(RES)\n",
    "\n",
    "    plot(d,RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-yemen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-engagement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-photography",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-investigation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-carolina",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-expert",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
