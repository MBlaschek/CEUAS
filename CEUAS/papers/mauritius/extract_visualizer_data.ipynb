{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6fb64e7-95d0-436f-af81-1c001a5a3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data from the Mauritius harvested files to compatible format for the visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca3f6b7-41f9-414b-b7b0-13b0b97418ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2513284/2179053797.py:10: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import h5py as h5\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7603f6a-1244-41e1-bc45-71a8bd3b1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inter_files = [data+'/'+f for f in os.listdir('/data') if 'intercomparison' in f ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd45a58-07fc-4b91-8f0c-fa66f39ff082",
   "metadata": {},
   "source": [
    "### document\n",
    "https://wmoomm.sharepoint.com/sites/wmocpdb/eve_activityarea/Forms/AllItems.aspx?id=%2Fsites%2Fwmocpdb%2Feve%5Factivityarea%2FInstruments%20and%20Methods%20of%20Observation%20Programme%20%28IMOP%29%5F67452102%2D7575%2De911%2Da98e%2D000d3a44bd9c%2FIntercomparisons%2FPast%20Intercomparisons%2FRSO%2DIC%2D2005%5FFinal%5FReport%2Epdf&parent=%2Fsites%2Fwmocpdb%2Feve%5Factivityarea%2FInstruments%20and%20Methods%20of%20Observation%20Programme%20%28IMOP%29%5F67452102%2D7575%2De911%2Da98e%2D000d3a44bd9c%2FIntercomparisons%2FPast%20Intercomparisons&p=true&ga=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baaffd12-98cf-4ab6-8b2a-91b72d8b37c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "digitized = '/scratch/das/federico/databases_service2/MAURITIUS/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bdb939a-57f9-4c55-839e-9cecb50b5ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Graw', 'Graw-GPS', 'MKII', 'Meisei', 'Modem', 'SRS', 'Sip',\n",
       "       'Vaisala', 'Vaisala-GPS'], dtype='<U11')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### create a single file for each sonde type\n",
    "\n",
    "# extracting the list of different sensor types\n",
    "sonde_types = np.unique( [ f.split('_')[1].split('.csv')[0] for f in os.listdir(digitized+'/temp/') if '.csv' in f  ])\n",
    "sonde_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43ba48af-7278-408a-bf72-570de07380e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating single files out of the single ascent files, per sensor \n",
    "\n",
    "# output directory for the processed files \n",
    "outdir = 'data_processed'\n",
    "if not os.path.isdir(outdir):\n",
    "    os.mkdir(outdir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986cd3f-f16f-4366-83a9-d3b912fbbc46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0c0ca86f-06f6-42e1-a991-a3940167334f",
   "metadata": {},
   "source": [
    "\n",
    "sonde_types = ['Vaisala','Graw-GPS']\n",
    "\n",
    "# looping over each distinct sensor, all the ascents\n",
    "for s in sonde_types:\n",
    "\n",
    "    ### temperature\n",
    "    print('+++ Extracting temperature data +++ ')\n",
    "    all_df_temp = []\n",
    "    files_temp = [ digitized + '/temp/' + f for f in os.listdir(digitized+'/temp/') if f.split('_')[1].split('.csv')[0]== s ]  \n",
    "    files_temp.sort()\n",
    "    for f in tqdm(files_temp):\n",
    "        df_temp = pd.read_csv(f , sep = ',' )\n",
    "        df_temp = df_temp[ [c for c in df_temp.columns if c in ['press','datetime','temp','hum']] ] \n",
    "        all_df_temp.append(df_temp)\n",
    "\n",
    "    df_sum_temp = pd.concat(all_df_temp)\n",
    "    df_sum_temp = df_sum_temp.round(decimals=2)\n",
    "    df_sum_temp['datetime']  = [v[:19] for v in df_sum_temp['datetime'] ]\n",
    "    df_sum_temp['datetime'] = pd.to_datetime(df_sum_temp.datetime, format=\"%d-%m-%Y %H:%M:%S\" ,  utc=True)\n",
    "    df_sum_temp['datetime'] = pd.to_datetime(df_sum_temp.datetime,  utc=True)\n",
    "    df_sum_temp = df_sum_temp.dropna(subset=['press'])\n",
    "    df_sum_temp['press'] = df_sum_temp.press.astype(int)\n",
    "    df_sum_temp['sensor'] = s \n",
    "    #df_sum_temp.to_csv( outdir + '/' + s + '_all_ascents_temp.csv' , sep ='\\t' )\n",
    "\n",
    "\n",
    "    # Extracting all time stamps\n",
    "    all_times = list( np.unique(df_sum_temp.datetime) ) + list( np.unique(df_sum_temp.datetime) ) \n",
    "\n",
    "    # Diving into launch files (all sensors included)\n",
    "\n",
    "    \n",
    "    for t in all_times:\n",
    "        print(t)\n",
    "        d = df_sum_temp.loc[df_sum_temp.datetime == t]\n",
    "        d.to_csv( outdir+'/single_ascent/' + str(t)[:19]+'_temp.csv' , sep='\\t')\n",
    "        #print(d.head() ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4391a1-cf36-43db-ab95-42c615c7f15e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463446b0-f9a7-461b-9ca8-7986695e22d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdf717b1-f7c8-471c-abfb-e72ad421643d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting temperature data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 161.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting humidity data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:00<00:00, 133.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting temperature data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 28.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting humidity data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:01<00:00, 19.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting temperature data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 35.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting humidity data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:01<00:00, 16.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting temperature data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:00<00:00, 33.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting humidity data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 17.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting temperature data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 29.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting humidity data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:01<00:00, 16.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting temperature data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:00<00:00, 34.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting humidity data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:01<00:00, 27.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting temperature data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 31.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting humidity data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:01<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting temperature data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:01<00:00, 32.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting humidity data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:03<00:00, 16.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting temperature data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:01<00:00, 39.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Extracting humidity data +++ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:03<00:00, 18.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating single files out of the single ascent files, per sensor \n",
    "\n",
    "# output directory for the processed files \n",
    "outdir = 'data_processed'\n",
    "if not os.path.isdir(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "if not os.path.isdir(outdir+'/single_ascent'):\n",
    "        os.mkdir(outdir+'/single_ascent')\n",
    "\n",
    "#sonde_types = ['Vaisala']\n",
    "\n",
    "# looping over each distinct sensor, all the ascents\n",
    "for s in sonde_types:\n",
    "\n",
    "    ### temperature\n",
    "    print('+++ Extracting temperature data +++ ')\n",
    "    all_df_temp = []\n",
    "    files_temp = [ digitized + '/temp/' + f for f in os.listdir(digitized+'/temp/') if f.split('_')[1].split('.csv')[0]== s ]  \n",
    "    files_temp.sort()\n",
    "    for f in tqdm(files_temp):\n",
    "        df_temp = pd.read_csv(f , sep = ',' )\n",
    "        df_temp = df_temp[ [c for c in df_temp.columns if c in ['press','datetime','temp','hum']] ]\n",
    "        all_df_temp.append(df_temp)\n",
    "\n",
    "    df_sum_temp = pd.concat(all_df_temp)\n",
    "    df_sum_temp = df_sum_temp.round(decimals=2)\n",
    "    df_sum_temp['datetime']  = [v[:19] for v in df_sum_temp['datetime'] ]\n",
    "    df_sum_temp['datetime'] = pd.to_datetime(df_sum_temp.datetime, format=\"%d-%m-%Y %H:%M:%S\" ,  utc=True)\n",
    "    df_sum_temp['datetime'] = pd.to_datetime(df_sum_temp.datetime,  utc=True)\n",
    "    df_sum_temp = df_sum_temp.dropna(subset=['press'])\n",
    "    df_sum_temp['press'] = df_sum_temp.press.astype(int)\n",
    "    df_sum_temp['sensor'] = s \n",
    "    df_sum_temp.to_csv( outdir + '/' + s + '_all_ascents_temp.csv' , sep ='\\t' )\n",
    "\n",
    "    ### humidity    \n",
    "    print('+++ Extracting humidity data +++ ')\n",
    "    all_df_hum = []\n",
    "    files_hum = [ digitized + '/hum/' + f for f in os.listdir(digitized+'/hum/') if f.split('_')[1].split('.csv')[0]== s ]\n",
    "    files_hum.sort()\n",
    "    for f in tqdm(files_hum):\n",
    "        df_hum = pd.read_csv(f , sep = ',')\n",
    "        df_hum = df_hum[ [c for c in df_hum.columns if c in ['press','datetime','temp','hum']] ]\n",
    "        all_df_hum.append(df_hum)\n",
    "\n",
    "    df_sum_hum = pd.concat(all_df_hum)\n",
    "    df_sum_hum = df_sum_hum.round(decimals=2)\n",
    "    df_sum_hum['datetime']  = [v[:19] for v in df_sum_hum['datetime'] ]\n",
    "    df_sum_hum['datetime'] = pd.to_datetime(df_sum_hum.datetime, format=\"%d-%m-%Y %H:%M:%S\" ,  utc=True)    \n",
    "    df_sum_hum = df_sum_hum.dropna(subset=['press'])\n",
    "    df_sum_hum['press'] = df_sum_hum.press.astype(int)\n",
    "    df_sum_hum['sensor'] = s\n",
    "    df_sum_hum.to_csv( outdir + '/' + s + '_all_ascents_hum.csv' , sep ='\\t' )\n",
    "\n",
    "\n",
    "    # Extracting all time stamps\n",
    "    all_times = list( np.unique(df_sum_temp.datetime) ) + list( np.unique(df_sum_temp.datetime) ) \n",
    "    \n",
    "    # merging temp and hum data in one single file per sonde \n",
    "    all_df = []\n",
    "    for dt in all_times:\n",
    "        hum = df_sum_hum.loc[df_sum_hum['datetime'] == dt ][['press','hum']]\n",
    "        temp = df_sum_temp.loc[df_sum_temp['datetime'] == dt ][['press','temp']]\n",
    "\n",
    "        mer= pd.merge(hum,temp, on='press', how='outer')\n",
    "\n",
    "        #mer = pd.concat( [ hum.join(temp, on='press'), temp.join(hum, on='press') ]  ).drop_duplicates(subset=['press'] )\n",
    "        #mer = mer[['press','hum','temp']]\n",
    "        mer['datetime'] = dt \n",
    "        all_df.append(mer)\n",
    "\n",
    "\n",
    "    df = pd.concat(all_df)\n",
    "    df = df.sort_values(by=['datetime','press']) \n",
    "    df['sensor'] = s\n",
    "\n",
    "    \n",
    "    df.to_csv( outdir + '/' + s + '_all_ascents_fulldata.csv' , sep ='\\t' )\n",
    "    red = df.iloc[::5, :]\n",
    "    red.to_csv( outdir + '/' + s + '_all_ascents_reduced_data.csv' , sep ='\\t' )\n",
    "\n",
    "    \n",
    "    # Extracting all date_times values \n",
    "    if s =='Vaisala':\n",
    "        dates,times = [],[] \n",
    "        for t in np.unique(df_sum_temp['datetime']):\n",
    "            times.append(t.time())\n",
    "            dates.append(t.date())\n",
    "            #print(t.date(), t.time())\n",
    "\n",
    "        df = pd.DataFrame.from_dict( {'date': dates , 'time': times } )\n",
    "        \n",
    "        df.to_csv('all_timestamps.csv' , sep = '\\t') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "218a906e-dbea-4dd7-8062-51af26c6abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_times = list( np.unique(df_sum_temp.datetime) ) + list( np.unique(df_sum_hum.datetime) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "805afd0b-d37a-4477-892a-815cde1cbf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2005-02-07 10:01:53+0000', tz='UTC'),\n",
       " Timestamp('2005-02-07 14:57:50+0000', tz='UTC')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_times[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06206dc2-eb0c-4867-8c2f-620ca6dbe806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-02-07 10:01:53+00:00\n",
      "2005-02-07 14:57:50+00:00\n",
      "2005-02-07 19:31:41+00:00\n",
      "2005-02-08 05:13:16+00:00\n",
      "2005-02-08 10:00:56+00:00\n",
      "2005-02-08 15:03:03+00:00\n",
      "2005-02-08 19:33:03+00:00\n",
      "2005-02-09 05:12:00+00:00\n",
      "2005-02-09 10:05:05+00:00\n",
      "2005-02-09 14:59:48+00:00\n",
      "2005-02-09 19:17:51+00:00\n",
      "2005-02-10 05:02:13+00:00\n",
      "2005-02-10 09:58:41+00:00\n",
      "2005-02-10 14:57:54+00:00\n",
      "2005-02-10 19:00:19+00:00\n",
      "2005-02-11 05:02:45+00:00\n",
      "2005-02-11 10:44:04+00:00\n",
      "2005-02-11 15:05:21+00:00\n",
      "2005-02-11 18:31:58+00:00\n",
      "2005-02-12 05:02:39+00:00\n",
      "2005-02-12 15:00:05+00:00\n",
      "2005-02-12 18:36:55+00:00\n",
      "2005-02-14 05:06:57+00:00\n",
      "2005-02-14 10:08:04+00:00\n",
      "2005-02-14 15:00:54+00:00\n",
      "2005-02-14 18:00:36+00:00\n",
      "2005-02-15 05:16:46+00:00\n",
      "2005-02-15 10:03:06+00:00\n",
      "2005-02-15 15:01:34+00:00\n",
      "2005-02-15 18:11:25+00:00\n",
      "2005-02-16 05:11:08+00:00\n",
      "2005-02-16 10:03:34+00:00\n",
      "2005-02-16 15:07:48+00:00\n",
      "2005-02-16 18:11:15+00:00\n",
      "2005-02-17 10:15:24+00:00\n",
      "2005-02-17 14:59:11+00:00\n",
      "2005-02-17 18:03:18+00:00\n",
      "2005-02-18 05:13:26+00:00\n",
      "2005-02-18 10:07:00+00:00\n",
      "2005-02-18 14:59:17+00:00\n",
      "2005-02-18 17:58:42+00:00\n",
      "2005-02-19 15:00:10+00:00\n",
      "2005-02-19 18:13:08+00:00\n",
      "2005-02-21 05:02:39+00:00\n",
      "2005-02-21 10:00:42+00:00\n",
      "2005-02-21 15:50:20+00:00\n",
      "2005-02-22 05:09:02+00:00\n",
      "2005-02-22 10:01:54+00:00\n",
      "2005-02-22 15:01:57+00:00\n",
      "2005-02-22 18:23:00+00:00\n",
      "2005-02-23 05:02:40+00:00\n",
      "2005-02-23 10:02:01+00:00\n",
      "2005-02-23 15:01:31+00:00\n",
      "2005-02-24 05:03:32+00:00\n",
      "2005-02-24 10:03:25+00:00\n",
      "2005-02-24 15:01:16+00:00\n",
      "2005-02-24 18:04:29+00:00\n",
      "2005-02-25 05:04:54+00:00\n",
      "2005-02-25 10:17:06+00:00\n",
      "2005-02-25 15:02:36+00:00\n",
      "2005-02-07 10:01:53+00:00\n",
      "2005-02-07 14:57:50+00:00\n",
      "2005-02-07 19:31:41+00:00\n",
      "2005-02-08 05:13:16+00:00\n",
      "2005-02-08 10:00:56+00:00\n",
      "2005-02-08 15:03:03+00:00\n",
      "2005-02-08 19:33:03+00:00\n",
      "2005-02-09 05:12:00+00:00\n",
      "2005-02-09 10:05:05+00:00\n",
      "2005-02-09 14:59:48+00:00\n",
      "2005-02-09 19:17:51+00:00\n",
      "2005-02-10 05:02:13+00:00\n",
      "2005-02-10 09:58:41+00:00\n",
      "2005-02-10 14:57:54+00:00\n",
      "2005-02-10 19:00:19+00:00\n",
      "2005-02-11 05:02:45+00:00\n",
      "2005-02-11 10:44:04+00:00\n",
      "2005-02-11 15:05:21+00:00\n",
      "2005-02-11 18:31:58+00:00\n",
      "2005-02-12 05:02:39+00:00\n",
      "2005-02-12 15:00:05+00:00\n",
      "2005-02-12 18:36:55+00:00\n",
      "2005-02-14 05:06:57+00:00\n",
      "2005-02-14 10:08:04+00:00\n",
      "2005-02-14 15:00:54+00:00\n",
      "2005-02-14 18:00:36+00:00\n",
      "2005-02-15 05:16:46+00:00\n",
      "2005-02-15 10:03:06+00:00\n",
      "2005-02-15 15:01:34+00:00\n",
      "2005-02-15 18:11:25+00:00\n",
      "2005-02-16 05:11:08+00:00\n",
      "2005-02-16 10:03:34+00:00\n",
      "2005-02-16 15:07:48+00:00\n",
      "2005-02-16 18:11:15+00:00\n",
      "2005-02-17 10:15:24+00:00\n",
      "2005-02-17 14:59:11+00:00\n",
      "2005-02-17 18:03:18+00:00\n",
      "2005-02-18 05:13:26+00:00\n",
      "2005-02-18 10:07:00+00:00\n",
      "2005-02-18 14:59:17+00:00\n",
      "2005-02-18 17:58:42+00:00\n",
      "2005-02-19 05:06:10+00:00\n",
      "2005-02-19 15:00:10+00:00\n",
      "2005-02-19 18:13:08+00:00\n",
      "2005-02-21 05:02:39+00:00\n",
      "2005-02-21 10:00:42+00:00\n",
      "2005-02-21 15:50:20+00:00\n",
      "2005-02-22 05:09:02+00:00\n",
      "2005-02-22 10:01:54+00:00\n",
      "2005-02-22 15:01:57+00:00\n",
      "2005-02-22 18:23:00+00:00\n",
      "2005-02-23 05:02:40+00:00\n",
      "2005-02-23 10:02:01+00:00\n",
      "2005-02-23 15:01:31+00:00\n",
      "2005-02-24 05:03:32+00:00\n",
      "2005-02-24 10:03:25+00:00\n",
      "2005-02-24 15:01:16+00:00\n",
      "2005-02-24 18:04:29+00:00\n",
      "2005-02-25 05:04:54+00:00\n",
      "2005-02-25 10:17:06+00:00\n",
      "2005-02-25 15:02:36+00:00\n"
     ]
    }
   ],
   "source": [
    "# Dividing into launch files (all sensors included)\n",
    "\n",
    "if not os.path.isdir(outdir+'/single_ascent/'):\n",
    "    os.mkdir(outdir+'/single_ascent/')\n",
    "    \n",
    "for t in all_times:\n",
    "    all_data = []\n",
    "    files = [outdir+'/'+f for f in os.listdir(outdir) if 'fulldata' in f  ]\n",
    "    print(t)\n",
    "    for fi in files:\n",
    "        df = pd.read_csv(fi, sep ='\\t' )\n",
    "        #print('original   ' , df.head() )\n",
    "        dff = df.loc[ df.datetime == str(t) ]\n",
    "        #print('datetime loc ' ,  dff.head() ) \n",
    "        all_data.append(dff)\n",
    "    DF = pd.concat(all_data)\n",
    "    #print( 'DF ' , DF.head() )\n",
    "    DF.to_csv( outdir+'/single_ascent/' + str(t)[:19] + '.csv' , sep='\\t')\n",
    "    df = DF.iloc[::5, :]\n",
    "    df.to_csv( outdir+'/single_ascent/' + str(t)[:19] + '_reduced.csv' , sep='\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f70cb2e1-597a-4305-a947-8b8fa86a8a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>press</th>\n",
       "      <th>temp</th>\n",
       "      <th>datetime</th>\n",
       "      <th>sensor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>431</td>\n",
       "      <td>300.48</td>\n",
       "      <td>2005-02-07 10:01:53+00:00</td>\n",
       "      <td>Vaisala-GPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>435</td>\n",
       "      <td>298.65</td>\n",
       "      <td>2005-02-07 10:01:53+00:00</td>\n",
       "      <td>Vaisala-GPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>439</td>\n",
       "      <td>300.80</td>\n",
       "      <td>2005-02-07 10:01:53+00:00</td>\n",
       "      <td>Vaisala-GPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>443</td>\n",
       "      <td>299.38</td>\n",
       "      <td>2005-02-07 10:01:53+00:00</td>\n",
       "      <td>Vaisala-GPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>448</td>\n",
       "      <td>298.38</td>\n",
       "      <td>2005-02-07 10:01:53+00:00</td>\n",
       "      <td>Vaisala-GPS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   press    temp                  datetime       sensor\n",
       "0  431    300.48 2005-02-07 10:01:53+00:00  Vaisala-GPS\n",
       "1  435    298.65 2005-02-07 10:01:53+00:00  Vaisala-GPS\n",
       "2  439    300.80 2005-02-07 10:01:53+00:00  Vaisala-GPS\n",
       "3  443    299.38 2005-02-07 10:01:53+00:00  Vaisala-GPS\n",
       "4  448    298.38 2005-02-07 10:01:53+00:00  Vaisala-GPS"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sum_temp)\n",
    "df_sum_temp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94b1451f-8812-46c2-b699-8544748d29e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum_temp.datetime\n",
    "\n",
    "all_times = list( np.unique(df_sum_temp.datetime) ) + list( np.unique(df_sum_hum.datetime) ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde1aae8-8221-43d0-bb8a-d0b2baa2bb87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9512ee-dfa3-4fac-a4af-c17522e85473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70b92d1d-a5ee-4811-ba88-d032afb69fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(df_sum_hum)\n",
    "#df_sum_hum.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a07e14a-2e92-45bc-be75-59ca372ad860",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m hum \u001b[38;5;241m=\u001b[39m df_sum_hum\u001b[38;5;241m.\u001b[39mloc[df_sum_hum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m dt ]\n\u001b[1;32m      4\u001b[0m temp \u001b[38;5;241m=\u001b[39m df_sum_temp\u001b[38;5;241m.\u001b[39mloc[df_sum_temp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m dt ]\n\u001b[0;32m----> 6\u001b[0m mer\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\u001b[43ma\u001b[49m,b, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpress\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m mer \u001b[38;5;241m=\u001b[39m mer[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpress\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhum\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      8\u001b[0m mer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dt \n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "all_df = []\n",
    "for dt in all_times:\n",
    "    hum = df_sum_hum.loc[df_sum_hum['datetime'] == dt ]\n",
    "    temp = df_sum_temp.loc[df_sum_temp['datetime'] == dt ]\n",
    "\n",
    "    mer= pd.merge(a,b, on='press', how='outer')\n",
    "    mer = mer[['press','hum','temp']]\n",
    "    mer['datetime'] = dt \n",
    "    all_df.append(mer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac4285-ebea-434d-94d5-8d5a179f133e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8fb197-f109-4222-af54-788f9ba4f414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca447940-c28a-437d-8c7d-a9f927490c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ae511-d375-4cab-b8cc-3e6aec305c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ec76f-3000-4cec-91dc-1fea778b2dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a04b6d-1c31-4313-8f01-4911321b0518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b781f9be-e7d7-425e-868d-e83899078d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c2581-bc21-4de4-9a29-02ab0ce62ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c3b0ac-7f46-4682-a331-196e9abce347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd88c5a0-d45a-45d3-bdd5-5e717423bf67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ebeae-4d8e-4878-a00f-8433a1489462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30d2f8-c883-47d7-a457-cb630a91626f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d75ba15-c3f0-40e7-bbf0-145534082ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd98388-14af-4d82-9a30-53abb19c839e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c69f0-d9c3-40b1-a544-676e7288af0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1247475-0114-49e5-a101-6599a17a72a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369cc5fa-729f-4f69-92bb-0f48e87a1668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380df72a-673a-4861-9e0a-5fdbf4d0452c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b45fc1-00b3-4440-9cb4-528da7bd43fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610688b-692b-47e2-8d1a-390083685046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a305ae25-47e8-49eb-9531-77a329e7a9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ccc6e-e639-4641-9f04-0e07998ff8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d1891-c651-4a16-a0d0-946ef7502c83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
