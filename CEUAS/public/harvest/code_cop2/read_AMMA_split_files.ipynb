{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eac690a-71bd-472f-8a09-b3a9a0c4ba40",
   "metadata": {},
   "source": [
    "# AMMA Campaign\n",
    "\n",
    "\n",
    "Utility to read and convert BUFR files from:\n",
    "\n",
    "\n",
    "https://confluence.ecmwf.int/display/TCBUF/Radiosonde+BUFR+templates\n",
    "\n",
    "https://confluence.ecmwf.int/display/TCBUF/Data+availability (see radiosondes)\n",
    "\n",
    "https://www.ncei.noaa.gov/data/ecmwf-global-upper-air-bufr/doc/\n",
    "\n",
    "\n",
    "See some BUFR instructions:\n",
    "https://confluence.ecmwf.int/display/ECC/BUFR+tools\n",
    "\n",
    "\n",
    "\n",
    "# AMMA\n",
    "file:///home/federico/Downloads/1520-0477-2008bams2436_1.pdf  -> check WMO ids for inventory\n",
    "\n",
    "http://www.amma-catch.org/sites/amma-catch.org/IMG/pdf/qjrms_ammafieldcampaigns_486.pdf \n",
    "\n",
    "https://journals.ametsoc.org/view/journals/bams/89/7/2008bams2436_1.xml\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6baee7-150b-423c-9e89-c629bf7c351b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b35c28c5-6860-488d-b05a-fc4abd539c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eccodes import * \n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c683285-4ac7-4601-a7c2-7d86ede1c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_AMMA(file):\n",
    "\n",
    "    f = open(file)\n",
    "    all_data = {}\n",
    "    \n",
    "    \n",
    "    \"\"\" Name of the columns as they will appear in the pandas dataframe (not necessarily CDM compliant) \"\"\"\n",
    "    #column_names = ['report_timestamp' , 'iday',  'station_id', 'latitude', 'longitude', 'pressure', 'value','varno@body']\n",
    "            \n",
    "    while 1:\n",
    "        \n",
    "        data = { \n",
    "        'statid': [] , \n",
    "        'latitude': [], \n",
    "        'longitude': [] , \n",
    "\n",
    "        'pressure': [],\n",
    "        'airTemperature': [] , \n",
    "        'nonCoordinateGeopotentialHeight' : [],\n",
    "                \n",
    "        'typicalDate': [], \n",
    "        'typicalTime': [],\n",
    "\n",
    "        'windSpeed': [],\n",
    "        'windDirection': [],\n",
    "                \n",
    "        \"airTemperature\": [],\n",
    "        \"dewpointTemperature\": [],\n",
    "        'nonCoordinateGeopotentialHeight':[],\n",
    "        'heightOfStation' : []\n",
    "                \n",
    "          }\n",
    "        \n",
    "        #lista = [] # temporary list\n",
    "        bufr = codes_bufr_new_from_file(f)\n",
    "   \n",
    "        if bufr is None:\n",
    "            break\n",
    "   \n",
    "        codes_set(bufr, 'unpack', 1) # eCcodes must expand all the descriptors and unpack the data section\n",
    "    \n",
    "        typicalDate = codes_get_array(bufr, \"typicalDate\")[0]\n",
    "        typicalTime = codes_get_array(bufr, \"typicalTime\")[0]   \n",
    "        \n",
    "        year, month, day =  typicalDate[0:4], typicalDate[4:6] , typicalDate[6:8]\n",
    "        hour, minutes = typicalTime[0:2] , typicalTime[2:4]\n",
    "        \n",
    "        # build station number \n",
    "        statid = \"00000   \"\n",
    "        try:\n",
    "            block = codes_get(bufr, \"blockNumber\")\n",
    "            stnum = codes_get(bufr, \"stationNumber\")\n",
    "            if (block > 0) and (block < 100):  # or block != CODES_MISSING_LONG\n",
    "                statid = str.format(\"%.2i%.3i   \" % (block, stnum))\n",
    "        except Exception:\n",
    "            statid = \"00000   \"\n",
    "        if statid == \"00000   \":\n",
    "            statid = statid[0:8]\n",
    "        statid = statid.replace(' ','')\n",
    "       \n",
    "        #if '64400' not in statid:\n",
    "        #    continue \n",
    "            \n",
    "            \n",
    "        idate =  datetime.strptime(year + month + day + hour + minutes, '%Y%m%d%H%M')\n",
    "        iday = int(year + month + day )\n",
    "\n",
    "        try:\n",
    "            pressure          = codes_get_array(bufr, \"pressure\") \n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            airTemperature    = codes_get_array(bufr, \"airTemperature\")  \n",
    "        except:\n",
    "            airTemperature = np.empty(len(pressure))\n",
    "            \n",
    "        windDirection = codes_get_array(bufr, \"windDirection\")\n",
    "        windSpeed     = codes_get_array(bufr, \"windSpeed\")\n",
    "        heightOfStation = codes_get_array(bufr, \"heightOfStation\")\n",
    "        latitude = codes_get_array(bufr, \"latitude\")\n",
    "        longitude = codes_get_array(bufr, \"longitude\")\n",
    "\n",
    "        \n",
    "        try:  # not all the bufr files have the dewpoint \n",
    "            dewpointTemperature          = codes_get_array(bufr, \"dewpointTemperature\")\n",
    "        except:\n",
    "            dewpointTemperature= np.empty(len(airTemperature))\n",
    "            dewpointTemperature[:] = np.nan\n",
    "            \n",
    "        num_lev             = len(pressure) # number of  distinct pressure levels \n",
    "        \n",
    "        try:\n",
    "            geopotential   = codes_get_array(bufr, \"nonCoordinateGeopotentialHeight\")         \n",
    "        except:\n",
    "            geopotential = np.full( (1,len(airTemperature)) , np.nan )[0,:]\n",
    "                \n",
    "        \"\"\"\n",
    "        if report_id == 0:\n",
    "            ''' Check again but these values should remain the same for all cnt, so it makes no sense to read them every time '''\n",
    "            lat                     = codes_get(bufr, \"latitude\")\n",
    "            lon                    = codes_get(bufr, \"longitude\")\n",
    "            alt                     = float(codes_get(bufr, \"heightOfStation\"))\n",
    "            blockNumber    = codes_get(bufr, \"blockNumber\")\n",
    "            stationNumber = codes_get(bufr, \"stationNumber\")\n",
    "            #statid                = str(blockNumber*1000+stationNumber) # changed to int instead of str\n",
    "            statid                = blockNumber*1000+stationNumber\n",
    "            if statid not in     stations_id:\n",
    "                stations_id.append(statid) \n",
    "        \"\"\"\n",
    "        \n",
    "        codes_release(bufr)\n",
    "   \n",
    "        miss_value = -1.e100     \n",
    "        \n",
    "        #print(statid) \n",
    "        for i in range(len(pressure)):\n",
    "            data['statid'].append(statid)\n",
    "            \n",
    "            \n",
    "            data['heightOfStation'].append(heightOfStation[0])\n",
    "            data['nonCoordinateGeopotentialHeight'].append(geopotential[i])\n",
    "            data['latitude'].append(latitude[0])\n",
    "            data['longitude'].append(longitude[0])\n",
    "            data['pressure'].append(pressure[i])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            if airTemperature[i] >0.1:\n",
    "                data['airTemperature'].append(airTemperature[i])\n",
    "            else:\n",
    "                data['airTemperature'].append(np.nan)\n",
    "\n",
    "            if dewpointTemperature[i] > 0.1:\n",
    "                data['dewpointTemperature'].append(dewpointTemperature[i])\n",
    "            else:\n",
    "                data['dewpointTemperature'].append(np.nan)\n",
    " \n",
    "            data['windDirection'].append(windDirection[i])\n",
    "            data['windSpeed'].append(windSpeed[i])\n",
    "            data['typicalDate'].append(typicalDate)\n",
    "            data['typicalTime'].append(typicalTime)\n",
    "\n",
    "            \"\"\"\n",
    "            dp = dewpointTemperature[i]\n",
    "            if press == miss_value:\n",
    "                press = np.nan \n",
    "            if dp == miss_value:\n",
    "                dp = np.nan\n",
    "            if airT == miss_value :    # replacing none values with numpy nans\n",
    "                airT = np.nan \n",
    "            if winds == miss_value:\n",
    "                winds = np.nan\n",
    "            if gph == miss_value:\n",
    "                gph = np.nan                \n",
    "            if windd == 2147483647 or windd == -2147483647:\n",
    "                windd = np.nan \n",
    "            \"\"\" \n",
    "        \n",
    "        #for k in data.keys():\n",
    "        #    print(k, '  ' , len(data[k]) ) \n",
    "            \n",
    "        #print(data)\n",
    "        df = pd.DataFrame.from_dict(data)\n",
    "        df = df.fillna(np.nan)\n",
    "        df = df.replace(miss_value, '')\n",
    "\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        if statid not in all_data.keys():\n",
    "            all_data[statid] = []\n",
    "        \n",
    "        all_data[statid].append(df)\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    all_df = {}\n",
    "    for k in all_data.keys():\n",
    "        all_df[k] = pd.concat(all_data[k])\n",
    "        \n",
    "    return all_df "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4aac6ed4-cf1d-4a35-936b-2f8a8bac6c01",
   "metadata": {},
   "source": [
    "def read_AMMA(file):\n",
    "\n",
    "    f = open(file)\n",
    "    all_data = {}\n",
    "    \n",
    "    \n",
    "    \"\"\" Name of the columns as they will appear in the pandas dataframe (not necessarily CDM compliant) \"\"\"\n",
    "    #column_names = ['report_timestamp' , 'iday',  'station_id', 'latitude', 'longitude', 'pressure', 'value','varno@body']\n",
    "            \n",
    "    while 1:\n",
    "        \n",
    "        data = { \n",
    "        'statid': [] , \n",
    "        'latitude': [], \n",
    "        'longitude': [] , \n",
    "\n",
    "        'pressure': [],\n",
    "        'airTemperature': [] , \n",
    "        'nonCoordinateGeopotentialHeight' : [],\n",
    "                \n",
    "        'typicalDate': [], \n",
    "        'typicalTime': [],\n",
    "\n",
    "        'windSpeed': [],\n",
    "        'windDirection': [],\n",
    "                \n",
    "        \"airTemperature\": [],\n",
    "        \"dewpointTemperature\": [],\n",
    "        'nonCoordinateGeopotentialHeight':[],\n",
    "        'heightOfStation' : []\n",
    "                \n",
    "          }\n",
    "        \n",
    "        #lista = [] # temporary list\n",
    "        bufr = codes_bufr_new_from_file(f)\n",
    "   \n",
    "        if bufr is None:\n",
    "            break\n",
    "   \n",
    "        codes_set(bufr, 'unpack', 1) # eCcodes must expand all the descriptors and unpack the data section\n",
    "    \n",
    "        typicalDate = codes_get_array(bufr, \"typicalDate\")[0]\n",
    "        typicalTime = codes_get_array(bufr, \"typicalTime\")[0]   \n",
    "        \n",
    "        year, month, day =  typicalDate[0:4], typicalDate[4:6] , typicalDate[6:8]\n",
    "        hour, minutes = typicalTime[0:2] , typicalTime[2:4]\n",
    "        \n",
    "        # build station number \n",
    "        statid = \"00000   \"\n",
    "        try:\n",
    "            block = codes_get(bufr, \"blockNumber\")\n",
    "            stnum = codes_get(bufr, \"stationNumber\")\n",
    "            if (block > 0) and (block < 100):  # or block != CODES_MISSING_LONG\n",
    "                statid = str.format(\"%.2i%.3i   \" % (block, stnum))\n",
    "        except Exception:\n",
    "            statid = \"00000   \"\n",
    "        if statid == \"00000   \":\n",
    "            statid = statid[0:8]\n",
    "        statid = statid.replace(' ','')\n",
    "       \n",
    "        idate =  datetime.strptime(year + month + day + hour + minutes, '%Y%m%d%H%M')\n",
    "        iday = int(year + month + day )\n",
    "\n",
    "        try:\n",
    "            pressure          = codes_get_array(bufr, \"pressure\") \n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            airTemperature    = codes_get_array(bufr, \"airTemperature\")  \n",
    "        except:\n",
    "            airTemperature = np.empty(len(pressure))\n",
    "            \n",
    "        windDirection = codes_get_array(bufr, \"windDirection\")\n",
    "        windSpeed     = codes_get_array(bufr, \"windSpeed\")\n",
    "        heightOfStation = codes_get_array(bufr, \"heightOfStation\")\n",
    "        latitude = codes_get_array(bufr, \"latitude\")\n",
    "        longitude = codes_get_array(bufr, \"longitude\")\n",
    "\n",
    "        \n",
    "        try:  # not all the bufr files have the dewpoint \n",
    "            dewpointTemperature          = codes_get_array(bufr, \"dewpointTemperature\")\n",
    "        except:\n",
    "            dewpointTemperature= np.empty(len(airTemperature))\n",
    "            dewpointTemperature[:] = np.nan\n",
    "            \n",
    "        num_lev             = len(pressure) # number of  distinct pressure levels \n",
    "        \n",
    "        try:\n",
    "            geopotential   = codes_get_array(bufr, \"nonCoordinateGeopotentialHeight\")         \n",
    "        except:\n",
    "            geopotential = np.full( (1,len(airTemperature)) , np.nan )[0,:]\n",
    "                \n",
    "        \"\"\"\n",
    "        if report_id == 0:\n",
    "            ''' Check again but these values should remain the same for all cnt, so it makes no sense to read them every time '''\n",
    "            lat                     = codes_get(bufr, \"latitude\")\n",
    "            lon                    = codes_get(bufr, \"longitude\")\n",
    "            alt                     = float(codes_get(bufr, \"heightOfStation\"))\n",
    "            blockNumber    = codes_get(bufr, \"blockNumber\")\n",
    "            stationNumber = codes_get(bufr, \"stationNumber\")\n",
    "            #statid                = str(blockNumber*1000+stationNumber) # changed to int instead of str\n",
    "            statid                = blockNumber*1000+stationNumber\n",
    "            if statid not in     stations_id:\n",
    "                stations_id.append(statid) \n",
    "        \"\"\"\n",
    "        \n",
    "        codes_release(bufr)\n",
    "   \n",
    "        miss_value = -1.e100     \n",
    "        \n",
    "        #print(statid) \n",
    "        for i in range(len(pressure)):\n",
    "            data['statid'].append(statid)\n",
    "        \n",
    "            if press == miss_value:\n",
    "              continue\n",
    "            \n",
    "            \n",
    "            dp = dewpointTemperature[i]\n",
    "            if dp == miss_value:\n",
    "                dp = np.nan\n",
    "            data['dewpointTemperature'].append(dp)\n",
    "    \n",
    "\n",
    "            t = airTemperature[i]\n",
    "            if t == miss_value:\n",
    "                t = np.nan\n",
    "            data['dewpointTemperature'].append(y)\n",
    "            \n",
    "            \n",
    "            \n",
    "            if airT == miss_value :    # replacing none values with numpy nans\n",
    "                airT = np.nan \n",
    "            if winds == miss_value:\n",
    "                winds = np.nan\n",
    "            if gph == miss_value:\n",
    "                gph = np.nan                \n",
    "            if windd == 2147483647 or windd == -2147483647:\n",
    "                windd = np.nan \n",
    "                \n",
    "                \n",
    "                \n",
    "            data['heightOfStation'].append(heightOfStation[0])\n",
    "            data['nonCoordinateGeopotentialHeight'].append(geopotential[i])\n",
    "            data['latitude'].append(latitude[0])\n",
    "            data['longitude'].append(longitude[0])\n",
    "            data['pressure'].append(pressure[i])\n",
    "            data['windDirection'].append(windDirection[i])\n",
    "            data['windSpeed'].append(windSpeed[i])\n",
    "            data['airTemperature'].append(airTemperature[i])\n",
    "            \n",
    "            data['typicalDate'].append(typicalDate)\n",
    "            data['typicalTime'].append(typicalTime)\n",
    "\n",
    "            \n",
    "            dp = dewpointTemperature[i]\n",
    "            if press == miss_value:\n",
    "                press = np.nan \n",
    "            if dp == miss_value:\n",
    "                dp = np.nan\n",
    "            if airT == miss_value :    # replacing none values with numpy nans\n",
    "                airT = np.nan \n",
    "            if winds == miss_value:\n",
    "                winds = np.nan\n",
    "            if gph == miss_value:\n",
    "                gph = np.nan                \n",
    "            if windd == 2147483647 or windd == -2147483647:\n",
    "                windd = np.nan \n",
    "        \n",
    "        \n",
    "        #for k in data.keys():\n",
    "        #    print(k, '  ' , len(data[k]) ) \n",
    "            \n",
    "        df = pd.DataFrame.from_dict(data)\n",
    "        df = df.fillna(np.nan)\n",
    "\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        if statid not in all_data.keys():\n",
    "            all_data[statid] = []\n",
    "        \n",
    "        all_data[statid].append(df)\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    all_df = {}\n",
    "    for k in all_data.keys():\n",
    "        all_df[k] = pd.concat(all_data[k])\n",
    "        \n",
    "    return all_df               \n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98790a7-b693-458c-83fa-3ff3fe3ab719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "d1a7d1b6-2174-44e2-bdd4-617a57f1b59e",
   "metadata": {},
   "source": [
    "def read_k(file):\n",
    "\n",
    "    f = open(file)\n",
    "    all_data = {}\n",
    "    \n",
    "    \n",
    "    \"\"\" Name of the columns as they will appear in the pandas dataframe (not necessarily CDM compliant) \"\"\"\n",
    "    #column_names = ['report_timestamp' , 'iday',  'station_id', 'latitude', 'longitude', 'pressure', 'value','varno@body']\n",
    "            \n",
    "    while 1:\n",
    "        \n",
    "        data = { \n",
    "        'statid': [] , \n",
    "        'latitude': [], \n",
    "        'longitude': [] , \n",
    "\n",
    "        'pressure': [],\n",
    "        'airTemperature': [] , \n",
    "        'nonCoordinateGeopotentialHeight' : [],\n",
    "                \n",
    "        'typicalDate': [], \n",
    "        'typicalTime': [],\n",
    "\n",
    "        'windSpeed': [],\n",
    "        'windDirection': [],\n",
    "                \n",
    "        \"airTemperature\": [],\n",
    "        \"dewpointTemperature\": [],\n",
    "        'nonCoordinateGeopotentialHeight':[],\n",
    "        'heightOfStation' : []\n",
    "                \n",
    "          }\n",
    "        \n",
    "        #lista = [] # temporary list\n",
    "        bufr = codes_bufr_new_from_file(f)\n",
    "   \n",
    "        if bufr is None:\n",
    "            break\n",
    "   \n",
    "        codes_set(bufr, 'unpack', 1) # eCcodes must expand all the descriptors and unpack the data section\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac2f1e3-acbd-4265-90b3-6e119327c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = read_k('/scratch/das/federico/databases_service2/AMMA_BUFR/HRT2006041118')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5125ec31-6956-4cd2-be9f-d532d9de8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = read_AMMA('/scratch/das/federico/databases_service2/AMMA_BUFR/HRT2006041118')\n",
    "#d['61052']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59b1c34c-ba10-4bfb-81ce-b190411a1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d['61052'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a396a6c-3817-476f-8b94-78d56fb4aa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3566/3566 [05:06<00:00, 11.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# run in loop\n",
    "amma_dir = '/scratch/das/federico/databases_service2/AMMA_BUFR' \n",
    "\n",
    "files = [f for f in os.listdir(amma_dir)  if 'py' not in f  and 'txt' not in f ]\n",
    "\n",
    "#files = files[:10]\n",
    "\n",
    "all_data_stations = {}\n",
    "\n",
    "# first loop, extracts station data from single day file\n",
    "# store each station in a pandas, and in a dictionary\n",
    "for f in tqdm(files):\n",
    "    if 'AMMA_split_csv' in f:\n",
    "        continue \n",
    "    file = amma_dir + '/' + f \n",
    "    d = read_AMMA(file)\n",
    "    for stat in d.keys():\n",
    "        df = d[stat]\n",
    "        if stat not in all_data_stations:\n",
    "            all_data_stations[stat] = []\n",
    "            \n",
    "        all_data_stations[stat].append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4036fa98-68ab-4073-9fc8-6c05876ea988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['60360', '61223', '61291', '61442', '61641', '62721', '62730', '64458', '65330', '65503', '65510', '67027', '00000', '60390', '61052', '64700', '65418', '67002', '67005', '67095', '64750', '61090', '67781', '61024', '65125', '65344', '61687', '67774', '62760', '64400', '64650', '64910', '08589', '64500', '65578', '60680', '64450', '60018', '60571', '60630', '60656', '60715', '60760', '61901', '61902', '62378', '62403', '62423', '63450', '63894', '64870', '67083', '08594', '60155', '62306', '62337', '62414', '63741', '61415', '68032', '62641', '62650', '61831', '65202', '62010', '62640', '65387', '61226', '62600', '67881', '62840', '65046'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_stations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21dfcd71-8738-4a0a-a098-3690b66bc483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data_stations.keys() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f404b5e-2196-4187-be5a-07ddc587ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "915f5a28-2c77-4837-9731-1e5f3a75608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINING STATION :::  60360\n",
      "DONE STATION :::  60360\n",
      "COMBINING STATION :::  61223\n",
      "DONE STATION :::  61223\n",
      "COMBINING STATION :::  61291\n",
      "DONE STATION :::  61291\n",
      "COMBINING STATION :::  61442\n",
      "DONE STATION :::  61442\n",
      "COMBINING STATION :::  61641\n",
      "DONE STATION :::  61641\n",
      "COMBINING STATION :::  62721\n",
      "DONE STATION :::  62721\n",
      "COMBINING STATION :::  62730\n",
      "DONE STATION :::  62730\n",
      "COMBINING STATION :::  64458\n",
      "DONE STATION :::  64458\n",
      "COMBINING STATION :::  65330\n",
      "DONE STATION :::  65330\n",
      "COMBINING STATION :::  65503\n",
      "DONE STATION :::  65503\n",
      "COMBINING STATION :::  65510\n",
      "DONE STATION :::  65510\n",
      "COMBINING STATION :::  67027\n",
      "DONE STATION :::  67027\n",
      "COMBINING STATION :::  00000\n",
      "DONE STATION :::  00000\n",
      "COMBINING STATION :::  60390\n",
      "DONE STATION :::  60390\n",
      "COMBINING STATION :::  61052\n",
      "DONE STATION :::  61052\n",
      "COMBINING STATION :::  64700\n",
      "DONE STATION :::  64700\n",
      "COMBINING STATION :::  65418\n",
      "DONE STATION :::  65418\n",
      "COMBINING STATION :::  67002\n",
      "DONE STATION :::  67002\n",
      "COMBINING STATION :::  67005\n",
      "DONE STATION :::  67005\n",
      "COMBINING STATION :::  67095\n",
      "DONE STATION :::  67095\n",
      "COMBINING STATION :::  64750\n",
      "DONE STATION :::  64750\n",
      "COMBINING STATION :::  61090\n",
      "DONE STATION :::  61090\n",
      "COMBINING STATION :::  67781\n",
      "DONE STATION :::  67781\n",
      "COMBINING STATION :::  61024\n",
      "DONE STATION :::  61024\n",
      "COMBINING STATION :::  65125\n",
      "DONE STATION :::  65125\n",
      "COMBINING STATION :::  65344\n",
      "DONE STATION :::  65344\n",
      "COMBINING STATION :::  61687\n",
      "DONE STATION :::  61687\n",
      "COMBINING STATION :::  67774\n",
      "DONE STATION :::  67774\n",
      "COMBINING STATION :::  62760\n",
      "DONE STATION :::  62760\n",
      "COMBINING STATION :::  64400\n",
      "DONE STATION :::  64400\n",
      "COMBINING STATION :::  64650\n",
      "DONE STATION :::  64650\n",
      "COMBINING STATION :::  64910\n",
      "DONE STATION :::  64910\n",
      "COMBINING STATION :::  08589\n",
      "DONE STATION :::  08589\n",
      "COMBINING STATION :::  64500\n",
      "DONE STATION :::  64500\n",
      "COMBINING STATION :::  65578\n",
      "DONE STATION :::  65578\n",
      "COMBINING STATION :::  60680\n",
      "DONE STATION :::  60680\n",
      "COMBINING STATION :::  64450\n",
      "DONE STATION :::  64450\n",
      "COMBINING STATION :::  60018\n",
      "DONE STATION :::  60018\n",
      "COMBINING STATION :::  60571\n",
      "DONE STATION :::  60571\n",
      "COMBINING STATION :::  60630\n",
      "DONE STATION :::  60630\n",
      "COMBINING STATION :::  60656\n",
      "DONE STATION :::  60656\n",
      "COMBINING STATION :::  60715\n",
      "DONE STATION :::  60715\n",
      "COMBINING STATION :::  60760\n",
      "DONE STATION :::  60760\n",
      "COMBINING STATION :::  61901\n",
      "DONE STATION :::  61901\n",
      "COMBINING STATION :::  61902\n",
      "DONE STATION :::  61902\n",
      "COMBINING STATION :::  62378\n",
      "DONE STATION :::  62378\n",
      "COMBINING STATION :::  62403\n",
      "DONE STATION :::  62403\n",
      "COMBINING STATION :::  62423\n",
      "DONE STATION :::  62423\n",
      "COMBINING STATION :::  63450\n",
      "DONE STATION :::  63450\n",
      "COMBINING STATION :::  63894\n",
      "DONE STATION :::  63894\n",
      "COMBINING STATION :::  64870\n",
      "DONE STATION :::  64870\n",
      "COMBINING STATION :::  67083\n",
      "DONE STATION :::  67083\n",
      "COMBINING STATION :::  08594\n",
      "DONE STATION :::  08594\n",
      "COMBINING STATION :::  60155\n",
      "DONE STATION :::  60155\n",
      "COMBINING STATION :::  62306\n",
      "DONE STATION :::  62306\n",
      "COMBINING STATION :::  62337\n",
      "DONE STATION :::  62337\n",
      "COMBINING STATION :::  62414\n",
      "DONE STATION :::  62414\n",
      "COMBINING STATION :::  63741\n",
      "DONE STATION :::  63741\n",
      "COMBINING STATION :::  61415\n",
      "DONE STATION :::  61415\n",
      "COMBINING STATION :::  68032\n",
      "DONE STATION :::  68032\n",
      "COMBINING STATION :::  62641\n",
      "DONE STATION :::  62641\n",
      "COMBINING STATION :::  62650\n",
      "DONE STATION :::  62650\n",
      "COMBINING STATION :::  61831\n",
      "DONE STATION :::  61831\n",
      "COMBINING STATION :::  65202\n",
      "DONE STATION :::  65202\n",
      "COMBINING STATION :::  62010\n",
      "DONE STATION :::  62010\n",
      "COMBINING STATION :::  62640\n",
      "DONE STATION :::  62640\n",
      "COMBINING STATION :::  65387\n",
      "DONE STATION :::  65387\n",
      "COMBINING STATION :::  61226\n",
      "DONE STATION :::  61226\n",
      "COMBINING STATION :::  62600\n",
      "DONE STATION :::  62600\n",
      "COMBINING STATION :::  67881\n",
      "DONE STATION :::  67881\n",
      "COMBINING STATION :::  62840\n",
      "DONE STATION :::  62840\n",
      "COMBINING STATION :::  65046\n",
      "DONE STATION :::  65046\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# second loop, combine each station from single df into a single df in a time series\n",
    "out = '/scratch/das/federico/databases_service2/AMMA_BUFR/AMMA_split_csv_22FEB2024'\n",
    "if not os.path.isdir(out):\n",
    "    os.mkdir(out)\n",
    "    \n",
    "for station in all_data_stations.keys():       \n",
    "    #print(all_data_stations[station])\n",
    "    print('COMBINING STATION ::: ' , station )\n",
    "    df_combi = pd.concat(all_data_stations[station])\n",
    "    df_combi = df_combi.sort_values(by=['typicalDate', 'typicalTime', 'pressure'])\n",
    "    df_combi = df_combi.reset_index() \n",
    "    out_name = out + '/' + station + '_amma' + '.csv'\n",
    "    \n",
    "    df_combi.to_csv(out_name, sep = '\\t' , na_rep='') \n",
    "    \n",
    "    print('DONE STATION ::: ' , station )\n",
    "\n",
    "\n",
    "\n",
    "        #print(out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741006ae-5591-45d5-a3fb-b56faa0926b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd9f2dc-b4af-473e-87b0-481c7d20e05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd822625-7184-45b3-ade6-68962ca58fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be3325a-0a24-4c88-bfd3-92ba8e7ff204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3517c-7aef-4785-b81c-3376a464a234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7dee5e-e217-4f57-ae50-90669d340131",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de0c6a-7ece-40c9-951c-571196748c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d41fb-74ff-4562-acde-8598c8da14f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26b2fad0-8069-4088-9de7-3a2f1938f324",
   "metadata": {},
   "source": [
    "# Simple map plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f656d-3036-46f6-8b5f-125e8061fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_analysis():\n",
    "    \"\"\" Reads files from the directory and check which stations are there \"\"\"\n",
    "    # out -> defined above, output directory of the split csv files \n",
    "\n",
    "    stations, lats, lons = [], [], [] \n",
    "    for station in tqdm(os.listdir(out)):\n",
    "        file = data_dir + '/' + station \n",
    "        df = pd.read_csv( file, sep='\\t')\n",
    "        try:\n",
    "            lat, lon = df.latitude[0], df.longitude[0]\n",
    "        except:\n",
    "            pass\n",
    "            #print(file)\n",
    "        stations.append(station)\n",
    "        lats.append(lat)\n",
    "        lons.append(lon)\n",
    "        \n",
    "    \n",
    "    map_data = pd.DataFrame.from_dict( { 'station':stations, 'lat':lats , 'lon':lons} )\n",
    "    \n",
    "    try:\n",
    "        date = file.split('_')[1][0:6]\n",
    "    except:\n",
    "        print( file)\n",
    "        \n",
    "    return map_data, date \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca924d1-4e9b-44c8-8757-73672b34545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_data, date = quick_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c9fc4-1b8e-4d7b-b2c6-d11da32507f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "map = px.scatter_geo(map_data,\n",
    "                    lat=map_data.lat,\n",
    "                    lon=map_data.lon,\n",
    "                    hover_name=\"station\")\n",
    "\n",
    "map.update_layout(\n",
    "        height=1100, width=1500,\n",
    "        title= { 'text': 'AMMA Station' ,  \"yref\": \"paper\",\"y\": 0.9, \"yanchor\": \"bottom\" },\n",
    "\n",
    "        font=dict( family=\"Courier New, monospace\", size=20, #color=\"RebeccaPurple\"\n",
    "        ),\n",
    "        # margin=dict(l=20, r=20, t=0, b=5),\n",
    "        legend=dict(font=dict(family=\"Courier New, monospace\", size=25, color=\"black\"), title = \"Sensor Id\"),\n",
    "        legend_title=dict(font=dict(family=\"Courier New, monospace\", size=25, color=\"blue\"))\n",
    "    )\n",
    "    \n",
    "map.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a397076-ea1e-4283-8db1-5e11ae669fbb",
   "metadata": {},
   "source": [
    "# AMMA Metadata from https://journals.ametsoc.org/view/journals/bams/89/7/2008bams2436_1.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e3d57-532a-49ef-b98b-a896239c5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('/users/staff/federico/GitHub/CEUAS_master_JULY2922/CEUAS/CEUAS/meta/inventory_comparison_2/data/tables/AMMA_campaign_digitized_metadata.csv' , sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b490b5-ce9a-45e2-8696-8a4cad514365",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af7a72-c267-4ab1-a781-3760d931a152",
   "metadata": {},
   "source": [
    "# Extend the basic table with information from the files such as min_date, max_date, elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b62d5-b08b-4146-8dc4-09de7f3e5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation, min_date, max_date, files, lats, lons = [],[],[],[], [], [] \n",
    "\n",
    "stats = meta['WMO station No.'].values\n",
    "out = '/scratch/das/federico/databases_service2/AMMA_BUFR/AMMA_split_csv'\n",
    "\n",
    "\"\"\"\n",
    "Index(['index', 'statid', 'latitude', 'longitude', 'pressure',\n",
    "       'airTemperature', 'nonCoordinateGeopotentialHeight', 'typicalDate',\n",
    "       'typicalTime', 'windSpeed', 'windDirection', 'dewpointTemperature',\n",
    "       'heightOfStation'],\n",
    "      dtype='object')\n",
    "\"\"\"\n",
    "\n",
    "for s in stats:\n",
    "    s = str(s)\n",
    "    # out -> defined above, output directory of the split csv files \n",
    "\n",
    "    print(s)\n",
    "    \n",
    "    try:\n",
    "        file = [f for f in os.listdir(out) if s in f and '.csv' in f ][0]\n",
    "        df = pd.read_csv(out+'/'+file, sep='\\t')\n",
    "        el = df['heightOfStation'].values[0]\n",
    "        mind = min(df['typicalDate'].values) \n",
    "        maxd =  max(df['typicalDate'].values)\n",
    "        lat =  df['latitude'].values[0]\n",
    "        lon =  df['longitude'].values[0]\n",
    "        \n",
    "    except:\n",
    "        el, mind, maxd, file, lat, lon = np.nan, np.nan, np.nan, np.nan, np.nan, np.nan \n",
    "        \n",
    "    elevation.append(el)\n",
    "    min_date.append (mind)\n",
    "    max_date.append (maxd)\n",
    "    files.append(file)\n",
    "    lats.append(lat)\n",
    "    lons.append(lon)\n",
    "\n",
    "                     \n",
    "meta['elevation'] = elevation\n",
    "meta['min_date'] = min_date\n",
    "meta['max_date'] = max_date\n",
    "meta['conv_lat'] = lats\n",
    "meta['conv_lon'] = lons\n",
    "\n",
    "meta['files'] = files\n",
    "\n",
    "meta.to_csv('/users/staff/federico/GitHub/CEUAS_master_JULY2922/CEUAS/CEUAS/meta/inventory_comparison_2/data/tables/AMMA_campaign_digitized_metadata_extended.csv' , sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a4708-27e4-45c7-8134-79f5b8d6fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b45bc2-cf4d-442b-bbc5-7beb87439cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4615f8e8-9631-44f7-9b96-25c550108d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb033f7b-a517-4cd2-8062-be9d99c6d351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1d4db-7d13-444b-96a2-e2d54dfc138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "harv  = os.listdir('/scratch/das/federico/COP2_HARVEST_JAN2023/amma')\n",
    "\n",
    "harv = [f.split('_')[0] for f in harv if harv if '.csv' in f and '20999' not in f ]\n",
    "merged = [f.split('_')[0] for f in os.listdir('/scratch/das/federico/MERGED_FEB2023') if '.nc' in f ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c816d52c-80d0-4dd0-b7bc-80ed0819ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(harv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe656a-be02-40ef-bf84-1e0a70c374fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "721d5676-924e-454e-a004-e5bc626b6dd0",
   "metadata": {},
   "source": [
    "mdir = '/scratch/das/federico/MERGED_FEB2023'\n",
    "remove = [] \n",
    "for s in harv:\n",
    "    if s in merged:\n",
    "        me = [mdir + '/' + f for f in os.listdir( mdir  ) if s in f ][0]\n",
    "        remove.append(me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df4ccc4-237f-46da-beb5-e72d4a0874a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a08d909-0825-4e0c-9f10-8897fba70267",
   "metadata": {},
   "source": [
    "for f in remove:\n",
    "    os.system('rm ' + f )\n",
    "    print(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
