{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "import sys,glob\n",
    "import zipfile, os, time\n",
    "import urllib3\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import h5py\n",
    "import plotly.express as px\n",
    "sys.path.append(os.getcwd()+'/../cds-backend/code/')\n",
    "sys.path.append(os.getcwd()+'/../harvest/code/')\n",
    "from harvest_convert_to_netCDF_newfixes import write_dict_h5\n",
    "import cds_eua3 as eua\n",
    "eua.logging_set_level(30)\n",
    "import xarray as xr\n",
    "\n",
    "import cdsapi, zipfile, os, time\n",
    "#import schedule\n",
    "import copy\n",
    "from shutil import copyfile\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/raid60/scratch/leo/scratch/RI/Pangaea/nc/0-20200-0-01501.nc'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('/raid60/scratch/leo/scratch/RI/Pangaea/nc/0*.nc')\n",
    "# files = glob.glob('/raid60/scratch/uli/0*.nc')\n",
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_resort(fn):\n",
    "    targetfile = '/raid60/scratch/uli/resorted_files/'+fn.split('/')[-1]  \n",
    "    \n",
    "    with h5py.File(fn, 'r') as file:\n",
    "        with h5py.File(targetfile, 'w') as newfile:\n",
    "            groups = []\n",
    "            for i in file.keys():\n",
    "                if type(file[i]) == h5py._hl.group.Group:\n",
    "                    newfile.create_group(i)\n",
    "                    groups.append(i)\n",
    "                elif i == 'recordindex' or i == 'recordtimestamp':\n",
    "                    pass\n",
    "                else:\n",
    "                    newfile.create_dataset(i, data=file[i][:])\n",
    "            for i in groups:\n",
    "                if(i == 'recordindices' or i == 'observations_table' or i == 'era5fb'):\n",
    "                    pass\n",
    "                else:\n",
    "                    for j in file[i].keys():\n",
    "                        newfile[i].create_dataset(j, data=file[i][j][:])\n",
    "    \n",
    "    data =  eua.CDMDataset(fn)\n",
    "    allvars = data.observations_table.observed_variable[()]\n",
    "    allvars.sort()\n",
    "    allvars = numpy.unique(allvars)\n",
    "    #\n",
    "    ri = data.recordindex[()]\n",
    "#     print('recordindex: ', len(ri))\n",
    "    rt = data.recordtimestamp[()]\n",
    "    keys = data.observations_table.keys()[:-1]\n",
    "    fbkeys = data.era5fb.keys()[:-1]\n",
    "    # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "    pops = []\n",
    "    for i in range(len(keys)):\n",
    "        if 'string' in keys[i]:\n",
    "            pops.append(keys[i])\n",
    "    for i in pops: keys.remove(i)\n",
    "    pops = []\n",
    "    for i in range(len(fbkeys)):\n",
    "        if 'string' in fbkeys[i]:\n",
    "            pops.append(fbkeys[i])\n",
    "    for i in pops: fbkeys.remove(i)\n",
    "\n",
    "    recordindices = [[] for i in range(len(allvars))]\n",
    "    recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "    # output variables (from observations_table)\n",
    "    ov = []\n",
    "    for o in keys:\n",
    "        ov.append([[] for i in range(len(allvars))])\n",
    "    fb = []\n",
    "    for o in fbkeys:\n",
    "        fb.append([[] for i in range(len(allvars))])\n",
    "    #\n",
    "    # loading the observed_variables\n",
    "    #\n",
    "    obsv = data.observations_table.observed_variable[:]\n",
    "    #\n",
    "    # resorting the data\n",
    "    #\n",
    "#     print('resort:start')\n",
    "    @njit\n",
    "    def make_vrindex(vridx,ridx,idx):\n",
    "        l=0\n",
    "        for i in range(1,len(idx)): # to set the recordindices\n",
    "            if ridx[i]>ridx[i-1]:\n",
    "                vridx[ridx[i-1]:ridx[i]]=l # next record after l\n",
    "                l=i\n",
    "        vridx[ridx[i]:]=len(idx) # next record for the last element is the len of the data\n",
    "\n",
    "\n",
    "    tt=time.time()\n",
    "\n",
    "    ridxall=np.zeros(obsv.shape[0],dtype=np.int64) # reverse index - index of the record index\n",
    "    j=-1\n",
    "    for j in range(len(ri)-1):\n",
    "        ridxall[ri[j]:ri[j+1]]=j\n",
    "    j+=1\n",
    "    ridxall[ri[j]:]=j # for the last elemenet\n",
    "    ridx=[]\n",
    "    vridx=[]\n",
    "    absidx=[]\n",
    "    abscount=0\n",
    "    for j in range(len(allvars)):\n",
    "        idx=np.where(obsv==allvars[j])[0] # index of all elements form certain variable j\n",
    "#         print(j,len(idx),',',end='')\n",
    "        vridx.append(np.zeros(ri.shape[0],dtype=np.int64)) # all zeros in lenght of record index\n",
    "        ridx=ridxall[idx] # ridxall where variable is j\n",
    "        make_vrindex(vridx[-1],ridx,idx)\n",
    "        vridx[-1]+=abscount # abscount for stacking the recordindex\n",
    "\n",
    "        absidx.append(copy.copy(idx)) # why copy? - to make sure it's not just the ref. - maybe ok without the cp\n",
    "        abscount+=len(idx)\n",
    "\n",
    "#     print('')\n",
    "    #\n",
    "    # finishing the sorting \n",
    "    #\n",
    "    absidx=np.concatenate(absidx)\n",
    "    #\n",
    "    # recordtimestamps are only necessary once\n",
    "    #\n",
    "    recordtimestamps = recordtimestamps[0]\n",
    "    #\n",
    "    # targetfile has to be a copy of the original file\n",
    "    #\n",
    "    targetfile = '/raid60/scratch/uli/resorted_files/'+fn.split('/')[-1]# 0-20000-0-63894_CEUAS_merged_v0.nc'\n",
    "    if os.path.isfile(targetfile):\n",
    "        mode='r+'\n",
    "    else:\n",
    "        mode='w'\n",
    "#     print()\n",
    "#     print('writing '+targetfile)\n",
    "\n",
    "    for i in range(len(keys)):\n",
    "        ov_vars = data.observations_table[keys[i]][:]\n",
    "        ov_vars = ov_vars[absidx]\n",
    "        if keys[i] == 'index':\n",
    "            pass\n",
    "        elif keys[i] == 'observation_id' or keys[i] == 'report_id' or keys[i] == 'sensor_id' or keys[i] == 'source_id':\n",
    "            alldict = {keys[i]:np.asarray(ov_vars, dtype='S1')}\n",
    "            write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])\n",
    "        else:\n",
    "            alldict = pandas.DataFrame({keys[i]:ov_vars})\n",
    "            write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])  \n",
    "\n",
    "    for i in range(len(fbkeys)):\n",
    "        fb_vars = data.era5fb[fbkeys[i]][:]\n",
    "        fb_vars = fb_vars[absidx]\n",
    "        if fbkeys[i] == 'index' or fbkeys[i] == 'string6' or fbkeys[i] == 'string7' or fbkeys[i] == 'string10':\n",
    "            pass\n",
    "        elif fbkeys[i] == 'expver' or fbkeys[i] == 'source@hdr' or fbkeys[i] == 'source_id' or fbkeys[i] == 'statid@hdr':\n",
    "            alldict = {fbkeys[i]:np.asarray(fb_vars, dtype='S1')}\n",
    "            write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]])\n",
    "        else:\n",
    "            alldict = pandas.DataFrame({fbkeys[i]:fb_vars})\n",
    "            write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]]) \n",
    "    #\n",
    "    # writing the recordindices and recordtimestamp.\n",
    "    #       \n",
    "    recordindices=vridx\n",
    "    for i in range(len(recordindices)):\n",
    "        testvar = pandas.DataFrame({str(allvars[i]):recordindices[i]})\n",
    "        write_dict_h5(targetfile, testvar, 'recordindices', {str(allvars[i]): { 'compression': None } }, [str(allvars[i])]) \n",
    "\n",
    "    write_dict_h5(targetfile, {'recordtimestamp':rt}, 'recordindices', {'recordtimestamp': { 'compression': None } }, ['recordtimestamp'])\n",
    "\n",
    "    print('elapsed:',time.time()-tt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_resort(fn):\n",
    "    targetfile = '/raid60/scratch/uli/resorted_files_intercomp/'+fn.split('/')[-1]  \n",
    "    \n",
    "    with h5py.File(fn, 'r') as file:\n",
    "        with h5py.File(targetfile, 'w') as newfile:\n",
    "            groups = []\n",
    "            for i in file.keys():\n",
    "                if type(file[i]) == h5py._hl.group.Group:\n",
    "                    newfile.create_group(i)\n",
    "                    groups.append(i)\n",
    "                elif i == 'recordindex' or i == 'recordtimestamp':\n",
    "                    pass\n",
    "                else:\n",
    "                    newfile.create_dataset(i, data=file[i][:])\n",
    "            for i in groups:\n",
    "                if(i == 'recordindices' or i == 'observations_table' or i == 'era5fb'):\n",
    "                    pass\n",
    "                else:\n",
    "                    for j in file[i].keys():\n",
    "                        newfile[i].create_dataset(j, data=file[i][j][:])\n",
    "    \n",
    "\n",
    "    data =  eua.CDMDataset(fn)\n",
    "    allvars = data.observations_table.observed_variable[()]\n",
    "    allvars.sort()\n",
    "    allvars = numpy.unique(allvars)\n",
    "    #\n",
    "    ri = data.recordindex[()]\n",
    "#     print('recordindex: ', len(ri))\n",
    "    rt = data.recordtimestamp[()]\n",
    "    keys = data.observations_table.keys()#[:-1]\n",
    "#     fbkeys = data.era5fb.keys()[:-1]\n",
    "    # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "    pops = []\n",
    "    for i in range(len(keys)):\n",
    "        if 'string' in keys[i]:\n",
    "            pops.append(keys[i])\n",
    "    for i in pops: keys.remove(i)\n",
    "    keys.remove('shape')\n",
    "#     pops = []\n",
    "#     for i in range(len(fbkeys)):\n",
    "#         if 'string' in fbkeys[i]:\n",
    "#             pops.append(fbkeys[i])\n",
    "#     for i in pops: fbkeys.remove(i)\n",
    "\n",
    "    recordindices = [[] for i in range(len(allvars))]\n",
    "    recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "    # output variables (from observations_table)\n",
    "    ov = []\n",
    "    for o in keys:\n",
    "        ov.append([[] for i in range(len(allvars))])\n",
    "#     fb = []\n",
    "#     for o in fbkeys:\n",
    "#         fb.append([[] for i in range(len(allvars))])\n",
    "    #\n",
    "    # loading the observed_variables\n",
    "    #\n",
    "    obsv = data.observations_table.observed_variable[:]\n",
    "    #\n",
    "    # resorting the data\n",
    "    #\n",
    "#     print('resort:start')\n",
    "    @njit\n",
    "    def make_vrindex(vridx,ridx,idx):\n",
    "        l=0\n",
    "        for i in range(1,len(idx)): # to set the recordindices\n",
    "            if ridx[i]>ridx[i-1]:\n",
    "                vridx[ridx[i-1]:ridx[i]]=l # next record after l\n",
    "                l=i\n",
    "        vridx[ridx[i]:]=len(idx) # next record for the last element is the len of the data\n",
    "\n",
    "\n",
    "    tt=time.time()\n",
    "\n",
    "    ridxall=np.zeros(obsv.shape[0],dtype=np.int64) # reverse index - index of the record index\n",
    "    j=-1\n",
    "    for j in range(len(ri)-1):\n",
    "        ridxall[ri[j]:ri[j+1]]=j\n",
    "    j+=1\n",
    "    ridxall[ri[j]:]=j # for the last elemenet\n",
    "    ridx=[]\n",
    "    vridx=[]\n",
    "    absidx=[]\n",
    "    abscount=0\n",
    "    for j in range(len(allvars)):\n",
    "        idx=np.where(obsv==allvars[j])[0] # index of all elements form certain variable j\n",
    "#         print(j,len(idx),',',end='')\n",
    "        vridx.append(np.zeros(ri.shape[0],dtype=np.int64)) # all zeros in lenght of record index\n",
    "        ridx=ridxall[idx] # ridxall where variable is j\n",
    "        make_vrindex(vridx[-1],ridx,idx)\n",
    "        vridx[-1]+=abscount # abscount for stacking the recordindex\n",
    "\n",
    "        absidx.append(copy.copy(idx)) # why copy? - to make sure it's not just the ref. - maybe ok without the cp\n",
    "        abscount+=len(idx)\n",
    "\n",
    "#     print('')\n",
    "    #\n",
    "    # finishing the sorting \n",
    "    #\n",
    "    absidx=np.concatenate(absidx)\n",
    "    #\n",
    "    # recordtimestamps are only necessary once\n",
    "    #\n",
    "    recordtimestamps = recordtimestamps[0]\n",
    "    #\n",
    "    # targetfile has to be a copy of the original file\n",
    "    #\n",
    "#     targetfile = '/raid60/scratch/uli/resorted_files/'+fn.split('/')[-1]# 0-20000-0-63894_CEUAS_merged_v0.nc'\n",
    "    if os.path.isfile(targetfile):\n",
    "        mode='r+'\n",
    "    else:\n",
    "        mode='w'\n",
    "#     print()\n",
    "#     print('writing '+targetfile)\n",
    "\n",
    "    for i in range(len(keys)):\n",
    "        ov_vars = data.observations_table[keys[i]][:]\n",
    "        ov_vars = ov_vars[absidx]\n",
    "        if keys[i] == 'index':\n",
    "            pass\n",
    "        elif keys[i] == 'observation_id' or keys[i] == 'report_id' or keys[i] == 'sensor_id' or keys[i] == 'reference_sensor_id' or keys[i] == 'adjustment_id':\n",
    "            alldict = {keys[i]:np.asarray(ov_vars, dtype='S1')}\n",
    "            write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])\n",
    "        else:\n",
    "            alldict = pandas.DataFrame({keys[i]:ov_vars})\n",
    "            write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])  \n",
    "\n",
    "#     for i in range(len(fbkeys)):\n",
    "#         fb_vars = data.era5fb[fbkeys[i]][:]\n",
    "#         fb_vars = fb_vars[absidx]\n",
    "#         if fbkeys[i] == 'index' or fbkeys[i] == 'string6' or fbkeys[i] == 'string7' or fbkeys[i] == 'string10':\n",
    "#             pass\n",
    "#         elif fbkeys[i] == 'expver' or fbkeys[i] == 'source@hdr' or fbkeys[i] == 'source_id' or fbkeys[i] == 'statid@hdr':\n",
    "#             alldict = {fbkeys[i]:np.asarray(fb_vars, dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({fbkeys[i]:fb_vars})\n",
    "#             write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]]) \n",
    "    #\n",
    "    # writing the recordindices and recordtimestamp.\n",
    "    #       \n",
    "    recordindices=vridx\n",
    "    for i in range(len(recordindices)):\n",
    "        testvar = pandas.DataFrame({str(allvars[i]):recordindices[i]})\n",
    "        write_dict_h5(targetfile, testvar, 'recordindices', {str(allvars[i]): { 'compression': None } }, [str(allvars[i])]) \n",
    "\n",
    "    write_dict_h5(targetfile, {'recordtimestamp':rt}, 'recordindices', {'recordtimestamp': { 'compression': None } }, ['recordtimestamp'])\n",
    "\n",
    "    print('elapsed:',time.time()-tt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with eua.CDMDataset('/raid60/scratch/leo/scratch/RI/Pangaea/nc/0-20100-0-00201.nc') as test:\n",
    "#     print(test.observations_table)\n",
    "# with eua.CDMDataset('/resorted_files_intercomp/0-20100-0-00201.nc') as test:\n",
    "#     print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 0.5056281089782715\n"
     ]
    }
   ],
   "source": [
    "do_resort('/raid60/scratch/leo/scratch/RI/Pangaea/nc/0-20100-0-00201.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 1.5315845012664795\n",
      "elapsed: 1.5997085571289062elapsed:\n",
      " elapsed:1.5588514804840088\n",
      " 1.5607447624206543\n",
      "elapsed: 1.568709135055542\n",
      "elapsed: 1.5629076957702637\n",
      "elapsed: 1.549851655960083\n",
      "elapsed: 1.4894349575042725elapsed:\n",
      " 1.5891351699829102\n",
      "elapsed: 1.559189796447754\n",
      "elapsed: 1.553767204284668\n",
      "elapsed:elapsed:elapsed:  1.59126877784729 1.5946612358093262\n",
      "\n",
      "1.6015119552612305\n",
      "elapsed: 1.6234643459320068\n",
      "elapsed: 1.6179451942443848\n",
      "elapsed: 1.5299904346466064\n",
      "elapsed: 1.6212189197540283\n",
      "elapsed: 1.8305962085723877\n",
      "elapsed: 1.792104721069336\n",
      "elapsed: 1.4161343574523926\n",
      "elapsed: 1.4175779819488525\n",
      "elapsed: elapsed:1.401658058166504\n",
      " 1.4413726329803467\n",
      "elapsed: 1.318955659866333\n",
      "elapsed: 1.4234223365783691\n",
      "elapsed: 1.4309673309326172\n",
      "elapsed: elapsed:1.4531896114349365 \n",
      "elapsed:1.4446642398834229 1.4488966464996338\n",
      "elapsed:\n",
      " 1.443387746810913\n",
      "elapsed: 1.517026424407959\n",
      "elapsed: 1.5012574195861816\n",
      "elapsed: 1.4731471538543701\n",
      "elapsed: 1.430403470993042\n",
      "elapsed: 1.4788234233856201\n",
      "elapsed: 1.525907278060913\n",
      "elapsed: 1.650867223739624\n",
      "elapsed: 1.8558197021484375\n",
      "elapsed: 1.8512978553771973\n",
      "elapsed: elapsed: 1.55805468559265141.5555920600891113\n",
      "\n",
      "elapsed: 1.5456757545471191\n",
      "elapsed: 1.5346496105194092\n",
      "elapsed: 1.593268871307373\n",
      "elapsed: 1.6024198532104492\n",
      "elapsed: 1.603999137878418\n",
      "elapsed:elapsed:  1.56862664222717291.6029689311981201\n",
      "\n",
      "elapsed: 1.7047021389007568\n",
      "elapsed:elapsed:  1.7076745033264161.754638433456421\n",
      "\n",
      "elapsed: 1.694688081741333\n",
      "elapsed: 1.6948602199554443\n",
      "elapsed: 1.663008689880371\n",
      "elapsed: 1.8707726001739502\n",
      "elapsed: 1.7773287296295166\n",
      "elapsed: 1.7761740684509277\n",
      "elapsed: elapsed: 1.87598919868469241.8302452564239502\n",
      "\n",
      "elapsed: 2.019498586654663\n",
      "elapsed: 2.0707244873046875\n",
      "elapsed: 2.0731899738311768\n",
      "elapsed: 2.005479335784912\n",
      "elapsed: elapsed:1.9995667934417725 \n",
      "2.017066717147827\n",
      "elapsed: 1.9927828311920166\n",
      "elapsed: 2.1673636436462402\n",
      "elapsed: 2.0233852863311768\n",
      "elapsed: 2.1468660831451416\n",
      "elapsed: 2.071150779724121\n",
      "elapsed: 1.906507968902588\n",
      "elapsed: 2.0863590240478516\n",
      "elapsed: 2.1034531593322754\n",
      "elapsed: 2.030117988586426\n",
      "elapsed: 2.1967122554779053\n",
      "elapsed: 2.133131265640259\n",
      "elapsed: 2.232747793197632\n",
      "elapsed: 1.849546194076538\n",
      "elapsed: 1.886688470840454\n",
      "elapsed: 1.5731403827667236\n",
      "elapsed: 1.5152063369750977\n",
      "elapsed: 1.4103655815124512\n",
      "elapsed: 1.4815757274627686\n",
      "elapsed: 1.4482421875\n",
      "elapsed: 1.5672388076782227\n",
      "elapsed: 1.542008638381958\n",
      "elapsed: 1.4606668949127197\n",
      "elapsed: 1.4847571849822998\n",
      "elapsed: 1.6510207653045654\n",
      "elapsed: 1.532285451889038\n",
      "elapsed: 1.5780155658721924\n",
      "elapsed: 1.6370000839233398\n",
      "elapsed: 1.5959830284118652\n",
      "elapsed: 1.6084208488464355\n",
      "elapsed: 1.6187565326690674\n",
      "elapsed:elapsed:  1.5091576576232911.668879508972168\n",
      "\n",
      "elapsed: 1.6349332332611084\n",
      "elapsed: 1.5923855304718018\n",
      "elapsed: 1.5038504600524902\n",
      "elapsed: 1.72225022315979\n",
      "elapsed: 1.6020376682281494\n",
      "elapsed: 1.6462371349334717\n",
      "elapsed: 1.5416343212127686\n",
      "elapsed: 1.462205171585083\n",
      "elapsed: 1.635676383972168\n",
      "elapsed: 1.6021411418914795\n",
      "elapsed: 1.6198756694793701\n",
      "elapsed:elapsed:  1.5677628517150881.503908634185791\n",
      "\n",
      "elapsed: 1.5828537940979004\n",
      "elapsed: 1.5615415573120117\n",
      "elapsed: 1.5874323844909668\n",
      "elapsed: 1.4801595211029053\n",
      "elapsed: 1.614945888519287\n",
      "elapsed: 1.5425446033477783\n",
      "elapsed: 1.4642534255981445\n",
      "elapsed: 1.5532710552215576\n",
      "elapsed: 1.506441593170166\n",
      "elapsed: 1.2057487964630127\n",
      "elapsed: 1.1385776996612549\n",
      "elapsed: 1.2657511234283447\n",
      "elapsed: 1.0794119834899902\n",
      "elapsed: 1.146876335144043\n",
      "elapsed: 1.1025934219360352\n",
      "elapsed: 1.0478932857513428\n",
      "elapsed: 1.1687912940979004\n",
      "elapsed: 1.1448678970336914\n",
      "elapsed: 1.0934944152832031\n",
      "elapsed:elapsed: 1.1749556064605713 \n",
      "1.1115403175354004\n",
      "elapsed: 1.1321609020233154\n",
      "elapsed: 1.1308419704437256\n",
      "elapsed: 1.326836347579956\n",
      "elapsed: 1.26617431640625\n",
      "elapsed: 1.2989461421966553\n",
      "elapsed: 1.7345914840698242\n",
      "elapsed: 1.6842401027679443\n",
      "elapsed: 1.5583949089050293\n",
      "elapsed: 1.555704116821289\n",
      "elapsed: 1.6295127868652344\n",
      "elapsed: 1.6974458694458008\n",
      "elapsed: 1.4785075187683105\n",
      "elapsed: 1.478327751159668\n",
      "elapsed: 1.659132480621338\n",
      "elapsed: 1.5786867141723633\n",
      "elapsed: 1.5667986869812012\n",
      "elapsed: 1.6255419254302979\n",
      "elapsed: 1.6495118141174316\n",
      "elapsed: 1.335533857345581\n",
      "elapsed: 1.307117223739624\n",
      "elapsed: 1.5589375495910645\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    pool = multiprocessing.Pool(processes=20)\n",
    "    result_list = pool.map(do_resort, files[:])\n",
    "    print(result_list)\n",
    "\n",
    "# 4250:4300\n",
    "# 4250:4255\n",
    "# sonst alle bis 5500\n",
    "# for i in range(len(files)):\n",
    "#     do_resort(files[i])\n",
    "#     if i > 2: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: <HDF5 file \"0-20200-0-01502.nc\" (mode r)>\n",
      "Filename: resorted_files_intercomp/0-20200-0-01502.nc\n",
      "(G)roups/(V)ariables: \n",
      "\n",
      " - G | crs__________________________________________ : : 4\n",
      " - V | dateindex____________________________________ : : (1, 3)\n",
      " - V | days_________________________________________ : : (1,)\n",
      " - V | drange_______________________________________ : : (3,)\n",
      " - G | header_table_________________________________ : : 46\n",
      " - G | id_scheme____________________________________ : : 4\n",
      " - G | iri_rstype_map_______________________________ : : 8\n",
      " - G | observations_table___________________________ : : 52\n",
      " - G | observed_variable____________________________ : : 15\n",
      " - V | record_______________________________________ : : (2,)\n",
      " - V | recordindex__________________________________ : : (2,)\n",
      " - G | recordindices________________________________ : : 4\n",
      " - V | recordtimestamp______________________________ : : (2,)\n",
      " - G | sensor_configuration_________________________ : : 12\n",
      " - G | source_configuration_________________________ : : 33\n",
      " - G | station_configuration_codes__________________ : : 10\n",
      " - G | station_type_________________________________ : : 4\n",
      " - G | units________________________________________ : : 9\n",
      " - G | z_coordinate_type____________________________ : : 4\n"
     ]
    }
   ],
   "source": [
    "with eua.CDMDataset('resorted_files_intercomp/0-20200-0-01502.nc') as data:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid60/scratch/federico/MERGED_DATABASE_OCTOBER2020_sensor/0-20000-0-22165_CEUAS_merged_v0.nc']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[4253:4254]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/raid60/scratch/federico/MERGED_DATABASE_OCTOBER2020_sensor/0-20000-0-22165_CEUAS_merged_v0.nc'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[4253]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allvars = data.observations_table.observed_variable[()]\n",
    "# allvars.sort()\n",
    "# allvars = np.unique(allvars)\n",
    "# # recordindices einzeln sortieren - fÃ¼r alle variablen eigene listen erstellen - dazu recordindices erstellen - alle daten hintereinander legen - len aller vorhergehender rec_in zu den nachkommenden addieren.\n",
    "# #\n",
    "# ri = data.recordindex[()]\n",
    "# rt = data.recordtimestamp[()]\n",
    "# keys = data.observations_table.keys()\n",
    "# recordindices = [[] for i in range(len(np.unique(allvars)))]\n",
    "# recordtimestamps = [[] for i in range(len(np.unique(allvars)))]\n",
    "\n",
    "# variables = [[] for i in range(len(np.unique(allvars)))]\n",
    "# values = [[] for i in range(len(np.unique(allvars)))]\n",
    "# zcoords = [[] for i in range(len(np.unique(allvars)))]\n",
    "\n",
    "# for i in range(len(ri)):\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         break\n",
    "        \n",
    "#     a = data.observations_table.observed_variable[()][start:end]\n",
    "#     b = data.observations_table.observation_value[()][start:end]\n",
    "#     c = data.observations_table.z_coordinate[()][start:end]\n",
    "\n",
    "#     sa, sb = zip(*sorted(zip(a, b)))\n",
    "#     sa, sc = zip(*sorted(zip(a, c)))\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 variables[j].append(sa[k])\n",
    "#                 values[j].append(sb[k])\n",
    "#                 zcoords[j].append(sc[k])\n",
    "#         recordindices[j].append(len(variables[j]))\n",
    "#         recordtimestamps[j].append(rt[i])\n",
    "        \n",
    "# lenadd = 0\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(variables[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allvars = data.observations_table.observed_variable[()]\n",
    "# allvars.sort()\n",
    "# allvars = np.unique(allvars)\n",
    "# #\n",
    "# ri = data.recordindex[()]\n",
    "# rt = data.recordtimestamp[()]\n",
    "# keys = data.observations_table.keys()[:-1]\n",
    "# recordindices = [[] for i in range(len(np.unique(allvars)))]\n",
    "# recordtimestamps = [[] for i in range(len(np.unique(allvars)))]\n",
    "\n",
    "# # output variables (from observations_table)\n",
    "# ov = []\n",
    "# for o in keys:\n",
    "#     ov.append([[] for i in range(len(np.unique(allvars)))])\n",
    "    \n",
    "# for i in range(len(ri)):\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         break\n",
    "        \n",
    "#     a = data.observations_table.observed_variable[()][start:end]\n",
    "#     helpvar = []\n",
    "#     for o in range(len(keys)):\n",
    "#         b = data.observations_table[keys[o]][()][start:end]\n",
    "#         try: # all varibales with different shape won't work here\n",
    "#             sa, sb = zip(*sorted(zip(a, b)))\n",
    "#             helpvar.append(sb)\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 for m in range(len(helpvar)):\n",
    "#                     try: # all varibales with different shape won't work here\n",
    "#                         ov[m][j].append(helpvar[m][k])\n",
    "#                     except:\n",
    "#                         pass\n",
    "#         recordindices[j].append(len(ov[0][j]))\n",
    "#         recordtimestamps[j].append(rt[i])\n",
    "        \n",
    "# lenadd = 0\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(variables[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# #\n",
    "# allvars = data.observations_table.observed_variable[()]\n",
    "# allvars.sort()\n",
    "# allvars = np.unique(allvars)\n",
    "# #\n",
    "# ri = data.recordindex[()]\n",
    "# print(len(ri))\n",
    "# # rt = data.recordtimestamp[()]\n",
    "# keys = data.observations_table.keys()[:-1]\n",
    "# # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "# keys.pop(43)\n",
    "# keys.pop(42)\n",
    "# keys.pop(41)\n",
    "# recordindices = [[] for i in range(len(allvars))]\n",
    "# recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "# # output variables (from observations_table)\n",
    "# ov = []\n",
    "# for o in keys:\n",
    "#     ov.append([[] for i in range(len(np.unique(allvars)))])    \n",
    "    \n",
    "# print('resort:start')\n",
    "# for i in range(len(ri)):\n",
    "#     print(i)\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         start = ri[i]\n",
    "#         end = len(data.observations_table.observed_variable[()])\n",
    "        \n",
    "#     a = data.observations_table.observed_variable[()][start:end]\n",
    "#     helpvar = []\n",
    "#     for o in range(len(keys)):\n",
    "#         if keys[o] == 'observation_id' or keys[o] == 'report_id' or keys[o] == 'sensor_id' or keys[o] == 'source_id':\n",
    "#             b = []\n",
    "#             for n in data.observations_table[keys[o]][()][start:end]:\n",
    "#                 c = ''\n",
    "#                 for bb in n:\n",
    "#                     c = c + bb.decode()\n",
    "#                 b.append(c)\n",
    "#         else:\n",
    "#             b = data.observations_table[keys[o]][()][start:end]\n",
    "#         sa, sb = zip(*sorted(zip(a, b)))\n",
    "#         helpvar.append(sb)\n",
    "\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 for m in range(len(helpvar)):\n",
    "#                     ov[m][j].append(helpvar[m][k])\n",
    "#         recordindices[j].append(len(ov[0][j]))\n",
    "# #         recordtimestamps[j].append(rt[i])\n",
    "\n",
    "# print('resort:done')\n",
    "        \n",
    "# #\n",
    "# # setting record_indices to the right value -> stacking them\n",
    "# #\n",
    "# lenadd = 0.\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(ov[0][i])\n",
    "\n",
    "# #\n",
    "# # shaping record_indices -> setting every missing value to nan\n",
    "# #\n",
    "# old = -1\n",
    "# for i in range(len(recordindices)):\n",
    "#     for j in range(len(recordindices[i])):\n",
    "#         if recordindices[i][j] != 0 and recordindices[i][j] == recordindices[i][j-1]:\n",
    "#             old = recordindices[i][j]\n",
    "#             recordindices[i][j] = np.nan\n",
    "#         elif recordindices[i][j] == old:\n",
    "#             recordindices[i][j] = np.nan\n",
    "\n",
    "# #\n",
    "# # recordtimestamps are only necessary once\n",
    "# #\n",
    "# recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "# # \n",
    "# # stacking all output variables\n",
    "# #\n",
    "# out = []\n",
    "# for j in ov:\n",
    "#     finalvar = []\n",
    "#     for i in j:\n",
    "#         finalvar = finalvar + i\n",
    "#     out.append(finalvar)\n",
    "    \n",
    "# #\n",
    "# # restoring byte arrays:\n",
    "# #\n",
    "# for o in range(len(keys)):\n",
    "#     if keys[o] == 'observation_id' or keys[o] == 'report_id' or keys[o] == 'sensor_id' or keys[o] == 'source_id':\n",
    "#         b = []\n",
    "#         for n in range(len(out[o])):\n",
    "#             c = []\n",
    "#             for bb in out[o][n]:\n",
    "#                 c.append(bb.encode())\n",
    "#             out[o][n] = c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allvars = data.observations_table.observed_variable[()]\n",
    "# allvars.sort()\n",
    "# allvars = np.unique(allvars)\n",
    "# #\n",
    "# ri = data.recordindex[()]\n",
    "# print('recordindex: ', len(ri))\n",
    "# # rt = data.recordtimestamp[()]\n",
    "# keys = data.observations_table.keys()[:-1]\n",
    "# # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "# keys.pop(43)\n",
    "# keys.pop(42)\n",
    "# keys.pop(41)\n",
    "# recordindices = [[] for i in range(len(allvars))]\n",
    "# recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "# # output variables (from observations_table)\n",
    "# ov = []\n",
    "# for o in keys:\n",
    "#     ov.append([[] for i in range(len(np.unique(allvars)))])\n",
    "\n",
    "# #\n",
    "# # load data into memory and decode byte arrays\n",
    "# #\n",
    "# print('loading data')\n",
    "# obsv = data.observations_table.observed_variable[:]\n",
    "# ov_vars = []\n",
    "# for o in range(len(keys)):\n",
    "#     ov_vars.append(data.observations_table[keys[o]][:])\n",
    "#     if keys[o] == 'observation_id' or keys[o] == 'report_id' or keys[o] == 'sensor_id' or keys[o] == 'source_id':\n",
    "#         b = []\n",
    "#         for n in ov_vars[o]:\n",
    "#             c = ''\n",
    "#             for bb in n:\n",
    "#                 c = c + bb.decode()\n",
    "#             b.append(c)\n",
    "#         ov_vars[o] = b\n",
    "\n",
    "# #\n",
    "# # resorting the data\n",
    "# #\n",
    "# print('resort:start')\n",
    "# for i in range(len(ri)):\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         start = ri[i]\n",
    "#         end = len(data.observations_table.observed_variable[()])\n",
    "        \n",
    "#     a = obsv[start:end]\n",
    "#     helpvar = []\n",
    "#     for o in range(len(keys)):\n",
    "#         b = ov_vars[o][start:end]\n",
    "#         sa, sb = zip(*sorted(zip(a, b)))\n",
    "#         helpvar.append(sb)\n",
    "\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 for m in range(len(helpvar)):\n",
    "#                     ov[m][j].append(helpvar[m][k])\n",
    "#         recordindices[j].append(len(ov[0][j]))\n",
    "# #         recordtimestamps[j].append(rt[i])\n",
    "\n",
    "# print('resort:done')\n",
    "        \n",
    "# #\n",
    "# # setting record_indices to the right value -> stacking them\n",
    "# #\n",
    "# lenadd = 0.\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(ov[0][i])\n",
    "\n",
    "# #\n",
    "# # shaping record_indices -> setting every missing value to nan\n",
    "# #\n",
    "# print('shaping record_indices')\n",
    "# old = -1\n",
    "# for i in range(len(recordindices)):\n",
    "#     for j in range(len(recordindices[i])):\n",
    "#         if recordindices[i][j] != 0 and recordindices[i][j] == recordindices[i][j-1]:\n",
    "#             old = recordindices[i][j]\n",
    "#             recordindices[i][j] = np.nan\n",
    "#         elif recordindices[i][j] == old:\n",
    "#             recordindices[i][j] = np.nan\n",
    "\n",
    "# #\n",
    "# # recordtimestamps are only necessary once\n",
    "# #\n",
    "# recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "# # \n",
    "# # stacking all output variables\n",
    "# #\n",
    "# print('stacking output variables')\n",
    "# out = []\n",
    "# for j in ov:\n",
    "#     finalvar = []\n",
    "#     for i in j:\n",
    "#         finalvar = finalvar + i\n",
    "#     out.append(finalvar)\n",
    "    \n",
    "# #\n",
    "# # restoring byte arrays:\n",
    "# # this takes very long -> find faster option\n",
    "# #\n",
    "# print('restoring byte arrays')\n",
    "# bytelist = [25, 34, 38, 39]\n",
    "# for o in bytelist:\n",
    "#     originallen = len(data.observations_table[keys[o]][()][0])\n",
    "#     for n in range(len(out[o])):\n",
    "#         c = [elem.encode() for elem in out[o][n]]\n",
    "#         # problem: if the string was [b'x', b'y', b''] in the first place, it get converted to 'xy' and then back to [b'x', b'y']\n",
    "#         # add empty byte strings until the data is as long as before:\n",
    "#         while(len(c) < originallen):\n",
    "#             c.append(str('').encode())\n",
    "#         out[o][n] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with eua.CDMDataset(files[0]) as data:\n",
    "#     allvars = data.observations_table.observed_variable[()]\n",
    "#     allvars.sort()\n",
    "#     allvars = np.unique(allvars)\n",
    "#     #\n",
    "#     ri = data.recordindex[()]\n",
    "#     print('recordindex: ', len(ri))\n",
    "#     keys = data.observations_table.keys()[:-1]\n",
    "#     # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "#     keys.pop(43)\n",
    "#     keys.pop(42)\n",
    "#     keys.pop(41)\n",
    "#     recordindices = [[] for i in range(len(allvars))]\n",
    "#     recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "#     # output variables (from observations_table)\n",
    "#     ov = []\n",
    "#     for o in keys:\n",
    "#         ov.append([[] for i in range(len(np.unique(allvars)))])\n",
    "\n",
    "#     #\n",
    "#     # load data into memory and decode byte arrays\n",
    "#     #\n",
    "#     print('loading data')\n",
    "#     obsv = data.observations_table.observed_variable[:]\n",
    "#     ov_vars = []\n",
    "#     for o in range(len(keys)):\n",
    "#         ov_vars.append(data.observations_table[keys[o]][:])\n",
    "\n",
    "# #\n",
    "# # resorting the data\n",
    "# #\n",
    "# print('resort:start')\n",
    "# for i in range(len(ri)):\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         start = ri[i]\n",
    "#         end = len(obsv)\n",
    "        \n",
    "#     a = obsv[start:end]\n",
    "#     sa, sortindex = zip(*sorted(zip(a, range(len(a)))))\n",
    "#     sortindex = np.array(sortindex)\n",
    "#     helpvar = []\n",
    "#     for o in range(len(keys)):\n",
    "#         b = ov_vars[o][start:end]\n",
    "#         helpvar.append(b[sortindex])\n",
    "\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 for m in range(len(helpvar)):\n",
    "#                     ov[m][j].append(helpvar[m][k])\n",
    "#         recordindices[j].append(len(ov[0][j]))\n",
    "\n",
    "# print('resort:done')\n",
    "        \n",
    "# #\n",
    "# # setting record_indices to the right value -> stacking them\n",
    "# #\n",
    "# lenadd = 0.\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(ov[0][i])\n",
    "\n",
    "# #\n",
    "# # shaping record_indices -> setting every missing value to nan\n",
    "# #\n",
    "# print('shaping record_indices')\n",
    "# old = -1\n",
    "# for i in range(len(recordindices)):\n",
    "#     for j in range(len(recordindices[i])):\n",
    "#         if i != 0 or j != 0:\n",
    "#             if recordindices[i][j] == recordindices[i][j-1]:\n",
    "#                 old = recordindices[i][j]\n",
    "#                 recordindices[i][j] = np.nan\n",
    "#             elif recordindices[i][j] == old:\n",
    "#                 recordindices[i][j] = np.nan\n",
    "\n",
    "# #\n",
    "# # recordtimestamps are only necessary once\n",
    "# #\n",
    "# recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "# # \n",
    "# # stacking all output variables\n",
    "# #\n",
    "# print('stacking output variables')\n",
    "# out = []\n",
    "# for k in ov:\n",
    "#     out.append([j for i in k for j in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_resort(fn):\n",
    "    \n",
    "#     data = eua.CDMDataset(fn)\n",
    "\n",
    "#     allvars = data.observations_table.observed_variable[()]\n",
    "#     allvars.sort()\n",
    "#     allvars = numpy.unique(allvars)\n",
    "#     #\n",
    "#     ri = data.recordindex[()]\n",
    "#     print('recordindex: ', len(ri))\n",
    "#     rt = data.recordtimestamp[()]\n",
    "#     keys = data.observations_table.keys()[:-1]\n",
    "#     fbkeys = data.era5fb.keys()[:]\n",
    "#     # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "#     keys.pop(43)\n",
    "#     keys.pop(42)\n",
    "#     keys.pop(41)\n",
    "#     recordindices = [[] for i in range(len(allvars))]\n",
    "#     recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "#     # output variables (from observations_table)\n",
    "#     ov = []\n",
    "#     for o in keys:\n",
    "#         ov.append([[] for i in range(len(numpy.unique(allvars)))])\n",
    "\n",
    "#     #\n",
    "#     # load data into memory and decode byte arrays\n",
    "#     #\n",
    "#     print('loading data')\n",
    "#     obsv = data.observations_table.observed_variable[:]\n",
    "#     ov_vars = []\n",
    "#     for o in range(len(keys)):\n",
    "#         ov_vars.append(data.observations_table[keys[o]][:])\n",
    "\n",
    "#     #\n",
    "#     # resorting the data\n",
    "#     #\n",
    "#     print('resort:start')\n",
    "#     @njit\n",
    "#     def make_vrindex(vridx,ridx,idx):\n",
    "#         l=0\n",
    "#         for i in range(1,len(idx)): # to set the recordindices\n",
    "#             if ridx[i]>ridx[i-1]:\n",
    "#                 vridx[ridx[i-1]:ridx[i]]=l # next record after l\n",
    "#                 l=i\n",
    "#         vridx[ridx[i]:]=len(idx) # next record for the last element is the len of the data\n",
    "\n",
    "\n",
    "#     tt=time.time()\n",
    "\n",
    "#     ridxall=np.zeros(ov_vars[0].shape[0],dtype=np.int64) # reverse index - index of the record index\n",
    "#     j=-1\n",
    "#     for j in range(len(ri)-1):\n",
    "#         ridxall[ri[j]:ri[j+1]]=j\n",
    "#     j+=1\n",
    "#     ridxall[ri[j]:]=j # for the last elemenet\n",
    "\n",
    "#     ridx=[]\n",
    "#     vridx=[]\n",
    "#     absidx=[]\n",
    "#     abscount=0\n",
    "#     for j in range(len(allvars)):\n",
    "#         idx=np.where(ov_vars[keys.index('observed_variable')]==allvars[j])[0] # index of all elements form certain variable j\n",
    "#         print(j,len(idx),',',end='')\n",
    "#         vridx.append(np.zeros(ri.shape[0],dtype=np.int64)) # all zeros in lenght of record index\n",
    "#         ridx=ridxall[idx] # ridxall where variable is j\n",
    "#         make_vrindex(vridx[-1],ridx,idx)\n",
    "#         vridx[-1]+=abscount # abscount for stacking the recordindex\n",
    "\n",
    "#         absidx.append(copy.copy(idx)) # why copy? - to make sure it's not just the ref. - maybe ok without the cp\n",
    "#         abscount+=len(idx)\n",
    "\n",
    "#     print('')\n",
    "#     absidx=np.concatenate(absidx)\n",
    "#     for o in range(len(keys)):\n",
    "#         ov_vars[o]=ov_vars[o][absidx]\n",
    "#         print(o,end='')\n",
    "#     #\n",
    "#     # recordtimestamps are only necessary once\n",
    "#     #\n",
    "#     recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "\n",
    "#     out = ov_vars\n",
    "#     # targetfile has to be a copy of the original file\n",
    "#     targetfile = '/raid60/scratch/uli/resorted_files_20201109/'+fn.split('/')[-1]# 0-20000-0-63894_CEUAS_merged_v0.nc'\n",
    "#     if os.path.isfile(targetfile):\n",
    "#         mode='r+'\n",
    "#     else:\n",
    "#         mode='w'\n",
    "        \n",
    "#     print()\n",
    "#     print('writing '+targetfile)\n",
    "#     #\n",
    "#     # writing data into observations_table\n",
    "#     #\n",
    "#     with h5py.File(targetfile, mode) as file:\n",
    "#         for i in range(len(keys)):\n",
    "#             try:\n",
    "#                 del file['observations_table'][keys[i]]\n",
    "#             except:\n",
    "#                 pass\n",
    "#     for i in range(len(keys)):\n",
    "#         if keys[i] == 'index':\n",
    "#             pass\n",
    "#         elif keys[i] == 'observation_id' or keys[i] == 'report_id' or keys[i] == 'sensor_id' or keys[i] == 'source_id':\n",
    "# #             slen = len(out[i][0])\n",
    "#             alldict = {keys[i]:np.asarray(out[i], dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({keys[i]:out[i]})\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])  \n",
    "#         #\n",
    "#         # writing the recordindices and recordtimestamp.\n",
    "#         #       \n",
    "#     recordindices=vridx\n",
    "#     with h5py.File(targetfile, 'r+') as file:\n",
    "#         for i in range(len(recordindices)):\n",
    "#             try:\n",
    "#                 del file['recordindices'][str(allvars[i])]\n",
    "#             except:\n",
    "#                 pass\n",
    "#     for i in range(len(recordindices)):\n",
    "#         testvar = pandas.DataFrame({str(allvars[i]):recordindices[i]})\n",
    "#         write_dict_h5(targetfile, testvar, 'recordindices', {str(allvars[i]): { 'compression': None } }, [str(allvars[i])]) \n",
    "\n",
    "#     with h5py.File(targetfile, 'r+') as file:\n",
    "#         try:\n",
    "#             del file['recordindex']\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             del file['recordtimestamp']\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             del file['recordindices']['recordtimestamp']\n",
    "#         except:\n",
    "#             pass\n",
    "#     write_dict_h5(targetfile, {'recordtimestamp':rt}, 'recordindices', {'recordtimestamp': { 'compression': None } }, ['recordtimestamp']) \n",
    "        \n",
    "#     print('elapsed:',time.time()-tt)\n",
    "#     print(end='')\n",
    "\n",
    "\n",
    "\n",
    "# # In[15]:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_resort(fn):\n",
    "    \n",
    "#     data = eua.CDMDataset(fn)\n",
    "\n",
    "#     allvars = data.observations_table.observed_variable[()]\n",
    "#     allvars.sort()\n",
    "#     allvars = numpy.unique(allvars)\n",
    "#     #\n",
    "#     ri = data.recordindex[()]\n",
    "#     print('recordindex: ', len(ri))\n",
    "#     rt = data.recordtimestamp[()]\n",
    "#     keys = data.observations_table.keys()[:-1]\n",
    "#     fbkeys = data.era5fb.keys()[:-1]\n",
    "#     # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "#     pops = []\n",
    "#     for i in range(len(keys)):\n",
    "#         if 'string' in keys[i]:\n",
    "#             pops.append(keys[i])\n",
    "#     for i in pops: keys.remove(i)\n",
    "#     pops = []\n",
    "#     for i in range(len(fbkeys)):\n",
    "#         if 'string' in fbkeys[i]:\n",
    "#             pops.append(fbkeys[i])\n",
    "#     for i in pops: fbkeys.remove(i)\n",
    "        \n",
    "#     recordindices = [[] for i in range(len(allvars))]\n",
    "#     recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "#     # output variables (from observations_table)\n",
    "#     ov = []\n",
    "#     for o in keys:\n",
    "#         ov.append([[] for i in range(len(allvars))])\n",
    "#     fb = []\n",
    "#     for o in fbkeys:\n",
    "#         fb.append([[] for i in range(len(allvars))])\n",
    "\n",
    "#     #\n",
    "#     # load data into memory and decode byte arrays\n",
    "#     #\n",
    "#     print('loading data')\n",
    "#     obsv = data.observations_table.observed_variable[:]\n",
    "#     ov_vars = []\n",
    "#     for o in range(len(keys)):\n",
    "#         ov_vars.append(data.observations_table[keys[o]][:])\n",
    "#     fb_vars = []\n",
    "#     for o in range(len(fbkeys)):\n",
    "#         fb_vars.append(data.era5fb[fbkeys[o]][:])\n",
    "\n",
    "#     #\n",
    "#     # resorting the data\n",
    "#     #\n",
    "#     print('resort:start')\n",
    "#     @njit\n",
    "#     def make_vrindex(vridx,ridx,idx):\n",
    "#         l=0\n",
    "#         for i in range(1,len(idx)): # to set the recordindices\n",
    "#             if ridx[i]>ridx[i-1]:\n",
    "#                 vridx[ridx[i-1]:ridx[i]]=l # next record after l\n",
    "#                 l=i\n",
    "#         vridx[ridx[i]:]=len(idx) # next record for the last element is the len of the data\n",
    "\n",
    "\n",
    "#     tt=time.time()\n",
    "\n",
    "#     ridxall=np.zeros(ov_vars[0].shape[0],dtype=np.int64) # reverse index - index of the record index\n",
    "#     j=-1\n",
    "#     for j in range(len(ri)-1):\n",
    "#         ridxall[ri[j]:ri[j+1]]=j\n",
    "#     j+=1\n",
    "#     ridxall[ri[j]:]=j # for the last elemenet\n",
    "\n",
    "#     ridx=[]\n",
    "#     vridx=[]\n",
    "#     absidx=[]\n",
    "#     abscount=0\n",
    "#     for j in range(len(allvars)):\n",
    "#         idx=np.where(ov_vars[keys.index('observed_variable')]==allvars[j])[0] # index of all elements form certain variable j\n",
    "#         print(j,len(idx),',',end='')\n",
    "#         vridx.append(np.zeros(ri.shape[0],dtype=np.int64)) # all zeros in lenght of record index\n",
    "#         ridx=ridxall[idx] # ridxall where variable is j\n",
    "#         make_vrindex(vridx[-1],ridx,idx)\n",
    "#         vridx[-1]+=abscount # abscount for stacking the recordindex\n",
    "\n",
    "#         absidx.append(copy.copy(idx)) # why copy? - to make sure it's not just the ref. - maybe ok without the cp\n",
    "#         abscount+=len(idx)\n",
    "\n",
    "#     print('')\n",
    "#     absidx=np.concatenate(absidx)\n",
    "#     for o in range(len(keys)):\n",
    "#         ov_vars[o]=ov_vars[o][absidx]\n",
    "#         print(o,end='')\n",
    "        \n",
    "#     for o in range(len(fbkeys)):\n",
    "#         fb_vars[o]=fb_vars[o][absidx]\n",
    "#         print(o,end='')\n",
    "\n",
    "#     #\n",
    "#     # recordtimestamps are only necessary once\n",
    "#     #\n",
    "#     recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "\n",
    "#     out = ov_vars\n",
    "#     # targetfile has to be a copy of the original file\n",
    "#     targetfile = '/raid60/scratch/uli/resorted_files_20201109/'+fn.split('/')[-1]# 0-20000-0-63894_CEUAS_merged_v0.nc'\n",
    "#     if os.path.isfile(targetfile):\n",
    "#         mode='r+'\n",
    "#     else:\n",
    "#         mode='w'\n",
    "        \n",
    "#     print()\n",
    "#     print('writing '+targetfile)\n",
    "#     #\n",
    "#     # writing data into observations_table\n",
    "#     #\n",
    "#     with h5py.File(targetfile, mode) as file:\n",
    "#         for i in range(len(keys)):\n",
    "#             try:\n",
    "#                 del file['observations_table'][keys[i]]\n",
    "#             except:\n",
    "#                 pass\n",
    "#         for i in range(len(fbkeys)):\n",
    "#             try:\n",
    "#                 del file['era5fb'][fbkeys[i]]\n",
    "#             except:\n",
    "#                 pass\n",
    "#     for i in range(len(keys)):\n",
    "#         if keys[i] == 'index':\n",
    "#             pass\n",
    "#         elif keys[i] == 'observation_id' or keys[i] == 'report_id' or keys[i] == 'sensor_id' or keys[i] == 'source_id':\n",
    "# #             slen = len(out[i][0])\n",
    "#             alldict = {keys[i]:np.asarray(out[i], dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({keys[i]:out[i]})\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])  \n",
    "            \n",
    "#     for i in range(len(fbkeys)):\n",
    "#         if fbkeys[i] == 'index' or fbkeys[i] == 'string6' or fbkeys[i] == 'string7' or fbkeys[i] == 'string10':\n",
    "#             pass\n",
    "#         elif fbkeys[i] == 'expver' or fbkeys[i] == 'source@hdr' or fbkeys[i] == 'source_id' or fbkeys[i] == 'statid@hdr':\n",
    "# #             slen = len(out[i][0])\n",
    "#             alldict = {fbkeys[i]:np.asarray(fb_vars[i], dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({fbkeys[i]:fb_vars[i]})\n",
    "#             write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]]) \n",
    "#     #\n",
    "#     # writing the recordindices and recordtimestamp.\n",
    "#     #       \n",
    "#     recordindices=vridx\n",
    "#     with h5py.File(targetfile, 'r+') as file:\n",
    "#         for i in range(len(recordindices)):\n",
    "#             try:\n",
    "#                 del file['recordindices'][str(allvars[i])]\n",
    "#             except:\n",
    "#                 pass\n",
    "#     for i in range(len(recordindices)):\n",
    "#         testvar = pandas.DataFrame({str(allvars[i]):recordindices[i]})\n",
    "#         write_dict_h5(targetfile, testvar, 'recordindices', {str(allvars[i]): { 'compression': None } }, [str(allvars[i])]) \n",
    "\n",
    "#     with h5py.File(targetfile, 'r+') as file:\n",
    "#         try:\n",
    "#             del file['recordindex']\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             del file['recordtimestamp']\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             del file['recordindices']['recordtimestamp']\n",
    "#         except:\n",
    "#             pass\n",
    "#     write_dict_h5(targetfile, {'recordtimestamp':rt}, 'recordindices', {'recordtimestamp': { 'compression': None } }, ['recordtimestamp']) \n",
    "        \n",
    "#     print('elapsed:',time.time()-tt)\n",
    "#     print(end='')\n",
    "\n",
    "\n",
    "\n",
    "# # In[15]:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_resort(fn):\n",
    "#     targetfile = '/raid60/scratch/uli/resorted_files/'+fn.split('/')[-1]  \n",
    "    \n",
    "#     with h5py.File(fn, 'r') as file:\n",
    "#         with h5py.File(targetfile, 'w') as newfile:\n",
    "#             newfile.copy(file['header_table'],newfile)\n",
    "#             newfile.copy(file['sensor_configuration'],newfile)\n",
    "#             newfile.copy(file['crs'],newfile)\n",
    "#             newfile.copy(file['observed_variable'],newfile)\n",
    "#             newfile.copy(file['source_configuration'],newfile)\n",
    "#             newfile.copy(file['station_configuration'],newfile)\n",
    "#             newfile.copy(file['station_type'],newfile)\n",
    "#             newfile.copy(file['station_configuration_codes'],newfile)\n",
    "#             newfile.copy(file['units'],newfile)\n",
    "#             newfile.copy(file['z_coordinate_type'],newfile)\n",
    "            \n",
    "#             newfile.create_dataset('dateindex', data=file['dateindex'][:]) \n",
    "\n",
    "    \n",
    "#     data =  eua.CDMDataset(fn)\n",
    "#     allvars = data.observations_table.observed_variable[()]\n",
    "#     allvars.sort()\n",
    "#     allvars = numpy.unique(allvars)\n",
    "#     #\n",
    "#     ri = data.recordindex[()]\n",
    "# #     print('recordindex: ', len(ri))\n",
    "#     rt = data.recordtimestamp[()]\n",
    "#     keys = data.observations_table.keys()[:-1]\n",
    "#     fbkeys = data.era5fb.keys()[:-1]\n",
    "#     # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "#     pops = []\n",
    "#     for i in range(len(keys)):\n",
    "#         if 'string' in keys[i]:\n",
    "#             pops.append(keys[i])\n",
    "#     for i in pops: keys.remove(i)\n",
    "#     pops = []\n",
    "#     for i in range(len(fbkeys)):\n",
    "#         if 'string' in fbkeys[i]:\n",
    "#             pops.append(fbkeys[i])\n",
    "#     for i in pops: fbkeys.remove(i)\n",
    "\n",
    "#     recordindices = [[] for i in range(len(allvars))]\n",
    "#     recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "#     # output variables (from observations_table)\n",
    "#     ov = []\n",
    "#     for o in keys:\n",
    "#         ov.append([[] for i in range(len(allvars))])\n",
    "#     fb = []\n",
    "#     for o in fbkeys:\n",
    "#         fb.append([[] for i in range(len(allvars))])\n",
    "#     #\n",
    "#     # loading the observed_variables\n",
    "#     #\n",
    "#     obsv = data.observations_table.observed_variable[:]\n",
    "#     #\n",
    "#     # resorting the data\n",
    "#     #\n",
    "# #     print('resort:start')\n",
    "#     @njit\n",
    "#     def make_vrindex(vridx,ridx,idx):\n",
    "#         l=0\n",
    "#         for i in range(1,len(idx)): # to set the recordindices\n",
    "#             if ridx[i]>ridx[i-1]:\n",
    "#                 vridx[ridx[i-1]:ridx[i]]=l # next record after l\n",
    "#                 l=i\n",
    "#         vridx[ridx[i]:]=len(idx) # next record for the last element is the len of the data\n",
    "\n",
    "\n",
    "#     tt=time.time()\n",
    "\n",
    "#     ridxall=np.zeros(obsv.shape[0],dtype=np.int64) # reverse index - index of the record index\n",
    "#     j=-1\n",
    "#     for j in range(len(ri)-1):\n",
    "#         ridxall[ri[j]:ri[j+1]]=j\n",
    "#     j+=1\n",
    "#     ridxall[ri[j]:]=j # for the last elemenet\n",
    "#     ridx=[]\n",
    "#     vridx=[]\n",
    "#     absidx=[]\n",
    "#     abscount=0\n",
    "#     for j in range(len(allvars)):\n",
    "#         idx=np.where(obsv==allvars[j])[0] # index of all elements form certain variable j\n",
    "# #         print(j,len(idx),',',end='')\n",
    "#         vridx.append(np.zeros(ri.shape[0],dtype=np.int64)) # all zeros in lenght of record index\n",
    "#         ridx=ridxall[idx] # ridxall where variable is j\n",
    "#         make_vrindex(vridx[-1],ridx,idx)\n",
    "#         vridx[-1]+=abscount # abscount for stacking the recordindex\n",
    "\n",
    "#         absidx.append(copy.copy(idx)) # why copy? - to make sure it's not just the ref. - maybe ok without the cp\n",
    "#         abscount+=len(idx)\n",
    "\n",
    "# #     print('')\n",
    "#     #\n",
    "#     # finishing the sorting \n",
    "#     #\n",
    "#     absidx=np.concatenate(absidx)\n",
    "#     #\n",
    "#     # recordtimestamps are only necessary once\n",
    "#     #\n",
    "#     recordtimestamps = recordtimestamps[0]\n",
    "#     #\n",
    "#     # targetfile has to be a copy of the original file\n",
    "#     #\n",
    "#     targetfile = '/raid60/scratch/uli/resorted_files/'+fn.split('/')[-1]# 0-20000-0-63894_CEUAS_merged_v0.nc'\n",
    "#     if os.path.isfile(targetfile):\n",
    "#         mode='r+'\n",
    "#     else:\n",
    "#         mode='w'\n",
    "# #     print()\n",
    "# #     print('writing '+targetfile)\n",
    "\n",
    "#     #\n",
    "#     # writing data into observations_table\n",
    "#     #\n",
    "# #     with h5py.File(targetfile, mode) as file:\n",
    "# #         for i in range(len(keys)):\n",
    "# #             try:\n",
    "# #                 del file['observations_table'][keys[i]]\n",
    "# #             except:\n",
    "# #                 pass\n",
    "# #         for i in range(len(fbkeys)):\n",
    "# #             try:\n",
    "# #                 del file['era5fb'][fbkeys[i]]\n",
    "# #             except:\n",
    "# #                 pass\n",
    "#     for i in range(len(keys)):\n",
    "#         ov_vars = data.observations_table[keys[i]][:]\n",
    "#         ov_vars = ov_vars[absidx]\n",
    "#         if keys[i] == 'index':\n",
    "#             pass\n",
    "#         elif keys[i] == 'observation_id' or keys[i] == 'report_id' or keys[i] == 'sensor_id' or keys[i] == 'source_id':\n",
    "#             alldict = {keys[i]:np.asarray(ov_vars, dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({keys[i]:ov_vars})\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])  \n",
    "\n",
    "#     for i in range(len(fbkeys)):\n",
    "#         fb_vars = data.era5fb[fbkeys[i]][:]\n",
    "#         fb_vars = fb_vars[absidx]\n",
    "#         if fbkeys[i] == 'index' or fbkeys[i] == 'string6' or fbkeys[i] == 'string7' or fbkeys[i] == 'string10':\n",
    "#             pass\n",
    "#         elif fbkeys[i] == 'expver' or fbkeys[i] == 'source@hdr' or fbkeys[i] == 'source_id' or fbkeys[i] == 'statid@hdr':\n",
    "#             alldict = {fbkeys[i]:np.asarray(fb_vars, dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({fbkeys[i]:fb_vars})\n",
    "#             write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]]) \n",
    "#     #\n",
    "#     # writing the recordindices and recordtimestamp.\n",
    "#     #       \n",
    "#     recordindices=vridx\n",
    "# #     with h5py.File(targetfile, 'w') as file:\n",
    "# #         for i in range(len(recordindices)):\n",
    "# #             try:\n",
    "# #                 del file['recordindices'][str(allvars[i])]\n",
    "# #             except:\n",
    "# #                 pass\n",
    "#     for i in range(len(recordindices)):\n",
    "#         testvar = pandas.DataFrame({str(allvars[i]):recordindices[i]})\n",
    "#         write_dict_h5(targetfile, testvar, 'recordindices', {str(allvars[i]): { 'compression': None } }, [str(allvars[i])]) \n",
    "\n",
    "# #     with h5py.File(targetfile, 'w') as file:\n",
    "# #         try:\n",
    "# #             del file['recordindex']\n",
    "# #         except:\n",
    "# #             pass\n",
    "# #         try:\n",
    "# #             del file['recordtimestamp']\n",
    "# #         except:\n",
    "# #             pass\n",
    "# #         try:\n",
    "# #             del file['recordindices']['recordtimestamp']\n",
    "# #         except:\n",
    "# #             pass\n",
    "#     write_dict_h5(targetfile, {'recordtimestamp':rt}, 'recordindices', {'recordtimestamp': { 'compression': None } }, ['recordtimestamp'])\n",
    "# #     with h5py.File(targetfile, 'w') as newfile:\n",
    "# #         for i in data.header_table.keys():\n",
    "# #             alldict = pandas.DataFrame({i:data.header_table[i]})\n",
    "# #             write_dict_h5(targetfile, alldict, 'header_table', {i: { 'compression': 'gzip' } }, [i])\n",
    "\n",
    "#     print('elapsed:',time.time()-tt)\n",
    "# #     print(end='')\n",
    "\n",
    "\n",
    "\n",
    "# # In[15]:\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Miniconda3 (4.8.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
