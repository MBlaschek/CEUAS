{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import sys\n",
    "import zipfile, os, time\n",
    "import urllib3\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import h5py\n",
    "import plotly.express as px\n",
    "fs = open('path.txt', 'r')\n",
    "path = fs.read()  \n",
    "fs.close()\n",
    "sys.path.append(path)\n",
    "\n",
    "import cds_eua3 as eua\n",
    "eua.logging_set_level(30)\n",
    "import xarray as xr\n",
    "\n",
    "import cdsapi, zipfile, os, time\n",
    "import schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "def write_dict_h5(dfile, f, k, fbencodings, var_selection=[], mode='a', attrs={}): \n",
    "    \"\"\" Writes each separate variable from the observation or feedback tables inot netcdf using h5py.\n",
    "          f is a pandas dataframe with one column, one for each variable\n",
    "          k is either 'era5fb' or 'observations_table'\n",
    "          fbencodings is the encodings of variable types, e.g. {'observations_id': { 'compression': 'gzip' } }\n",
    "    \"\"\"\n",
    "\n",
    "    #attrs=  {'date_time':('units','seconds since 1900-01-01 00:00:00')}\n",
    "    #attrs = {'observation_id': ('description', 'unique ID for observation'), 'report_id': ('description', 'Link to header information') , 'date_time':('units','seconds since 1900-01-01 00:00:00') }\n",
    "    \n",
    "    with h5py.File(dfile,mode) as fd:\n",
    "        if k == '':\n",
    "            v = var_selection[0]\n",
    "            fd.create_dataset(v,f[v].shape,f[v].dtype,compression=fbencodings[v]['compression'], chunks=True)\n",
    "            fd[v][:]=f[v].values[:]\n",
    "        else:\n",
    "            try:\n",
    "                fd.create_group(k)\n",
    "                index=numpy.zeros (f[list(f.keys())[0]].shape[0], dtype='S1')\n",
    "                fd[k].create_dataset('index', data=index)\n",
    "            except:\n",
    "                pass\n",
    "            if not var_selection:\n",
    "                var_selection=list(f.keys())\n",
    "            fixed_string_len = 20\n",
    "            string10=numpy.zeros(fixed_string_len,dtype='S1')\n",
    "            sdict={}\n",
    "            slist=[]\n",
    "\n",
    "            #groupencodings     \n",
    "\n",
    "            for v in var_selection:          \n",
    "                #variables_dic[v] = ''\n",
    "                if type(f[v]) == pd.core.series.Series:\n",
    "                    fvv=f[v].values\n",
    "                else:\n",
    "                    fvv=f[v]\n",
    "\n",
    "                if type(fvv[0]) not in [str,bytes,numpy.bytes_]:\n",
    "\n",
    "                    if fvv.dtype !='S1':\n",
    "\n",
    "                        fd[k].create_dataset(v,fvv.shape,fvv.dtype,compression=fbencodings[v]['compression'], chunks=True)\n",
    "                        fd[k][v][:]=fvv[:]\n",
    "                        if attrs:    #  attrs={'date_time':('units','seconds since 1900-01-01 00:00:00')}\n",
    "                            if v in attrs.keys():\n",
    "                                for kk,vv in attrs[v].items():\n",
    "                                    if type(vv) is str:  \n",
    "                                        fd[k][v].attrs[kk]=numpy.bytes_(vv)\n",
    "                                    else:\n",
    "                                        fd[k][v].attrs[kk]=vv\n",
    "\n",
    "                        if v in ['date_time','report_timestamp','record_timestamp']:\n",
    "                            fd[k][v].attrs['units']=numpy.bytes_('seconds since 1900-01-01 00:00:00')                            #print (  fk, ' ' , v , ' ' ,   ) \n",
    "\n",
    "                    else:\n",
    "                        fd[k].create_dataset(v,fvv.shape,fvv.dtype,compression=fbencodings[v]['compression'], chunks=True)\n",
    "                        fd[k][v][:]=fvv[:]\n",
    "                        slen=fvv.shape[1]\n",
    "                        sdict[v]=slen\n",
    "                        if slen not in slist:\n",
    "                            slist.append(slen)\n",
    "                            try:\n",
    "                                fd[k].create_dataset( 'string{}'.format(slen),  data=string10[:slen]  )\n",
    "                            except:\n",
    "                                pass               \n",
    "                        if v in attrs.keys():\n",
    "                            fd[k][v].attrs['description']=numpy.bytes_(attrs[v]['description'])\n",
    "                            fd[k][v].attrs['external_table']=numpy.bytes_(attrs[v]['external_table'])\n",
    "\n",
    "                else:\n",
    "                    sleno=len(fvv[0])\n",
    "                    slen=sleno\n",
    "                    try:\n",
    "                        slen=int(fvv.dtype.descr[0][1].split('S')[1])\n",
    "                    except:  \n",
    "                        pass\n",
    "\n",
    "                    sdict[v]=slen\n",
    "                    if slen not in slist:\n",
    "                        slist.append(slen)\n",
    "                        try:\n",
    "                            print('trying to cread dim')\n",
    "                            fd[k].create_dataset( 'string{}'.format(slen),  data=string10[:slen]  )\n",
    "                            print('dim created')\n",
    "                        except:\n",
    "                            print('dim creation failed')\n",
    "                            pass               \n",
    "\n",
    "                    #x=x.reshape(fvv.shape[0],slen)\n",
    "                    fd[k].create_dataset(v,data=fvv.view('S1').reshape(fvv.shape[0],slen),compression=fbencodings[v]['compression'],chunks=True)\n",
    "                    if v in attrs.keys():\n",
    "                        fd[k][v].attrs['description']     =numpy.bytes_(attrs[v]['description'])\n",
    "                        fd[k][v].attrs['external_table']=numpy.bytes_(attrs[v]['external_table'])                \n",
    "\n",
    "                #variables_dic[v] = f[v].values.dtype\n",
    "\n",
    "            for v in fd[k].keys(): #var_selection:\n",
    "                l=0      \n",
    "\n",
    "                '''\n",
    "                if v == 'primary_station_id':\n",
    "                    try:\n",
    "                        fd[k][v].dims[l].attach_scale(fd[k]['index'])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    try:\n",
    "                        slen =  len( fd[k][v][0] )\n",
    "                        stringa=numpy.zeros( slen , dtype='S1')\n",
    "                        fd[k].create_dataset( 'string{}'.format(slen),  data= stringa  )                                        \n",
    "                        fd[k][v].dims[1].attach_scale(   fd[k]['string{}'.format(slen)]  )                                        \n",
    "                    except:\n",
    "                        fd[k][v].dims[1].attach_scale(   fd[k]['string{}'.format(slen)]  )                    \n",
    "\n",
    "\n",
    "                if v == 'station_name':\n",
    "                    try:\n",
    "                        fd[k][v].dims[l].attach_scale(fd[k]['index'])\n",
    "                        slen =  len( fd[k][v][0][0])\n",
    "                        stringa=numpy.zeros( slen , dtype='S1')\n",
    "                    except:\n",
    "                        pass\n",
    "                    try:\n",
    "                        fd[k].create_dataset( 'string{}'.format(slen),  data= stringa  )\n",
    "                        fd[k][v].dims[1].attach_scale(   fd[k]['string{}'.format(slen)]  )                    \n",
    "                        print('done attaching')\n",
    "                    except:\n",
    "                        print('not working')\n",
    "\n",
    "                '''             \n",
    "                try:\n",
    "                    if type(f[v]) == pd.core.series.Series:\n",
    "                        fvv=f[v].values\n",
    "                    else:\n",
    "                        fvv=f[v]\n",
    "                    if 'string' not in v and v!='index':                    \n",
    "                        fd[k][v].dims[l].attach_scale(fd[k]['index'])\n",
    "                        #print(v,fvv.ndim,type(fvv[0]))\n",
    "                        if fvv.ndim==2 or type(fvv[0]) in [str,bytes,numpy.bytes_]:\n",
    "                            slen=sdict[v]\n",
    "                            #slen=10\n",
    "                            fd[k][v].dims[1].attach_scale(fd[k]['string{}'.format(slen)])\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "\n",
    "            i=4        \n",
    "            for v in slist:\n",
    "                s='string{}'.format(v)\n",
    "                for a in ['NAME']:\n",
    "                    fd[k][s].attrs[a]=numpy.bytes_('This is a netCDF dimension but not a netCDF variable.')\n",
    "\n",
    "                i+=1\n",
    "\n",
    "    return\n",
    "#variables_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/raid60/scratch/uli/0-20000-0-63894_CEUAS_merged_v0.nc'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# files = glob.glob('/raid60/scratch/federico/MERGED_DATABASE_OCTOBER2020_sensor/0-20000-0-11035*.nc')\n",
    "files = glob.glob('/raid60/scratch/uli/0-20000-0-6*.nc')\n",
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File: <HDF5 file \"0-20000-0-63894_CEUAS_merged_v0.nc\" (mode r)>\n",
       "Filename: /raid60/scratch/uli/0-20000-0-63894_CEUAS_merged_v0.nc\n",
       "(G)roups/(V)ariables: \n",
       "\n",
       " - G | crs__________________________________________ : : 4\n",
       " - V | dateindex____________________________________ : : (10515,)\n",
       " - G | era5fb_______________________________________ : : 72\n",
       " - G | header_table_________________________________ : : 56\n",
       " - G | observations_table___________________________ : : 50\n",
       " - G | observed_variable____________________________ : : 9\n",
       " - V | recordindex__________________________________ : : (15942,)\n",
       " - V | recordtimestamp______________________________ : : (15942,)\n",
       " - G | sensor_configuration_________________________ : : 12\n",
       " - G | source_configuration_________________________ : : 2\n",
       " - G | station_configuration________________________ : : 48\n",
       " - G | station_configuration_codes__________________ : : 7\n",
       " - G | station_type_________________________________ : : 4\n",
       " - G | units________________________________________ : : 6\n",
       " - G | z_coordinate_type____________________________ : : 4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = eua.CDMDataset(files[0])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  34,  36,  38,  39,  85, 104, 105, 106, 107, 117, 136, 137,\n",
       "       138, 139, 140])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allvars = data.observations_table.observed_variable[()]\n",
    "np.unique(allvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recordindex:  15942\n",
      "loading data\n",
      "resort:start\n",
      "resort:done\n",
      "shaping record_indices\n",
      "stacking output variables\n",
      "restoring byte arrays\n"
     ]
    }
   ],
   "source": [
    "allvars = data.observations_table.observed_variable[()]\n",
    "allvars.sort()\n",
    "allvars = np.unique(allvars)\n",
    "#\n",
    "ri = data.recordindex[()]\n",
    "print('recordindex: ', len(ri))\n",
    "# rt = data.recordtimestamp[()]\n",
    "keys = data.observations_table.keys()[:-1]\n",
    "# dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "keys.pop(43)\n",
    "keys.pop(42)\n",
    "keys.pop(41)\n",
    "recordindices = [[] for i in range(len(allvars))]\n",
    "recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "# output variables (from observations_table)\n",
    "ov = []\n",
    "for o in keys:\n",
    "    ov.append([[] for i in range(len(np.unique(allvars)))])\n",
    "\n",
    "#\n",
    "# load data into memory and decode byte arrays\n",
    "#\n",
    "print('loading data')\n",
    "obsv = data.observations_table.observed_variable[:]\n",
    "ov_vars = []\n",
    "for o in range(len(keys)):\n",
    "    ov_vars.append(data.observations_table[keys[o]][:])\n",
    "    if keys[o] == 'observation_id' or keys[o] == 'report_id' or keys[o] == 'sensor_id' or keys[o] == 'source_id':\n",
    "        b = []\n",
    "        for n in ov_vars[o]:\n",
    "            c = ''\n",
    "            for bb in n:\n",
    "                c = c + bb.decode()\n",
    "            b.append(c)\n",
    "        ov_vars[o] = b\n",
    "\n",
    "#\n",
    "# resorting the data\n",
    "#\n",
    "print('resort:start')\n",
    "for i in range(len(ri)):\n",
    "    try:\n",
    "        start = ri[i]\n",
    "        end = ri[i+1]\n",
    "    except:\n",
    "        start = ri[i]\n",
    "        end = len(data.observations_table.observed_variable[()])\n",
    "        \n",
    "    a = obsv[start:end]\n",
    "    helpvar = []\n",
    "    for o in range(len(keys)):\n",
    "        b = ov_vars[o][start:end]\n",
    "        sa, sb = zip(*sorted(zip(a, b)))\n",
    "        helpvar.append(sb)\n",
    "\n",
    "    for j in range(len(allvars)):\n",
    "        for k in range(len(sa)):\n",
    "            if sa[k] == allvars[j]:\n",
    "                for m in range(len(helpvar)):\n",
    "                    ov[m][j].append(helpvar[m][k])\n",
    "        recordindices[j].append(len(ov[0][j]))\n",
    "#         recordtimestamps[j].append(rt[i])\n",
    "\n",
    "print('resort:done')\n",
    "        \n",
    "#\n",
    "# setting record_indices to the right value -> stacking them\n",
    "#\n",
    "lenadd = 0.\n",
    "for i in range(len(recordindices)):\n",
    "    recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "    lenadd += len(ov[0][i])\n",
    "\n",
    "#\n",
    "# shaping record_indices -> setting every missing value to nan\n",
    "#\n",
    "print('shaping record_indices')\n",
    "old = -1\n",
    "for i in range(len(recordindices)):\n",
    "    for j in range(len(recordindices[i])):\n",
    "        if recordindices[i][j] != 0 and recordindices[i][j] == recordindices[i][j-1]:\n",
    "            old = recordindices[i][j]\n",
    "            recordindices[i][j] = np.nan\n",
    "        elif recordindices[i][j] == old:\n",
    "            recordindices[i][j] = np.nan\n",
    "\n",
    "#\n",
    "# recordtimestamps are only necessary once\n",
    "#\n",
    "recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "# \n",
    "# stacking all output variables\n",
    "#\n",
    "print('stacking output variables')\n",
    "out = []\n",
    "for j in ov:\n",
    "    finalvar = []\n",
    "    for i in j:\n",
    "        finalvar = finalvar + i\n",
    "    out.append(finalvar)\n",
    "    \n",
    "#\n",
    "# restoring byte arrays:\n",
    "# this takes very long -> find faster option\n",
    "#\n",
    "print('restoring byte arrays')\n",
    "bytelist = [25, 34, 38, 39]\n",
    "for o in bytelist:\n",
    "    originallen = len(data.observations_table[keys[o]][()][0])\n",
    "    for n in range(len(out[o])):\n",
    "        c = [elem.encode() for elem in out[o][n]]\n",
    "        # problem: if the string was [b'x', b'y', b''] in the first place, it get converted to 'xy' and then back to [b'x', b'y']\n",
    "        # add empty byte strings until the data is as long as before:\n",
    "        while(len(c) < originallen):\n",
    "            c.append(str('').encode())\n",
    "        out[o][n] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targetfile has to be a copy of the original file\n",
    "targetfile = '/raid60/scratch/uli/testoutputfile3.nc'\n",
    "file = h5py.File(targetfile, 'r+')\n",
    "\n",
    "#\n",
    "# writing data into observations_table\n",
    "#\n",
    "for i in range(len(keys)):\n",
    "    try:\n",
    "        del file['observations_table'][keys[i]]\n",
    "    except:\n",
    "        pass\n",
    "    if keys[i] == 'index':\n",
    "        pass\n",
    "    elif keys[i] == 'observation_id' or keys[i] == 'report_id' or keys[i] == 'sensor_id' or keys[i] == 'source_id':\n",
    "        slen = len(out[i][0])\n",
    "        alldict = {keys[i]:np.asarray(out[i], dtype='S{}'.format(slen))}\n",
    "        write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])\n",
    "    else:\n",
    "        alldict = pandas.DataFrame({keys[i]:out[i]})\n",
    "        write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])  \n",
    "#\n",
    "# writing the recordindices and recordtimestamp.\n",
    "#       \n",
    "for i in range(len(recordindices)):\n",
    "    try:\n",
    "        del file['recordindices'][str(allvars[i])]\n",
    "    except:\n",
    "        pass\n",
    "    testvar = pandas.DataFrame({str(allvars[i]):recordindices[i]})\n",
    "    write_dict_h5(targetfile, testvar, 'recordindices', {str(allvars[i]): { 'compression': 'gzip' } }, [str(allvars[i])]) \n",
    "\n",
    "try:\n",
    "    del file['recordindex']\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File: <HDF5 file \"testoutputfile3.nc\" (mode r+)>\n",
       "Filename: /raid60/scratch/uli/testoutputfile3.nc\n",
       "(G)roups/(V)ariables: \n",
       "\n",
       " - G | crs__________________________________________ : : 4\n",
       " - V | dateindex____________________________________ : : (10515,)\n",
       " - G | era5fb_______________________________________ : : 72\n",
       " - G | header_table_________________________________ : : 56\n",
       " - G | observations_table___________________________ : : 49\n",
       " - G | observed_variable____________________________ : : 9\n",
       " - G | recordindices________________________________ : : 17\n",
       " - V | recordtimestamp______________________________ : : (15942,)\n",
       " - G | sensor_configuration_________________________ : : 12\n",
       " - G | source_configuration_________________________ : : 2\n",
       " - G | station_configuration________________________ : : 48\n",
       " - G | station_configuration_codes__________________ : : 7\n",
       " - G | station_type_________________________________ : : 4\n",
       " - G | units________________________________________ : : 6\n",
       " - G | z_coordinate_type____________________________ : : 4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = eua.CDMDataset(targetfile)\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recordindices:\n",
       "\n",
       "0_________________________________________________ : : (15942,)\n",
       "104_______________________________________________ : : (15942,)\n",
       "105_______________________________________________ : : (15942,)\n",
       "106_______________________________________________ : : (15942,)\n",
       "107_______________________________________________ : : (15942,)\n",
       "117_______________________________________________ : : (15942,)\n",
       "136_______________________________________________ : : (15942,)\n",
       "137_______________________________________________ : : (15942,)\n",
       "138_______________________________________________ : : (15942,)\n",
       "139_______________________________________________ : : (15942,)\n",
       "140_______________________________________________ : : (15942,)\n",
       "34________________________________________________ : : (15942,)\n",
       "36________________________________________________ : : (15942,)\n",
       "38________________________________________________ : : (15942,)\n",
       "39________________________________________________ : : (15942,)\n",
       "85________________________________________________ : : (15942,)\n",
       "index_____________________________________________ : : (15942,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.recordindices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  34,  36,  36,  36,  36,  36,  36,  36,  36,  36,  36,  36,\n",
       "        36,  36,  36,  36,  36,  36,  36,  36,  36,  36,  36,  36,  36,\n",
       "        36,  36,  36,  36,  36,  36,  36,  36,  36,  36,  36,  36,  36,\n",
       "        36,  36,  36,  36,  36,  36,  36,  36,  36,  36,  36,  36,  36,\n",
       "        36,  36,  36,  36,  36,  36,  36,  36,  36,  36,  36,  36,  38,\n",
       "        38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,\n",
       "        38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,\n",
       "        38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,\n",
       "        38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,\n",
       "        38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,\n",
       "        38,  39,  39,  39,  39,  39,  39,  39,  39,  39,  39,  39,  39,\n",
       "        39,  39,  39,  39,  39,  39,  39,  39,  39,  39,  39,  39,  39,\n",
       "        39,  39,  39,  39,  39,  39,  39,  39,  39,  39,  39,  39,  39,\n",
       "        39,  39,  39,  39,  39,  39,  39,  39,  39,  39,  39,  39,  39,\n",
       "        39,  39,  39,  39,  39,  39,  39,  39,  39,  39,  39,  39,  39,\n",
       "        85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,\n",
       "        85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,\n",
       "        85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,\n",
       "        85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,\n",
       "        85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,\n",
       "        85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,\n",
       "        85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,\n",
       "        85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,\n",
       "        85,  85,  85,  85,  85,  85, 104, 104, 104, 104, 104, 104, 104,\n",
       "       104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104,\n",
       "       104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104,\n",
       "       104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104,\n",
       "       104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104,\n",
       "       104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104,\n",
       "       104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104,\n",
       "       104, 104, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105,\n",
       "       105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105,\n",
       "       105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105,\n",
       "       105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105,\n",
       "       105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105,\n",
       "       105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105,\n",
       "       105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 106,\n",
       "       106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106,\n",
       "       106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106,\n",
       "       106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106,\n",
       "       106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106,\n",
       "       106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106,\n",
       "       106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106,\n",
       "       106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106,\n",
       "       106, 106, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,\n",
       "       107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,\n",
       "       107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,\n",
       "       107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,\n",
       "       107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,\n",
       "       107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,\n",
       "       107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,\n",
       "       107, 107, 107, 107, 107, 117, 117, 117, 117, 117, 117, 117, 117,\n",
       "       117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117,\n",
       "       117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117,\n",
       "       117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117,\n",
       "       117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117,\n",
       "       117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117,\n",
       "       117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117,\n",
       "       117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117,\n",
       "       117, 117, 117, 117, 117, 117, 117, 117, 117, 136, 136, 136, 136,\n",
       "       137, 137, 137, 137, 138, 138, 138, 138, 139, 139, 139, 139, 140,\n",
       "       140, 140, 140, 140])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.observations_table.observed_variable[0:-1:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allvars = data.observations_table.observed_variable[()]\n",
    "# allvars.sort()\n",
    "# allvars = np.unique(allvars)\n",
    "# # recordindices einzeln sortieren - für alle variablen eigene listen erstellen - dazu recordindices erstellen - alle daten hintereinander legen - len aller vorhergehender rec_in zu den nachkommenden addieren.\n",
    "# #\n",
    "# ri = data.recordindex[()]\n",
    "# rt = data.recordtimestamp[()]\n",
    "# keys = data.observations_table.keys()\n",
    "# recordindices = [[] for i in range(len(np.unique(allvars)))]\n",
    "# recordtimestamps = [[] for i in range(len(np.unique(allvars)))]\n",
    "\n",
    "# variables = [[] for i in range(len(np.unique(allvars)))]\n",
    "# values = [[] for i in range(len(np.unique(allvars)))]\n",
    "# zcoords = [[] for i in range(len(np.unique(allvars)))]\n",
    "\n",
    "# for i in range(len(ri)):\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         break\n",
    "        \n",
    "#     a = data.observations_table.observed_variable[()][start:end]\n",
    "#     b = data.observations_table.observation_value[()][start:end]\n",
    "#     c = data.observations_table.z_coordinate[()][start:end]\n",
    "\n",
    "#     sa, sb = zip(*sorted(zip(a, b)))\n",
    "#     sa, sc = zip(*sorted(zip(a, c)))\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 variables[j].append(sa[k])\n",
    "#                 values[j].append(sb[k])\n",
    "#                 zcoords[j].append(sc[k])\n",
    "#         recordindices[j].append(len(variables[j]))\n",
    "#         recordtimestamps[j].append(rt[i])\n",
    "        \n",
    "# lenadd = 0\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(variables[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allvars = data.observations_table.observed_variable[()]\n",
    "# allvars.sort()\n",
    "# allvars = np.unique(allvars)\n",
    "# #\n",
    "# ri = data.recordindex[()]\n",
    "# rt = data.recordtimestamp[()]\n",
    "# keys = data.observations_table.keys()[:-1]\n",
    "# recordindices = [[] for i in range(len(np.unique(allvars)))]\n",
    "# recordtimestamps = [[] for i in range(len(np.unique(allvars)))]\n",
    "\n",
    "# # output variables (from observations_table)\n",
    "# ov = []\n",
    "# for o in keys:\n",
    "#     ov.append([[] for i in range(len(np.unique(allvars)))])\n",
    "    \n",
    "# for i in range(len(ri)):\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         break\n",
    "        \n",
    "#     a = data.observations_table.observed_variable[()][start:end]\n",
    "#     helpvar = []\n",
    "#     for o in range(len(keys)):\n",
    "#         b = data.observations_table[keys[o]][()][start:end]\n",
    "#         try: # all varibales with different shape won't work here\n",
    "#             sa, sb = zip(*sorted(zip(a, b)))\n",
    "#             helpvar.append(sb)\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 for m in range(len(helpvar)):\n",
    "#                     try: # all varibales with different shape won't work here\n",
    "#                         ov[m][j].append(helpvar[m][k])\n",
    "#                     except:\n",
    "#                         pass\n",
    "#         recordindices[j].append(len(ov[0][j]))\n",
    "#         recordtimestamps[j].append(rt[i])\n",
    "        \n",
    "# lenadd = 0\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(variables[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# #\n",
    "# allvars = data.observations_table.observed_variable[()]\n",
    "# allvars.sort()\n",
    "# allvars = np.unique(allvars)\n",
    "# #\n",
    "# ri = data.recordindex[()]\n",
    "# print(len(ri))\n",
    "# # rt = data.recordtimestamp[()]\n",
    "# keys = data.observations_table.keys()[:-1]\n",
    "# # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "# keys.pop(43)\n",
    "# keys.pop(42)\n",
    "# keys.pop(41)\n",
    "# recordindices = [[] for i in range(len(allvars))]\n",
    "# recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "# # output variables (from observations_table)\n",
    "# ov = []\n",
    "# for o in keys:\n",
    "#     ov.append([[] for i in range(len(np.unique(allvars)))])    \n",
    "    \n",
    "# print('resort:start')\n",
    "# for i in range(len(ri)):\n",
    "#     print(i)\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         start = ri[i]\n",
    "#         end = len(data.observations_table.observed_variable[()])\n",
    "        \n",
    "#     a = data.observations_table.observed_variable[()][start:end]\n",
    "#     helpvar = []\n",
    "#     for o in range(len(keys)):\n",
    "#         if keys[o] == 'observation_id' or keys[o] == 'report_id' or keys[o] == 'sensor_id' or keys[o] == 'source_id':\n",
    "#             b = []\n",
    "#             for n in data.observations_table[keys[o]][()][start:end]:\n",
    "#                 c = ''\n",
    "#                 for bb in n:\n",
    "#                     c = c + bb.decode()\n",
    "#                 b.append(c)\n",
    "#         else:\n",
    "#             b = data.observations_table[keys[o]][()][start:end]\n",
    "#         sa, sb = zip(*sorted(zip(a, b)))\n",
    "#         helpvar.append(sb)\n",
    "\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 for m in range(len(helpvar)):\n",
    "#                     ov[m][j].append(helpvar[m][k])\n",
    "#         recordindices[j].append(len(ov[0][j]))\n",
    "# #         recordtimestamps[j].append(rt[i])\n",
    "\n",
    "# print('resort:done')\n",
    "        \n",
    "# #\n",
    "# # setting record_indices to the right value -> stacking them\n",
    "# #\n",
    "# lenadd = 0.\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(ov[0][i])\n",
    "\n",
    "# #\n",
    "# # shaping record_indices -> setting every missing value to nan\n",
    "# #\n",
    "# old = -1\n",
    "# for i in range(len(recordindices)):\n",
    "#     for j in range(len(recordindices[i])):\n",
    "#         if recordindices[i][j] != 0 and recordindices[i][j] == recordindices[i][j-1]:\n",
    "#             old = recordindices[i][j]\n",
    "#             recordindices[i][j] = np.nan\n",
    "#         elif recordindices[i][j] == old:\n",
    "#             recordindices[i][j] = np.nan\n",
    "\n",
    "# #\n",
    "# # recordtimestamps are only necessary once\n",
    "# #\n",
    "# recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "# # \n",
    "# # stacking all output variables\n",
    "# #\n",
    "# out = []\n",
    "# for j in ov:\n",
    "#     finalvar = []\n",
    "#     for i in j:\n",
    "#         finalvar = finalvar + i\n",
    "#     out.append(finalvar)\n",
    "    \n",
    "# #\n",
    "# # restoring byte arrays:\n",
    "# #\n",
    "# for o in range(len(keys)):\n",
    "#     if keys[o] == 'observation_id' or keys[o] == 'report_id' or keys[o] == 'sensor_id' or keys[o] == 'source_id':\n",
    "#         b = []\n",
    "#         for n in range(len(out[o])):\n",
    "#             c = []\n",
    "#             for bb in out[o][n]:\n",
    "#                 c.append(bb.encode())\n",
    "#             out[o][n] = c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
