{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "import sys,glob\n",
    "import zipfile, os, time\n",
    "import urllib3\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import h5py\n",
    "import plotly.express as px\n",
    "sys.path.append(os.getcwd()+'/../cds-backend/code/')\n",
    "sys.path.append(os.getcwd()+'/../harvest/code/')\n",
    "from harvest_convert_to_netCDF_newfixes import write_dict_h5\n",
    "import cds_eua3 as eua\n",
    "eua.logging_set_level(30)\n",
    "import xarray as xr\n",
    "\n",
    "import cdsapi, zipfile, os, time\n",
    "#import schedule\n",
    "import copy\n",
    "from shutil import copyfile\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/raid60/scratch/leo/scratch/RI/Pangaea/nc/0-20200-0-01501.nc'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('/raid60/scratch/leo/scratch/RI/Pangaea/nc/0*.nc')\n",
    "# files = glob.glob('/raid60/scratch/uli/0*.nc')\n",
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_resort(fn):\n",
    "    targetfile = '/raid60/scratch/uli/resorted_files/'+fn.split('/')[-1]  \n",
    "    \n",
    "    with h5py.File(fn, 'r') as file:\n",
    "        with h5py.File(targetfile, 'w') as newfile:\n",
    "            try: newfile.copy(file['header_table'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['sensor_configuration'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['crs'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['observed_variable'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['source_configuration'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['station_configuration'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['station_type'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['station_configuration_codes'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['units'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['z_coordinate_type'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.create_dataset('dateindex', data=file['dateindex'][:]) \n",
    "            except: pass\n",
    "    \n",
    "    data =  eua.CDMDataset(fn)\n",
    "    allvars = data.observations_table.observed_variable[()]\n",
    "    allvars.sort()\n",
    "    allvars = numpy.unique(allvars)\n",
    "    #\n",
    "    ri = data.recordindex[()]\n",
    "#     print('recordindex: ', len(ri))\n",
    "    rt = data.recordtimestamp[()]\n",
    "    keys = data.observations_table.keys()[:-1]\n",
    "    fbkeys = data.era5fb.keys()[:-1]\n",
    "    # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "    pops = []\n",
    "    for i in range(len(keys)):\n",
    "        if 'string' in keys[i]:\n",
    "            pops.append(keys[i])\n",
    "    for i in pops: keys.remove(i)\n",
    "    pops = []\n",
    "    for i in range(len(fbkeys)):\n",
    "        if 'string' in fbkeys[i]:\n",
    "            pops.append(fbkeys[i])\n",
    "    for i in pops: fbkeys.remove(i)\n",
    "\n",
    "    recordindices = [[] for i in range(len(allvars))]\n",
    "    recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "    # output variables (from observations_table)\n",
    "    ov = []\n",
    "    for o in keys:\n",
    "        ov.append([[] for i in range(len(allvars))])\n",
    "    fb = []\n",
    "    for o in fbkeys:\n",
    "        fb.append([[] for i in range(len(allvars))])\n",
    "    #\n",
    "    # loading the observed_variables\n",
    "    #\n",
    "    obsv = data.observations_table.observed_variable[:]\n",
    "    #\n",
    "    # resorting the data\n",
    "    #\n",
    "#     print('resort:start')\n",
    "    @njit\n",
    "    def make_vrindex(vridx,ridx,idx):\n",
    "        l=0\n",
    "        for i in range(1,len(idx)): # to set the recordindices\n",
    "            if ridx[i]>ridx[i-1]:\n",
    "                vridx[ridx[i-1]:ridx[i]]=l # next record after l\n",
    "                l=i\n",
    "        vridx[ridx[i]:]=len(idx) # next record for the last element is the len of the data\n",
    "\n",
    "\n",
    "    tt=time.time()\n",
    "\n",
    "    ridxall=np.zeros(obsv.shape[0],dtype=np.int64) # reverse index - index of the record index\n",
    "    j=-1\n",
    "    for j in range(len(ri)-1):\n",
    "        ridxall[ri[j]:ri[j+1]]=j\n",
    "    j+=1\n",
    "    ridxall[ri[j]:]=j # for the last elemenet\n",
    "    ridx=[]\n",
    "    vridx=[]\n",
    "    absidx=[]\n",
    "    abscount=0\n",
    "    for j in range(len(allvars)):\n",
    "        idx=np.where(obsv==allvars[j])[0] # index of all elements form certain variable j\n",
    "#         print(j,len(idx),',',end='')\n",
    "        vridx.append(np.zeros(ri.shape[0],dtype=np.int64)) # all zeros in lenght of record index\n",
    "        ridx=ridxall[idx] # ridxall where variable is j\n",
    "        make_vrindex(vridx[-1],ridx,idx)\n",
    "        vridx[-1]+=abscount # abscount for stacking the recordindex\n",
    "\n",
    "        absidx.append(copy.copy(idx)) # why copy? - to make sure it's not just the ref. - maybe ok without the cp\n",
    "        abscount+=len(idx)\n",
    "\n",
    "#     print('')\n",
    "    #\n",
    "    # finishing the sorting \n",
    "    #\n",
    "    absidx=np.concatenate(absidx)\n",
    "    #\n",
    "    # recordtimestamps are only necessary once\n",
    "    #\n",
    "    recordtimestamps = recordtimestamps[0]\n",
    "    #\n",
    "    # targetfile has to be a copy of the original file\n",
    "    #\n",
    "    targetfile = '/raid60/scratch/uli/resorted_files/'+fn.split('/')[-1]# 0-20000-0-63894_CEUAS_merged_v0.nc'\n",
    "    if os.path.isfile(targetfile):\n",
    "        mode='r+'\n",
    "    else:\n",
    "        mode='w'\n",
    "#     print()\n",
    "#     print('writing '+targetfile)\n",
    "\n",
    "    for i in range(len(keys)):\n",
    "        ov_vars = data.observations_table[keys[i]][:]\n",
    "        ov_vars = ov_vars[absidx]\n",
    "        if keys[i] == 'index':\n",
    "            pass\n",
    "        elif keys[i] == 'observation_id' or keys[i] == 'report_id' or keys[i] == 'sensor_id' or keys[i] == 'source_id':\n",
    "            alldict = {keys[i]:np.asarray(ov_vars, dtype='S1')}\n",
    "            write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])\n",
    "        else:\n",
    "            alldict = pandas.DataFrame({keys[i]:ov_vars})\n",
    "            write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])  \n",
    "\n",
    "    for i in range(len(fbkeys)):\n",
    "        fb_vars = data.era5fb[fbkeys[i]][:]\n",
    "        fb_vars = fb_vars[absidx]\n",
    "        if fbkeys[i] == 'index' or fbkeys[i] == 'string6' or fbkeys[i] == 'string7' or fbkeys[i] == 'string10':\n",
    "            pass\n",
    "        elif fbkeys[i] == 'expver' or fbkeys[i] == 'source@hdr' or fbkeys[i] == 'source_id' or fbkeys[i] == 'statid@hdr':\n",
    "            alldict = {fbkeys[i]:np.asarray(fb_vars, dtype='S1')}\n",
    "            write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]])\n",
    "        else:\n",
    "            alldict = pandas.DataFrame({fbkeys[i]:fb_vars})\n",
    "            write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]]) \n",
    "    #\n",
    "    # writing the recordindices and recordtimestamp.\n",
    "    #       \n",
    "    recordindices=vridx\n",
    "    for i in range(len(recordindices)):\n",
    "        testvar = pandas.DataFrame({str(allvars[i]):recordindices[i]})\n",
    "        write_dict_h5(targetfile, testvar, 'recordindices', {str(allvars[i]): { 'compression': None } }, [str(allvars[i])]) \n",
    "\n",
    "    write_dict_h5(targetfile, {'recordtimestamp':rt}, 'recordindices', {'recordtimestamp': { 'compression': None } }, ['recordtimestamp'])\n",
    "\n",
    "    print('elapsed:',time.time()-tt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_resort(fn):\n",
    "    targetfile = './resorted_files_intercomp/'+fn.split('/')[-1]  \n",
    "    \n",
    "    with h5py.File(fn, 'r') as file:\n",
    "        with h5py.File(targetfile, 'w') as newfile:\n",
    "            groups = []\n",
    "            for i in file.keys():\n",
    "#                 if i == 'observations_table':\n",
    "#                     pass\n",
    "                if type(file[i]) == h5py._hl.group.Group:\n",
    "                    newfile.create_group(i)\n",
    "                    groups.append(i)\n",
    "                else:\n",
    "                    newfile.create_dataset(i, data=file[i][:])\n",
    "            for i in groups:\n",
    "                if(i == 'recordindices' or i == 'observations_table' or i == 'era5fb'):\n",
    "                    pass\n",
    "                else:\n",
    "                    for j in file[i].keys():\n",
    "                        newfile[i].create_dataset(j, data=file[i][j][:])\n",
    "\n",
    "    data =  eua.CDMDataset(fn)\n",
    "    allvars = data.observations_table.observed_variable[()]\n",
    "    allvars.sort()\n",
    "    allvars = numpy.unique(allvars)\n",
    "    #\n",
    "    ri = data.recordindex[()]\n",
    "#     print('recordindex: ', len(ri))\n",
    "    rt = data.recordtimestamp[()]\n",
    "    keys = data.observations_table.keys()#[:-1]\n",
    "#     fbkeys = data.era5fb.keys()[:-1]\n",
    "    # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "    pops = []\n",
    "    for i in range(len(keys)):\n",
    "        if 'string' in keys[i]:\n",
    "            pops.append(keys[i])\n",
    "    for i in pops: keys.remove(i)\n",
    "    pops = []\n",
    "#     for i in range(len(fbkeys)):\n",
    "#         if 'string' in fbkeys[i]:\n",
    "#             pops.append(fbkeys[i])\n",
    "#     for i in pops: fbkeys.remove(i)\n",
    "\n",
    "    recordindices = [[] for i in range(len(allvars))]\n",
    "    recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "    # output variables (from observations_table)\n",
    "    ov = []\n",
    "    for o in keys:\n",
    "        ov.append([[] for i in range(len(allvars))])\n",
    "#     fb = []\n",
    "#     for o in fbkeys:\n",
    "#         fb.append([[] for i in range(len(allvars))])\n",
    "    #\n",
    "    # loading the observed_variables\n",
    "    #\n",
    "    obsv = data.observations_table.observed_variable[:]\n",
    "    #\n",
    "    # resorting the data\n",
    "    #\n",
    "#     print('resort:start')\n",
    "    @njit\n",
    "    def make_vrindex(vridx,ridx,idx):\n",
    "        l=0\n",
    "        for i in range(1,len(idx)): # to set the recordindices\n",
    "            if ridx[i]>ridx[i-1]:\n",
    "                vridx[ridx[i-1]:ridx[i]]=l # next record after l\n",
    "                l=i\n",
    "        vridx[ridx[i]:]=len(idx) # next record for the last element is the len of the data\n",
    "\n",
    "\n",
    "    tt=time.time()\n",
    "\n",
    "    ridxall=np.zeros(obsv.shape[0],dtype=np.int64) # reverse index - index of the record index\n",
    "    j=-1\n",
    "    for j in range(len(ri)-1):\n",
    "        ridxall[ri[j]:ri[j+1]]=j\n",
    "    j+=1\n",
    "    ridxall[ri[j]:]=j # for the last elemenet\n",
    "    ridx=[]\n",
    "    vridx=[]\n",
    "    absidx=[]\n",
    "    abscount=0\n",
    "    for j in range(len(allvars)):\n",
    "        idx=np.where(obsv==allvars[j])[0] # index of all elements form certain variable j\n",
    "#         print(j,len(idx),',',end='')\n",
    "        vridx.append(np.zeros(ri.shape[0],dtype=np.int64)) # all zeros in lenght of record index\n",
    "        ridx=ridxall[idx] # ridxall where variable is j\n",
    "        make_vrindex(vridx[-1],ridx,idx)\n",
    "        vridx[-1]+=abscount # abscount for stacking the recordindex\n",
    "\n",
    "        absidx.append(copy.copy(idx)) # why copy? - to make sure it's not just the ref. - maybe ok without the cp\n",
    "        abscount+=len(idx)\n",
    "\n",
    "#     print('')\n",
    "    #\n",
    "    # finishing the sorting \n",
    "    #\n",
    "    absidx=np.concatenate(absidx)\n",
    "    #\n",
    "    # recordtimestamps are only necessary once\n",
    "    #\n",
    "    recordtimestamps = recordtimestamps[0]\n",
    "    #\n",
    "    # targetfile has to be a copy of the original file\n",
    "    #\n",
    "    targetfile = '/raid60/scratch/uli/resorted_files/'+fn.split('/')[-1]# 0-20000-0-63894_CEUAS_merged_v0.nc'\n",
    "    if os.path.isfile(targetfile):\n",
    "        mode='r+'\n",
    "    else:\n",
    "        mode='w'\n",
    "#     print()\n",
    "#     print('writing '+targetfile)\n",
    "\n",
    "    for i in range(len(keys)):\n",
    "        ov_vars = data.observations_table[keys[i]][:]\n",
    "        ov_vars = ov_vars[absidx]\n",
    "        if keys[i] == 'index':\n",
    "            pass\n",
    "        elif keys[i] == 'observation_id' or keys[i] == 'report_id' or keys[i] == 'sensor_id' or keys[i] == 'reference_sensor_id' or keys[i] == 'adjustment_id':\n",
    "            alldict = {keys[i]:np.asarray(ov_vars, dtype='S1')}\n",
    "            print(alldict)\n",
    "            write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])\n",
    "        else:\n",
    "            alldict = pandas.DataFrame({keys[i]:ov_vars})\n",
    "            write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])  \n",
    "\n",
    "#     for i in range(len(fbkeys)):\n",
    "#         fb_vars = data.era5fb[fbkeys[i]][:]\n",
    "#         fb_vars = fb_vars[absidx]\n",
    "#         if fbkeys[i] == 'index' or fbkeys[i] == 'string6' or fbkeys[i] == 'string7' or fbkeys[i] == 'string10':\n",
    "#             pass\n",
    "#         elif fbkeys[i] == 'expver' or fbkeys[i] == 'source@hdr' or fbkeys[i] == 'source_id' or fbkeys[i] == 'statid@hdr':\n",
    "#             alldict = {fbkeys[i]:np.asarray(fb_vars, dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({fbkeys[i]:fb_vars})\n",
    "#             write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]]) \n",
    "    #\n",
    "    # writing the recordindices and recordtimestamp.\n",
    "    #       \n",
    "    recordindices=vridx\n",
    "    for i in range(len(recordindices)):\n",
    "        testvar = pandas.DataFrame({str(allvars[i]):recordindices[i]})\n",
    "        write_dict_h5(targetfile, testvar, 'recordindices', {str(allvars[i]): { 'compression': None } }, [str(allvars[i])]) \n",
    "\n",
    "    write_dict_h5(targetfile, {'recordtimestamp':rt}, 'recordindices', {'recordtimestamp': { 'compression': None } }, ['recordtimestamp'])\n",
    "\n",
    "    print('elapsed:',time.time()-tt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with eua.CDMDataset('/raid60/scratch/leo/scratch/RI/Pangaea/nc/0-20100-0-00201.nc') as test:\n",
    "#     print(test.observations_table)\n",
    "# with eua.CDMDataset('/resorted_files_intercomp/0-20100-0-00201.nc') as test:\n",
    "#     print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment_id\n",
      "{'adjustment_id': array([[b'n', b'a', b'n', ..., b'', b'', b''],\n",
      "       [b'n', b'a', b'n', ..., b'', b'', b''],\n",
      "       [b'n', b'a', b'n', ..., b'', b'', b''],\n",
      "       ...,\n",
      "       [b'n', b'a', b'n', ..., b'', b'', b''],\n",
      "       [b'n', b'a', b'n', ..., b'', b'', b''],\n",
      "       [b'n', b'a', b'n', ..., b'', b'', b'']], dtype='|S1')}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create dataset (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-116c728fda5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdo_resort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/raid60/scratch/leo/scratch/RI/Pangaea/nc/0-20100-0-00201.nc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-b4fcf0719dd7>\u001b[0m in \u001b[0;36mdo_resort\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0malldict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mov_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'S1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malldict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mwrite_dict_h5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malldict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'observations_table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m'compression'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'gzip'\u001b[0m \u001b[0;34m}\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0malldict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mov_vars\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CEUAS/CEUAS/public/resort/../harvest/code/harvest_convert_to_netCDF_newfixes.py\u001b[0m in \u001b[0;36mwrite_dict_h5\u001b[0;34m(dfile, f, k, fbencodings, var_selection, mode, attrs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m                     \u001b[0mfd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfvv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfvv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfbencodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compression'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m                     \u001b[0mfd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m                     \u001b[0mslen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    146\u001b[0m                     \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mdsid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_new_dset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, allow_unknown_filter)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mdset_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create dataset (name already exists)"
     ]
    }
   ],
   "source": [
    "do_resort('/raid60/scratch/leo/scratch/RI/Pangaea/nc/0-20100-0-00201.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'create_group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/jetfs/spack/opt/spack/linux-rhel8-skylake_avx512/gcc-8.3.1/miniconda3-4.8.2-3m7b6t2kgedyr3jnd2nasmgiq7wm27iv/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/jetfs/spack/opt/spack/linux-rhel8-skylake_avx512/gcc-8.3.1/miniconda3-4.8.2-3m7b6t2kgedyr3jnd2nasmgiq7wm27iv/lib/python3.8/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-6-40c8bc5883e4>\", line 9, in do_resort\n    targetfile.create_group(i)\nAttributeError: 'str' object has no attribute 'create_group'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-64166b99c24a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresult_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_resort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jetfs/spack/opt/spack/linux-rhel8-skylake_avx512/gcc-8.3.1/miniconda3-4.8.2-3m7b6t2kgedyr3jnd2nasmgiq7wm27iv/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         '''\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jetfs/spack/opt/spack/linux-rhel8-skylake_avx512/gcc-8.3.1/miniconda3-4.8.2-3m7b6t2kgedyr3jnd2nasmgiq7wm27iv/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'create_group'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    pool = multiprocessing.Pool(processes=20)\n",
    "    result_list = pool.map(do_resort, files[:])\n",
    "    print(result_list)\n",
    "\n",
    "# 4250:4300\n",
    "# 4250:4255\n",
    "# sonst alle bis 5500\n",
    "# for i in range(len(files)):\n",
    "#     do_resort(files[i])\n",
    "#     if i > 2: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid60/scratch/federico/MERGED_DATABASE_OCTOBER2020_sensor/0-20000-0-22165_CEUAS_merged_v0.nc']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[4253:4254]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/raid60/scratch/federico/MERGED_DATABASE_OCTOBER2020_sensor/0-20000-0-22165_CEUAS_merged_v0.nc'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[4253]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allvars = data.observations_table.observed_variable[()]\n",
    "# allvars.sort()\n",
    "# allvars = np.unique(allvars)\n",
    "# # recordindices einzeln sortieren - fÃ¼r alle variablen eigene listen erstellen - dazu recordindices erstellen - alle daten hintereinander legen - len aller vorhergehender rec_in zu den nachkommenden addieren.\n",
    "# #\n",
    "# ri = data.recordindex[()]\n",
    "# rt = data.recordtimestamp[()]\n",
    "# keys = data.observations_table.keys()\n",
    "# recordindices = [[] for i in range(len(np.unique(allvars)))]\n",
    "# recordtimestamps = [[] for i in range(len(np.unique(allvars)))]\n",
    "\n",
    "# variables = [[] for i in range(len(np.unique(allvars)))]\n",
    "# values = [[] for i in range(len(np.unique(allvars)))]\n",
    "# zcoords = [[] for i in range(len(np.unique(allvars)))]\n",
    "\n",
    "# for i in range(len(ri)):\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         break\n",
    "        \n",
    "#     a = data.observations_table.observed_variable[()][start:end]\n",
    "#     b = data.observations_table.observation_value[()][start:end]\n",
    "#     c = data.observations_table.z_coordinate[()][start:end]\n",
    "\n",
    "#     sa, sb = zip(*sorted(zip(a, b)))\n",
    "#     sa, sc = zip(*sorted(zip(a, c)))\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 variables[j].append(sa[k])\n",
    "#                 values[j].append(sb[k])\n",
    "#                 zcoords[j].append(sc[k])\n",
    "#         recordindices[j].append(len(variables[j]))\n",
    "#         recordtimestamps[j].append(rt[i])\n",
    "        \n",
    "# lenadd = 0\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(variables[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allvars = data.observations_table.observed_variable[()]\n",
    "# allvars.sort()\n",
    "# allvars = np.unique(allvars)\n",
    "# #\n",
    "# ri = data.recordindex[()]\n",
    "# rt = data.recordtimestamp[()]\n",
    "# keys = data.observations_table.keys()[:-1]\n",
    "# recordindices = [[] for i in range(len(np.unique(allvars)))]\n",
    "# recordtimestamps = [[] for i in range(len(np.unique(allvars)))]\n",
    "\n",
    "# # output variables (from observations_table)\n",
    "# ov = []\n",
    "# for o in keys:\n",
    "#     ov.append([[] for i in range(len(np.unique(allvars)))])\n",
    "    \n",
    "# for i in range(len(ri)):\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         break\n",
    "        \n",
    "#     a = data.observations_table.observed_variable[()][start:end]\n",
    "#     helpvar = []\n",
    "#     for o in range(len(keys)):\n",
    "#         b = data.observations_table[keys[o]][()][start:end]\n",
    "#         try: # all varibales with different shape won't work here\n",
    "#             sa, sb = zip(*sorted(zip(a, b)))\n",
    "#             helpvar.append(sb)\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 for m in range(len(helpvar)):\n",
    "#                     try: # all varibales with different shape won't work here\n",
    "#                         ov[m][j].append(helpvar[m][k])\n",
    "#                     except:\n",
    "#                         pass\n",
    "#         recordindices[j].append(len(ov[0][j]))\n",
    "#         recordtimestamps[j].append(rt[i])\n",
    "        \n",
    "# lenadd = 0\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(variables[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# #\n",
    "# allvars = data.observations_table.observed_variable[()]\n",
    "# allvars.sort()\n",
    "# allvars = np.unique(allvars)\n",
    "# #\n",
    "# ri = data.recordindex[()]\n",
    "# print(len(ri))\n",
    "# # rt = data.recordtimestamp[()]\n",
    "# keys = data.observations_table.keys()[:-1]\n",
    "# # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "# keys.pop(43)\n",
    "# keys.pop(42)\n",
    "# keys.pop(41)\n",
    "# recordindices = [[] for i in range(len(allvars))]\n",
    "# recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "# # output variables (from observations_table)\n",
    "# ov = []\n",
    "# for o in keys:\n",
    "#     ov.append([[] for i in range(len(np.unique(allvars)))])    \n",
    "    \n",
    "# print('resort:start')\n",
    "# for i in range(len(ri)):\n",
    "#     print(i)\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         start = ri[i]\n",
    "#         end = len(data.observations_table.observed_variable[()])\n",
    "        \n",
    "#     a = data.observations_table.observed_variable[()][start:end]\n",
    "#     helpvar = []\n",
    "#     for o in range(len(keys)):\n",
    "#         if keys[o] == 'observation_id' or keys[o] == 'report_id' or keys[o] == 'sensor_id' or keys[o] == 'source_id':\n",
    "#             b = []\n",
    "#             for n in data.observations_table[keys[o]][()][start:end]:\n",
    "#                 c = ''\n",
    "#                 for bb in n:\n",
    "#                     c = c + bb.decode()\n",
    "#                 b.append(c)\n",
    "#         else:\n",
    "#             b = data.observations_table[keys[o]][()][start:end]\n",
    "#         sa, sb = zip(*sorted(zip(a, b)))\n",
    "#         helpvar.append(sb)\n",
    "\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 for m in range(len(helpvar)):\n",
    "#                     ov[m][j].append(helpvar[m][k])\n",
    "#         recordindices[j].append(len(ov[0][j]))\n",
    "# #         recordtimestamps[j].append(rt[i])\n",
    "\n",
    "# print('resort:done')\n",
    "        \n",
    "# #\n",
    "# # setting record_indices to the right value -> stacking them\n",
    "# #\n",
    "# lenadd = 0.\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(ov[0][i])\n",
    "\n",
    "# #\n",
    "# # shaping record_indices -> setting every missing value to nan\n",
    "# #\n",
    "# old = -1\n",
    "# for i in range(len(recordindices)):\n",
    "#     for j in range(len(recordindices[i])):\n",
    "#         if recordindices[i][j] != 0 and recordindices[i][j] == recordindices[i][j-1]:\n",
    "#             old = recordindices[i][j]\n",
    "#             recordindices[i][j] = np.nan\n",
    "#         elif recordindices[i][j] == old:\n",
    "#             recordindices[i][j] = np.nan\n",
    "\n",
    "# #\n",
    "# # recordtimestamps are only necessary once\n",
    "# #\n",
    "# recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "# # \n",
    "# # stacking all output variables\n",
    "# #\n",
    "# out = []\n",
    "# for j in ov:\n",
    "#     finalvar = []\n",
    "#     for i in j:\n",
    "#         finalvar = finalvar + i\n",
    "#     out.append(finalvar)\n",
    "    \n",
    "# #\n",
    "# # restoring byte arrays:\n",
    "# #\n",
    "# for o in range(len(keys)):\n",
    "#     if keys[o] == 'observation_id' or keys[o] == 'report_id' or keys[o] == 'sensor_id' or keys[o] == 'source_id':\n",
    "#         b = []\n",
    "#         for n in range(len(out[o])):\n",
    "#             c = []\n",
    "#             for bb in out[o][n]:\n",
    "#                 c.append(bb.encode())\n",
    "#             out[o][n] = c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allvars = data.observations_table.observed_variable[()]\n",
    "# allvars.sort()\n",
    "# allvars = np.unique(allvars)\n",
    "# #\n",
    "# ri = data.recordindex[()]\n",
    "# print('recordindex: ', len(ri))\n",
    "# # rt = data.recordtimestamp[()]\n",
    "# keys = data.observations_table.keys()[:-1]\n",
    "# # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "# keys.pop(43)\n",
    "# keys.pop(42)\n",
    "# keys.pop(41)\n",
    "# recordindices = [[] for i in range(len(allvars))]\n",
    "# recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "# # output variables (from observations_table)\n",
    "# ov = []\n",
    "# for o in keys:\n",
    "#     ov.append([[] for i in range(len(np.unique(allvars)))])\n",
    "\n",
    "# #\n",
    "# # load data into memory and decode byte arrays\n",
    "# #\n",
    "# print('loading data')\n",
    "# obsv = data.observations_table.observed_variable[:]\n",
    "# ov_vars = []\n",
    "# for o in range(len(keys)):\n",
    "#     ov_vars.append(data.observations_table[keys[o]][:])\n",
    "#     if keys[o] == 'observation_id' or keys[o] == 'report_id' or keys[o] == 'sensor_id' or keys[o] == 'source_id':\n",
    "#         b = []\n",
    "#         for n in ov_vars[o]:\n",
    "#             c = ''\n",
    "#             for bb in n:\n",
    "#                 c = c + bb.decode()\n",
    "#             b.append(c)\n",
    "#         ov_vars[o] = b\n",
    "\n",
    "# #\n",
    "# # resorting the data\n",
    "# #\n",
    "# print('resort:start')\n",
    "# for i in range(len(ri)):\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         start = ri[i]\n",
    "#         end = len(data.observations_table.observed_variable[()])\n",
    "        \n",
    "#     a = obsv[start:end]\n",
    "#     helpvar = []\n",
    "#     for o in range(len(keys)):\n",
    "#         b = ov_vars[o][start:end]\n",
    "#         sa, sb = zip(*sorted(zip(a, b)))\n",
    "#         helpvar.append(sb)\n",
    "\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 for m in range(len(helpvar)):\n",
    "#                     ov[m][j].append(helpvar[m][k])\n",
    "#         recordindices[j].append(len(ov[0][j]))\n",
    "# #         recordtimestamps[j].append(rt[i])\n",
    "\n",
    "# print('resort:done')\n",
    "        \n",
    "# #\n",
    "# # setting record_indices to the right value -> stacking them\n",
    "# #\n",
    "# lenadd = 0.\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(ov[0][i])\n",
    "\n",
    "# #\n",
    "# # shaping record_indices -> setting every missing value to nan\n",
    "# #\n",
    "# print('shaping record_indices')\n",
    "# old = -1\n",
    "# for i in range(len(recordindices)):\n",
    "#     for j in range(len(recordindices[i])):\n",
    "#         if recordindices[i][j] != 0 and recordindices[i][j] == recordindices[i][j-1]:\n",
    "#             old = recordindices[i][j]\n",
    "#             recordindices[i][j] = np.nan\n",
    "#         elif recordindices[i][j] == old:\n",
    "#             recordindices[i][j] = np.nan\n",
    "\n",
    "# #\n",
    "# # recordtimestamps are only necessary once\n",
    "# #\n",
    "# recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "# # \n",
    "# # stacking all output variables\n",
    "# #\n",
    "# print('stacking output variables')\n",
    "# out = []\n",
    "# for j in ov:\n",
    "#     finalvar = []\n",
    "#     for i in j:\n",
    "#         finalvar = finalvar + i\n",
    "#     out.append(finalvar)\n",
    "    \n",
    "# #\n",
    "# # restoring byte arrays:\n",
    "# # this takes very long -> find faster option\n",
    "# #\n",
    "# print('restoring byte arrays')\n",
    "# bytelist = [25, 34, 38, 39]\n",
    "# for o in bytelist:\n",
    "#     originallen = len(data.observations_table[keys[o]][()][0])\n",
    "#     for n in range(len(out[o])):\n",
    "#         c = [elem.encode() for elem in out[o][n]]\n",
    "#         # problem: if the string was [b'x', b'y', b''] in the first place, it get converted to 'xy' and then back to [b'x', b'y']\n",
    "#         # add empty byte strings until the data is as long as before:\n",
    "#         while(len(c) < originallen):\n",
    "#             c.append(str('').encode())\n",
    "#         out[o][n] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with eua.CDMDataset(files[0]) as data:\n",
    "#     allvars = data.observations_table.observed_variable[()]\n",
    "#     allvars.sort()\n",
    "#     allvars = np.unique(allvars)\n",
    "#     #\n",
    "#     ri = data.recordindex[()]\n",
    "#     print('recordindex: ', len(ri))\n",
    "#     keys = data.observations_table.keys()[:-1]\n",
    "#     # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "#     keys.pop(43)\n",
    "#     keys.pop(42)\n",
    "#     keys.pop(41)\n",
    "#     recordindices = [[] for i in range(len(allvars))]\n",
    "#     recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "#     # output variables (from observations_table)\n",
    "#     ov = []\n",
    "#     for o in keys:\n",
    "#         ov.append([[] for i in range(len(np.unique(allvars)))])\n",
    "\n",
    "#     #\n",
    "#     # load data into memory and decode byte arrays\n",
    "#     #\n",
    "#     print('loading data')\n",
    "#     obsv = data.observations_table.observed_variable[:]\n",
    "#     ov_vars = []\n",
    "#     for o in range(len(keys)):\n",
    "#         ov_vars.append(data.observations_table[keys[o]][:])\n",
    "\n",
    "# #\n",
    "# # resorting the data\n",
    "# #\n",
    "# print('resort:start')\n",
    "# for i in range(len(ri)):\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         start = ri[i]\n",
    "#         end = len(obsv)\n",
    "        \n",
    "#     a = obsv[start:end]\n",
    "#     sa, sortindex = zip(*sorted(zip(a, range(len(a)))))\n",
    "#     sortindex = np.array(sortindex)\n",
    "#     helpvar = []\n",
    "#     for o in range(len(keys)):\n",
    "#         b = ov_vars[o][start:end]\n",
    "#         helpvar.append(b[sortindex])\n",
    "\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 for m in range(len(helpvar)):\n",
    "#                     ov[m][j].append(helpvar[m][k])\n",
    "#         recordindices[j].append(len(ov[0][j]))\n",
    "\n",
    "# print('resort:done')\n",
    "        \n",
    "# #\n",
    "# # setting record_indices to the right value -> stacking them\n",
    "# #\n",
    "# lenadd = 0.\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(ov[0][i])\n",
    "\n",
    "# #\n",
    "# # shaping record_indices -> setting every missing value to nan\n",
    "# #\n",
    "# print('shaping record_indices')\n",
    "# old = -1\n",
    "# for i in range(len(recordindices)):\n",
    "#     for j in range(len(recordindices[i])):\n",
    "#         if i != 0 or j != 0:\n",
    "#             if recordindices[i][j] == recordindices[i][j-1]:\n",
    "#                 old = recordindices[i][j]\n",
    "#                 recordindices[i][j] = np.nan\n",
    "#             elif recordindices[i][j] == old:\n",
    "#                 recordindices[i][j] = np.nan\n",
    "\n",
    "# #\n",
    "# # recordtimestamps are only necessary once\n",
    "# #\n",
    "# recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "# # \n",
    "# # stacking all output variables\n",
    "# #\n",
    "# print('stacking output variables')\n",
    "# out = []\n",
    "# for k in ov:\n",
    "#     out.append([j for i in k for j in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_resort(fn):\n",
    "    \n",
    "#     data = eua.CDMDataset(fn)\n",
    "\n",
    "#     allvars = data.observations_table.observed_variable[()]\n",
    "#     allvars.sort()\n",
    "#     allvars = numpy.unique(allvars)\n",
    "#     #\n",
    "#     ri = data.recordindex[()]\n",
    "#     print('recordindex: ', len(ri))\n",
    "#     rt = data.recordtimestamp[()]\n",
    "#     keys = data.observations_table.keys()[:-1]\n",
    "#     fbkeys = data.era5fb.keys()[:]\n",
    "#     # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "#     keys.pop(43)\n",
    "#     keys.pop(42)\n",
    "#     keys.pop(41)\n",
    "#     recordindices = [[] for i in range(len(allvars))]\n",
    "#     recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "#     # output variables (from observations_table)\n",
    "#     ov = []\n",
    "#     for o in keys:\n",
    "#         ov.append([[] for i in range(len(numpy.unique(allvars)))])\n",
    "\n",
    "#     #\n",
    "#     # load data into memory and decode byte arrays\n",
    "#     #\n",
    "#     print('loading data')\n",
    "#     obsv = data.observations_table.observed_variable[:]\n",
    "#     ov_vars = []\n",
    "#     for o in range(len(keys)):\n",
    "#         ov_vars.append(data.observations_table[keys[o]][:])\n",
    "\n",
    "#     #\n",
    "#     # resorting the data\n",
    "#     #\n",
    "#     print('resort:start')\n",
    "#     @njit\n",
    "#     def make_vrindex(vridx,ridx,idx):\n",
    "#         l=0\n",
    "#         for i in range(1,len(idx)): # to set the recordindices\n",
    "#             if ridx[i]>ridx[i-1]:\n",
    "#                 vridx[ridx[i-1]:ridx[i]]=l # next record after l\n",
    "#                 l=i\n",
    "#         vridx[ridx[i]:]=len(idx) # next record for the last element is the len of the data\n",
    "\n",
    "\n",
    "#     tt=time.time()\n",
    "\n",
    "#     ridxall=np.zeros(ov_vars[0].shape[0],dtype=np.int64) # reverse index - index of the record index\n",
    "#     j=-1\n",
    "#     for j in range(len(ri)-1):\n",
    "#         ridxall[ri[j]:ri[j+1]]=j\n",
    "#     j+=1\n",
    "#     ridxall[ri[j]:]=j # for the last elemenet\n",
    "\n",
    "#     ridx=[]\n",
    "#     vridx=[]\n",
    "#     absidx=[]\n",
    "#     abscount=0\n",
    "#     for j in range(len(allvars)):\n",
    "#         idx=np.where(ov_vars[keys.index('observed_variable')]==allvars[j])[0] # index of all elements form certain variable j\n",
    "#         print(j,len(idx),',',end='')\n",
    "#         vridx.append(np.zeros(ri.shape[0],dtype=np.int64)) # all zeros in lenght of record index\n",
    "#         ridx=ridxall[idx] # ridxall where variable is j\n",
    "#         make_vrindex(vridx[-1],ridx,idx)\n",
    "#         vridx[-1]+=abscount # abscount for stacking the recordindex\n",
    "\n",
    "#         absidx.append(copy.copy(idx)) # why copy? - to make sure it's not just the ref. - maybe ok without the cp\n",
    "#         abscount+=len(idx)\n",
    "\n",
    "#     print('')\n",
    "#     absidx=np.concatenate(absidx)\n",
    "#     for o in range(len(keys)):\n",
    "#         ov_vars[o]=ov_vars[o][absidx]\n",
    "#         print(o,end='')\n",
    "#     #\n",
    "#     # recordtimestamps are only necessary once\n",
    "#     #\n",
    "#     recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "\n",
    "#     out = ov_vars\n",
    "#     # targetfile has to be a copy of the original file\n",
    "#     targetfile = '/raid60/scratch/uli/resorted_files_20201109/'+fn.split('/')[-1]# 0-20000-0-63894_CEUAS_merged_v0.nc'\n",
    "#     if os.path.isfile(targetfile):\n",
    "#         mode='r+'\n",
    "#     else:\n",
    "#         mode='w'\n",
    "        \n",
    "#     print()\n",
    "#     print('writing '+targetfile)\n",
    "#     #\n",
    "#     # writing data into observations_table\n",
    "#     #\n",
    "#     with h5py.File(targetfile, mode) as file:\n",
    "#         for i in range(len(keys)):\n",
    "#             try:\n",
    "#                 del file['observations_table'][keys[i]]\n",
    "#             except:\n",
    "#                 pass\n",
    "#     for i in range(len(keys)):\n",
    "#         if keys[i] == 'index':\n",
    "#             pass\n",
    "#         elif keys[i] == 'observation_id' or keys[i] == 'report_id' or keys[i] == 'sensor_id' or keys[i] == 'source_id':\n",
    "# #             slen = len(out[i][0])\n",
    "#             alldict = {keys[i]:np.asarray(out[i], dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({keys[i]:out[i]})\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])  \n",
    "#         #\n",
    "#         # writing the recordindices and recordtimestamp.\n",
    "#         #       \n",
    "#     recordindices=vridx\n",
    "#     with h5py.File(targetfile, 'r+') as file:\n",
    "#         for i in range(len(recordindices)):\n",
    "#             try:\n",
    "#                 del file['recordindices'][str(allvars[i])]\n",
    "#             except:\n",
    "#                 pass\n",
    "#     for i in range(len(recordindices)):\n",
    "#         testvar = pandas.DataFrame({str(allvars[i]):recordindices[i]})\n",
    "#         write_dict_h5(targetfile, testvar, 'recordindices', {str(allvars[i]): { 'compression': None } }, [str(allvars[i])]) \n",
    "\n",
    "#     with h5py.File(targetfile, 'r+') as file:\n",
    "#         try:\n",
    "#             del file['recordindex']\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             del file['recordtimestamp']\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             del file['recordindices']['recordtimestamp']\n",
    "#         except:\n",
    "#             pass\n",
    "#     write_dict_h5(targetfile, {'recordtimestamp':rt}, 'recordindices', {'recordtimestamp': { 'compression': None } }, ['recordtimestamp']) \n",
    "        \n",
    "#     print('elapsed:',time.time()-tt)\n",
    "#     print(end='')\n",
    "\n",
    "\n",
    "\n",
    "# # In[15]:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_resort(fn):\n",
    "    \n",
    "#     data = eua.CDMDataset(fn)\n",
    "\n",
    "#     allvars = data.observations_table.observed_variable[()]\n",
    "#     allvars.sort()\n",
    "#     allvars = numpy.unique(allvars)\n",
    "#     #\n",
    "#     ri = data.recordindex[()]\n",
    "#     print('recordindex: ', len(ri))\n",
    "#     rt = data.recordtimestamp[()]\n",
    "#     keys = data.observations_table.keys()[:-1]\n",
    "#     fbkeys = data.era5fb.keys()[:-1]\n",
    "#     # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "#     pops = []\n",
    "#     for i in range(len(keys)):\n",
    "#         if 'string' in keys[i]:\n",
    "#             pops.append(keys[i])\n",
    "#     for i in pops: keys.remove(i)\n",
    "#     pops = []\n",
    "#     for i in range(len(fbkeys)):\n",
    "#         if 'string' in fbkeys[i]:\n",
    "#             pops.append(fbkeys[i])\n",
    "#     for i in pops: fbkeys.remove(i)\n",
    "        \n",
    "#     recordindices = [[] for i in range(len(allvars))]\n",
    "#     recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "#     # output variables (from observations_table)\n",
    "#     ov = []\n",
    "#     for o in keys:\n",
    "#         ov.append([[] for i in range(len(allvars))])\n",
    "#     fb = []\n",
    "#     for o in fbkeys:\n",
    "#         fb.append([[] for i in range(len(allvars))])\n",
    "\n",
    "#     #\n",
    "#     # load data into memory and decode byte arrays\n",
    "#     #\n",
    "#     print('loading data')\n",
    "#     obsv = data.observations_table.observed_variable[:]\n",
    "#     ov_vars = []\n",
    "#     for o in range(len(keys)):\n",
    "#         ov_vars.append(data.observations_table[keys[o]][:])\n",
    "#     fb_vars = []\n",
    "#     for o in range(len(fbkeys)):\n",
    "#         fb_vars.append(data.era5fb[fbkeys[o]][:])\n",
    "\n",
    "#     #\n",
    "#     # resorting the data\n",
    "#     #\n",
    "#     print('resort:start')\n",
    "#     @njit\n",
    "#     def make_vrindex(vridx,ridx,idx):\n",
    "#         l=0\n",
    "#         for i in range(1,len(idx)): # to set the recordindices\n",
    "#             if ridx[i]>ridx[i-1]:\n",
    "#                 vridx[ridx[i-1]:ridx[i]]=l # next record after l\n",
    "#                 l=i\n",
    "#         vridx[ridx[i]:]=len(idx) # next record for the last element is the len of the data\n",
    "\n",
    "\n",
    "#     tt=time.time()\n",
    "\n",
    "#     ridxall=np.zeros(ov_vars[0].shape[0],dtype=np.int64) # reverse index - index of the record index\n",
    "#     j=-1\n",
    "#     for j in range(len(ri)-1):\n",
    "#         ridxall[ri[j]:ri[j+1]]=j\n",
    "#     j+=1\n",
    "#     ridxall[ri[j]:]=j # for the last elemenet\n",
    "\n",
    "#     ridx=[]\n",
    "#     vridx=[]\n",
    "#     absidx=[]\n",
    "#     abscount=0\n",
    "#     for j in range(len(allvars)):\n",
    "#         idx=np.where(ov_vars[keys.index('observed_variable')]==allvars[j])[0] # index of all elements form certain variable j\n",
    "#         print(j,len(idx),',',end='')\n",
    "#         vridx.append(np.zeros(ri.shape[0],dtype=np.int64)) # all zeros in lenght of record index\n",
    "#         ridx=ridxall[idx] # ridxall where variable is j\n",
    "#         make_vrindex(vridx[-1],ridx,idx)\n",
    "#         vridx[-1]+=abscount # abscount for stacking the recordindex\n",
    "\n",
    "#         absidx.append(copy.copy(idx)) # why copy? - to make sure it's not just the ref. - maybe ok without the cp\n",
    "#         abscount+=len(idx)\n",
    "\n",
    "#     print('')\n",
    "#     absidx=np.concatenate(absidx)\n",
    "#     for o in range(len(keys)):\n",
    "#         ov_vars[o]=ov_vars[o][absidx]\n",
    "#         print(o,end='')\n",
    "        \n",
    "#     for o in range(len(fbkeys)):\n",
    "#         fb_vars[o]=fb_vars[o][absidx]\n",
    "#         print(o,end='')\n",
    "\n",
    "#     #\n",
    "#     # recordtimestamps are only necessary once\n",
    "#     #\n",
    "#     recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "\n",
    "#     out = ov_vars\n",
    "#     # targetfile has to be a copy of the original file\n",
    "#     targetfile = '/raid60/scratch/uli/resorted_files_20201109/'+fn.split('/')[-1]# 0-20000-0-63894_CEUAS_merged_v0.nc'\n",
    "#     if os.path.isfile(targetfile):\n",
    "#         mode='r+'\n",
    "#     else:\n",
    "#         mode='w'\n",
    "        \n",
    "#     print()\n",
    "#     print('writing '+targetfile)\n",
    "#     #\n",
    "#     # writing data into observations_table\n",
    "#     #\n",
    "#     with h5py.File(targetfile, mode) as file:\n",
    "#         for i in range(len(keys)):\n",
    "#             try:\n",
    "#                 del file['observations_table'][keys[i]]\n",
    "#             except:\n",
    "#                 pass\n",
    "#         for i in range(len(fbkeys)):\n",
    "#             try:\n",
    "#                 del file['era5fb'][fbkeys[i]]\n",
    "#             except:\n",
    "#                 pass\n",
    "#     for i in range(len(keys)):\n",
    "#         if keys[i] == 'index':\n",
    "#             pass\n",
    "#         elif keys[i] == 'observation_id' or keys[i] == 'report_id' or keys[i] == 'sensor_id' or keys[i] == 'source_id':\n",
    "# #             slen = len(out[i][0])\n",
    "#             alldict = {keys[i]:np.asarray(out[i], dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({keys[i]:out[i]})\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])  \n",
    "            \n",
    "#     for i in range(len(fbkeys)):\n",
    "#         if fbkeys[i] == 'index' or fbkeys[i] == 'string6' or fbkeys[i] == 'string7' or fbkeys[i] == 'string10':\n",
    "#             pass\n",
    "#         elif fbkeys[i] == 'expver' or fbkeys[i] == 'source@hdr' or fbkeys[i] == 'source_id' or fbkeys[i] == 'statid@hdr':\n",
    "# #             slen = len(out[i][0])\n",
    "#             alldict = {fbkeys[i]:np.asarray(fb_vars[i], dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({fbkeys[i]:fb_vars[i]})\n",
    "#             write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]]) \n",
    "#     #\n",
    "#     # writing the recordindices and recordtimestamp.\n",
    "#     #       \n",
    "#     recordindices=vridx\n",
    "#     with h5py.File(targetfile, 'r+') as file:\n",
    "#         for i in range(len(recordindices)):\n",
    "#             try:\n",
    "#                 del file['recordindices'][str(allvars[i])]\n",
    "#             except:\n",
    "#                 pass\n",
    "#     for i in range(len(recordindices)):\n",
    "#         testvar = pandas.DataFrame({str(allvars[i]):recordindices[i]})\n",
    "#         write_dict_h5(targetfile, testvar, 'recordindices', {str(allvars[i]): { 'compression': None } }, [str(allvars[i])]) \n",
    "\n",
    "#     with h5py.File(targetfile, 'r+') as file:\n",
    "#         try:\n",
    "#             del file['recordindex']\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             del file['recordtimestamp']\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             del file['recordindices']['recordtimestamp']\n",
    "#         except:\n",
    "#             pass\n",
    "#     write_dict_h5(targetfile, {'recordtimestamp':rt}, 'recordindices', {'recordtimestamp': { 'compression': None } }, ['recordtimestamp']) \n",
    "        \n",
    "#     print('elapsed:',time.time()-tt)\n",
    "#     print(end='')\n",
    "\n",
    "\n",
    "\n",
    "# # In[15]:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_resort(fn):\n",
    "#     targetfile = '/raid60/scratch/uli/resorted_files/'+fn.split('/')[-1]  \n",
    "    \n",
    "#     with h5py.File(fn, 'r') as file:\n",
    "#         with h5py.File(targetfile, 'w') as newfile:\n",
    "#             newfile.copy(file['header_table'],newfile)\n",
    "#             newfile.copy(file['sensor_configuration'],newfile)\n",
    "#             newfile.copy(file['crs'],newfile)\n",
    "#             newfile.copy(file['observed_variable'],newfile)\n",
    "#             newfile.copy(file['source_configuration'],newfile)\n",
    "#             newfile.copy(file['station_configuration'],newfile)\n",
    "#             newfile.copy(file['station_type'],newfile)\n",
    "#             newfile.copy(file['station_configuration_codes'],newfile)\n",
    "#             newfile.copy(file['units'],newfile)\n",
    "#             newfile.copy(file['z_coordinate_type'],newfile)\n",
    "            \n",
    "#             newfile.create_dataset('dateindex', data=file['dateindex'][:]) \n",
    "\n",
    "    \n",
    "#     data =  eua.CDMDataset(fn)\n",
    "#     allvars = data.observations_table.observed_variable[()]\n",
    "#     allvars.sort()\n",
    "#     allvars = numpy.unique(allvars)\n",
    "#     #\n",
    "#     ri = data.recordindex[()]\n",
    "# #     print('recordindex: ', len(ri))\n",
    "#     rt = data.recordtimestamp[()]\n",
    "#     keys = data.observations_table.keys()[:-1]\n",
    "#     fbkeys = data.era5fb.keys()[:-1]\n",
    "#     # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "#     pops = []\n",
    "#     for i in range(len(keys)):\n",
    "#         if 'string' in keys[i]:\n",
    "#             pops.append(keys[i])\n",
    "#     for i in pops: keys.remove(i)\n",
    "#     pops = []\n",
    "#     for i in range(len(fbkeys)):\n",
    "#         if 'string' in fbkeys[i]:\n",
    "#             pops.append(fbkeys[i])\n",
    "#     for i in pops: fbkeys.remove(i)\n",
    "\n",
    "#     recordindices = [[] for i in range(len(allvars))]\n",
    "#     recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "#     # output variables (from observations_table)\n",
    "#     ov = []\n",
    "#     for o in keys:\n",
    "#         ov.append([[] for i in range(len(allvars))])\n",
    "#     fb = []\n",
    "#     for o in fbkeys:\n",
    "#         fb.append([[] for i in range(len(allvars))])\n",
    "#     #\n",
    "#     # loading the observed_variables\n",
    "#     #\n",
    "#     obsv = data.observations_table.observed_variable[:]\n",
    "#     #\n",
    "#     # resorting the data\n",
    "#     #\n",
    "# #     print('resort:start')\n",
    "#     @njit\n",
    "#     def make_vrindex(vridx,ridx,idx):\n",
    "#         l=0\n",
    "#         for i in range(1,len(idx)): # to set the recordindices\n",
    "#             if ridx[i]>ridx[i-1]:\n",
    "#                 vridx[ridx[i-1]:ridx[i]]=l # next record after l\n",
    "#                 l=i\n",
    "#         vridx[ridx[i]:]=len(idx) # next record for the last element is the len of the data\n",
    "\n",
    "\n",
    "#     tt=time.time()\n",
    "\n",
    "#     ridxall=np.zeros(obsv.shape[0],dtype=np.int64) # reverse index - index of the record index\n",
    "#     j=-1\n",
    "#     for j in range(len(ri)-1):\n",
    "#         ridxall[ri[j]:ri[j+1]]=j\n",
    "#     j+=1\n",
    "#     ridxall[ri[j]:]=j # for the last elemenet\n",
    "#     ridx=[]\n",
    "#     vridx=[]\n",
    "#     absidx=[]\n",
    "#     abscount=0\n",
    "#     for j in range(len(allvars)):\n",
    "#         idx=np.where(obsv==allvars[j])[0] # index of all elements form certain variable j\n",
    "# #         print(j,len(idx),',',end='')\n",
    "#         vridx.append(np.zeros(ri.shape[0],dtype=np.int64)) # all zeros in lenght of record index\n",
    "#         ridx=ridxall[idx] # ridxall where variable is j\n",
    "#         make_vrindex(vridx[-1],ridx,idx)\n",
    "#         vridx[-1]+=abscount # abscount for stacking the recordindex\n",
    "\n",
    "#         absidx.append(copy.copy(idx)) # why copy? - to make sure it's not just the ref. - maybe ok without the cp\n",
    "#         abscount+=len(idx)\n",
    "\n",
    "# #     print('')\n",
    "#     #\n",
    "#     # finishing the sorting \n",
    "#     #\n",
    "#     absidx=np.concatenate(absidx)\n",
    "#     #\n",
    "#     # recordtimestamps are only necessary once\n",
    "#     #\n",
    "#     recordtimestamps = recordtimestamps[0]\n",
    "#     #\n",
    "#     # targetfile has to be a copy of the original file\n",
    "#     #\n",
    "#     targetfile = '/raid60/scratch/uli/resorted_files/'+fn.split('/')[-1]# 0-20000-0-63894_CEUAS_merged_v0.nc'\n",
    "#     if os.path.isfile(targetfile):\n",
    "#         mode='r+'\n",
    "#     else:\n",
    "#         mode='w'\n",
    "# #     print()\n",
    "# #     print('writing '+targetfile)\n",
    "\n",
    "#     #\n",
    "#     # writing data into observations_table\n",
    "#     #\n",
    "# #     with h5py.File(targetfile, mode) as file:\n",
    "# #         for i in range(len(keys)):\n",
    "# #             try:\n",
    "# #                 del file['observations_table'][keys[i]]\n",
    "# #             except:\n",
    "# #                 pass\n",
    "# #         for i in range(len(fbkeys)):\n",
    "# #             try:\n",
    "# #                 del file['era5fb'][fbkeys[i]]\n",
    "# #             except:\n",
    "# #                 pass\n",
    "#     for i in range(len(keys)):\n",
    "#         ov_vars = data.observations_table[keys[i]][:]\n",
    "#         ov_vars = ov_vars[absidx]\n",
    "#         if keys[i] == 'index':\n",
    "#             pass\n",
    "#         elif keys[i] == 'observation_id' or keys[i] == 'report_id' or keys[i] == 'sensor_id' or keys[i] == 'source_id':\n",
    "#             alldict = {keys[i]:np.asarray(ov_vars, dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({keys[i]:ov_vars})\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])  \n",
    "\n",
    "#     for i in range(len(fbkeys)):\n",
    "#         fb_vars = data.era5fb[fbkeys[i]][:]\n",
    "#         fb_vars = fb_vars[absidx]\n",
    "#         if fbkeys[i] == 'index' or fbkeys[i] == 'string6' or fbkeys[i] == 'string7' or fbkeys[i] == 'string10':\n",
    "#             pass\n",
    "#         elif fbkeys[i] == 'expver' or fbkeys[i] == 'source@hdr' or fbkeys[i] == 'source_id' or fbkeys[i] == 'statid@hdr':\n",
    "#             alldict = {fbkeys[i]:np.asarray(fb_vars, dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({fbkeys[i]:fb_vars})\n",
    "#             write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]]) \n",
    "#     #\n",
    "#     # writing the recordindices and recordtimestamp.\n",
    "#     #       \n",
    "#     recordindices=vridx\n",
    "# #     with h5py.File(targetfile, 'w') as file:\n",
    "# #         for i in range(len(recordindices)):\n",
    "# #             try:\n",
    "# #                 del file['recordindices'][str(allvars[i])]\n",
    "# #             except:\n",
    "# #                 pass\n",
    "#     for i in range(len(recordindices)):\n",
    "#         testvar = pandas.DataFrame({str(allvars[i]):recordindices[i]})\n",
    "#         write_dict_h5(targetfile, testvar, 'recordindices', {str(allvars[i]): { 'compression': None } }, [str(allvars[i])]) \n",
    "\n",
    "# #     with h5py.File(targetfile, 'w') as file:\n",
    "# #         try:\n",
    "# #             del file['recordindex']\n",
    "# #         except:\n",
    "# #             pass\n",
    "# #         try:\n",
    "# #             del file['recordtimestamp']\n",
    "# #         except:\n",
    "# #             pass\n",
    "# #         try:\n",
    "# #             del file['recordindices']['recordtimestamp']\n",
    "# #         except:\n",
    "# #             pass\n",
    "#     write_dict_h5(targetfile, {'recordtimestamp':rt}, 'recordindices', {'recordtimestamp': { 'compression': None } }, ['recordtimestamp'])\n",
    "# #     with h5py.File(targetfile, 'w') as newfile:\n",
    "# #         for i in data.header_table.keys():\n",
    "# #             alldict = pandas.DataFrame({i:data.header_table[i]})\n",
    "# #             write_dict_h5(targetfile, alldict, 'header_table', {i: { 'compression': 'gzip' } }, [i])\n",
    "\n",
    "#     print('elapsed:',time.time()-tt)\n",
    "# #     print(end='')\n",
    "\n",
    "\n",
    "\n",
    "# # In[15]:\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Miniconda3 (4.8.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
