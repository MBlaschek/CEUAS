{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "import sys,glob\n",
    "import zipfile, os, time\n",
    "import urllib3\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import h5py\n",
    "import plotly.express as px\n",
    "sys.path.append(os.getcwd()+'/../cds-backend/code/')\n",
    "sys.path.append(os.getcwd()+'/../harvest/code/')\n",
    "from harvest_convert_to_netCDF_newfixes import write_dict_h5\n",
    "import cds_eua3 as eua\n",
    "eua.logging_set_level(30)\n",
    "import xarray as xr\n",
    "\n",
    "import cdsapi, zipfile, os, time\n",
    "#import schedule\n",
    "import copy\n",
    "from shutil import copyfile\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/raid60/scratch/federico/MERGED_DATABASE_OCTOBER2020_sensor/0-20001-0-01001_CEUAS_merged_v0.nc'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('/raid60/scratch/federico/MERGED_DATABASE_OCTOBER2020_sensor/0*.nc')\n",
    "# files = glob.glob('/raid60/scratch/uli/0*.nc')\n",
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_resort(fn):\n",
    "    targetfile = '/raid60/scratch/uli/resorted_files/'+fn.split('/')[-1]  \n",
    "    \n",
    "    with h5py.File(fn, 'r') as file:\n",
    "        with h5py.File(targetfile, 'w') as newfile:\n",
    "            try: newfile.copy(file['header_table'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['sensor_configuration'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['crs'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['observed_variable'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['source_configuration'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['station_configuration'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['station_type'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['station_configuration_codes'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['units'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.copy(file['z_coordinate_type'],newfile)\n",
    "            except: pass\n",
    "            try: newfile.create_dataset('dateindex', data=file['dateindex'][:]) \n",
    "            except: pass\n",
    "    \n",
    "    data =  eua.CDMDataset(fn)\n",
    "    allvars = data.observations_table.observed_variable[()]\n",
    "    allvars.sort()\n",
    "    allvars = numpy.unique(allvars)\n",
    "    #\n",
    "    ri = data.recordindex[()]\n",
    "#     print('recordindex: ', len(ri))\n",
    "    rt = data.recordtimestamp[()]\n",
    "    keys = data.observations_table.keys()[:-1]\n",
    "    fbkeys = data.era5fb.keys()[:-1]\n",
    "    # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "    pops = []\n",
    "    for i in range(len(keys)):\n",
    "        if 'string' in keys[i]:\n",
    "            pops.append(keys[i])\n",
    "    for i in pops: keys.remove(i)\n",
    "    pops = []\n",
    "    for i in range(len(fbkeys)):\n",
    "        if 'string' in fbkeys[i]:\n",
    "            pops.append(fbkeys[i])\n",
    "    for i in pops: fbkeys.remove(i)\n",
    "\n",
    "    recordindices = [[] for i in range(len(allvars))]\n",
    "    recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "    # output variables (from observations_table)\n",
    "    ov = []\n",
    "    for o in keys:\n",
    "        ov.append([[] for i in range(len(allvars))])\n",
    "    fb = []\n",
    "    for o in fbkeys:\n",
    "        fb.append([[] for i in range(len(allvars))])\n",
    "    #\n",
    "    # loading the observed_variables\n",
    "    #\n",
    "    obsv = data.observations_table.observed_variable[:]\n",
    "    #\n",
    "    # resorting the data\n",
    "    #\n",
    "#     print('resort:start')\n",
    "    @njit\n",
    "    def make_vrindex(vridx,ridx,idx):\n",
    "        l=0\n",
    "        for i in range(1,len(idx)): # to set the recordindices\n",
    "            if ridx[i]>ridx[i-1]:\n",
    "                vridx[ridx[i-1]:ridx[i]]=l # next record after l\n",
    "                l=i\n",
    "        vridx[ridx[i]:]=len(idx) # next record for the last element is the len of the data\n",
    "\n",
    "\n",
    "    tt=time.time()\n",
    "\n",
    "    ridxall=np.zeros(obsv.shape[0],dtype=np.int64) # reverse index - index of the record index\n",
    "    j=-1\n",
    "    for j in range(len(ri)-1):\n",
    "        ridxall[ri[j]:ri[j+1]]=j\n",
    "    j+=1\n",
    "    ridxall[ri[j]:]=j # for the last elemenet\n",
    "    ridx=[]\n",
    "    vridx=[]\n",
    "    absidx=[]\n",
    "    abscount=0\n",
    "    for j in range(len(allvars)):\n",
    "        idx=np.where(obsv==allvars[j])[0] # index of all elements form certain variable j\n",
    "#         print(j,len(idx),',',end='')\n",
    "        vridx.append(np.zeros(ri.shape[0],dtype=np.int64)) # all zeros in lenght of record index\n",
    "        ridx=ridxall[idx] # ridxall where variable is j\n",
    "        make_vrindex(vridx[-1],ridx,idx)\n",
    "        vridx[-1]+=abscount # abscount for stacking the recordindex\n",
    "\n",
    "        absidx.append(copy.copy(idx)) # why copy? - to make sure it's not just the ref. - maybe ok without the cp\n",
    "        abscount+=len(idx)\n",
    "\n",
    "#     print('')\n",
    "    #\n",
    "    # finishing the sorting \n",
    "    #\n",
    "    absidx=np.concatenate(absidx)\n",
    "    #\n",
    "    # recordtimestamps are only necessary once\n",
    "    #\n",
    "    recordtimestamps = recordtimestamps[0]\n",
    "    #\n",
    "    # targetfile has to be a copy of the original file\n",
    "    #\n",
    "    targetfile = '/raid60/scratch/uli/resorted_files/'+fn.split('/')[-1]# 0-20000-0-63894_CEUAS_merged_v0.nc'\n",
    "    if os.path.isfile(targetfile):\n",
    "        mode='r+'\n",
    "    else:\n",
    "        mode='w'\n",
    "#     print()\n",
    "#     print('writing '+targetfile)\n",
    "\n",
    "    for i in range(len(keys)):\n",
    "        ov_vars = data.observations_table[keys[i]][:]\n",
    "        ov_vars = ov_vars[absidx]\n",
    "        if keys[i] == 'index':\n",
    "            pass\n",
    "        elif keys[i] == 'observation_id' or keys[i] == 'report_id' or keys[i] == 'sensor_id' or keys[i] == 'source_id':\n",
    "            alldict = {keys[i]:np.asarray(ov_vars, dtype='S1')}\n",
    "            write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])\n",
    "        else:\n",
    "            alldict = pandas.DataFrame({keys[i]:ov_vars})\n",
    "            write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])  \n",
    "\n",
    "    for i in range(len(fbkeys)):\n",
    "        fb_vars = data.era5fb[fbkeys[i]][:]\n",
    "        fb_vars = fb_vars[absidx]\n",
    "        if fbkeys[i] == 'index' or fbkeys[i] == 'string6' or fbkeys[i] == 'string7' or fbkeys[i] == 'string10':\n",
    "            pass\n",
    "        elif fbkeys[i] == 'expver' or fbkeys[i] == 'source@hdr' or fbkeys[i] == 'source_id' or fbkeys[i] == 'statid@hdr':\n",
    "            alldict = {fbkeys[i]:np.asarray(fb_vars, dtype='S1')}\n",
    "            write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]])\n",
    "        else:\n",
    "            alldict = pandas.DataFrame({fbkeys[i]:fb_vars})\n",
    "            write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]]) \n",
    "    #\n",
    "    # writing the recordindices and recordtimestamp.\n",
    "    #       \n",
    "    recordindices=vridx\n",
    "    for i in range(len(recordindices)):\n",
    "        testvar = pandas.DataFrame({str(allvars[i]):recordindices[i]})\n",
    "        write_dict_h5(targetfile, testvar, 'recordindices', {str(allvars[i]): { 'compression': None } }, [str(allvars[i])]) \n",
    "\n",
    "    write_dict_h5(targetfile, {'recordtimestamp':rt}, 'recordindices', {'recordtimestamp': { 'compression': None } }, ['recordtimestamp'])\n",
    "\n",
    "    print('elapsed:',time.time()-tt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 1.740513801574707\n"
     ]
    }
   ],
   "source": [
    "do_resort('/raid60/scratch/federico/MERGED_DATABASE_OCTOBER2020_sensor/0-20000-0-22165_CEUAS_merged_v0.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 31.17629098892212\n",
      "elapsed: 117.4464168548584\n",
      "elapsed: 133.2662889957428\n",
      "elapsed: 245.76932835578918\n",
      "elapsed: 266.817058801651\n",
      "elapsed: 352.21881437301636\n",
      "elapsed: 359.7045419216156\n",
      "elapsed: 371.9640471935272\n",
      "elapsed: 372.3046760559082\n",
      "elapsed: 422.12972688674927\n",
      "elapsed: 470.8770101070404\n",
      "elapsed: 474.2775387763977\n",
      "elapsed: 491.6773679256439elapsed:\n",
      " 493.1476948261261\n",
      "elapsed: 514.2700593471527\n",
      "elapsed: 518.7990231513977\n",
      "elapsed: 545.7188308238983\n",
      "elapsed: 580.1745924949646\n",
      "elapsed: 583.7271282672882\n",
      "elapsed: 620.4465999603271\n",
      "elapsed: 639.024792432785\n",
      "elapsed: 643.4347960948944\n",
      "elapsed: 701.2159516811371\n",
      "elapsed: 694.8717873096466\n",
      "elapsed: 708.7162294387817\n",
      "elapsed: 704.3860831260681\n",
      "elapsed: 739.2533106803894\n",
      "elapsed: 744.4398629665375\n",
      "elapsed: 121.97534537315369\n",
      "elapsed: 666.1423337459564\n",
      "elapsed: 801.162394285202\n",
      "elapsed: 107.39090347290039\n",
      "elapsed: 807.105179309845\n",
      "elapsed: 26.162172555923462\n",
      "elapsed: 824.1924254894257\n",
      "elapsed: 818.522308588028\n",
      "elapsed: 858.9893069267273\n",
      "elapsed: 860.1200203895569\n",
      "elapsed: 868.622321844101\n",
      "elapsed: 871.4404425621033\n",
      "elapsed: 15.048321723937988\n",
      "elapsed: 889.4467439651489\n",
      "elapsed: 890.2042288780212\n",
      "elapsed: 920.382462978363\n",
      "elapsed: 934.4618408679962\n",
      "elapsed: 959.6242029666901\n",
      "elapsed: 731.7319965362549\n",
      "elapsed: 806.1398243904114\n",
      "elapsed: 362.2804288864136\n",
      "elapsed: 762.571270942688\n",
      "elapsed: 735.0828859806061\n",
      "elapsed:elapsed:  801.3141131401062825.3802926540375\n",
      "\n",
      "elapsed: 701.4848673343658\n",
      "elapsed: 838.4940330982208\n",
      "elapsed: 1106.3065888881683\n",
      "elapsed: 776.6135244369507\n",
      "elapsed: 715.2629590034485\n",
      "elapsed: 846.4255890846252\n",
      "elapsed: 804.9706530570984\n",
      "elapsed: 748.7102403640747\n",
      "elapsed: 802.5762929916382\n",
      "elapsed: 782.1046175956726\n",
      "elapsed: 831.8188178539276\n",
      "elapsed: 763.7153687477112\n",
      "elapsed: 748.7185230255127\n",
      "elapsed: 827.7037057876587\n",
      "elapsed: 734.8296489715576\n",
      "elapsed: 1071.5883209705353\n",
      "elapsed: 1086.539945602417\n",
      "elapsed: 801.2215683460236\n",
      "elapsed: elapsed: 778.0224416255951780.1906487941742\n",
      "\n",
      "elapsed: 856.793794631958\n",
      "elapsed: 755.7638292312622\n",
      "elapsed: 963.3199224472046\n",
      "elapsed: 891.461829662323\n",
      "elapsed: 20.57757592201233\n",
      "elapsed: 850.3806178569794\n",
      "elapsed: 932.1714880466461\n",
      "elapsed: 806.744579076767\n",
      "elapsed: 872.6950986385345\n",
      "elapsed: 1062.3835515975952\n",
      "elapsed: 944.503977060318\n",
      "elapsed: 890.9003481864929\n",
      "elapsed: 888.0826146602631\n",
      "elapsed: 819.0818755626678\n",
      "elapsed: 1087.5563974380493\n",
      "elapsed: 763.3250155448914\n",
      "elapsed: 875.7715580463409\n",
      "elapsed: 699.4140872955322\n",
      "elapsed: 744.8496491909027\n",
      "elapsed: 1092.5323450565338\n",
      "elapsed: 880.6285152435303\n",
      "elapsed: 819.9320175647736\n",
      "elapsed: 899.5149059295654\n",
      "elapsed: 878.8396360874176\n",
      "elapsed: 828.2196078300476\n",
      "elapsed: 951.2076251506805\n",
      "elapsed: 822.0021569728851\n",
      "elapsed: 760.2244238853455\n",
      "elapsed: 728.0518953800201\n",
      "elapsed: 976.0696396827698\n",
      "elapsed: 965.538272857666\n",
      "elapsed: 1028.4136188030243\n",
      "elapsed: 801.4678592681885\n",
      "elapsed: 958.0480399131775\n",
      "elapsed: 764.2469925880432\n",
      "elapsed: 883.8616843223572\n",
      "elapsed: 1035.956493139267\n",
      "elapsed: 1013.982097864151\n",
      "elapsed: 959.0466544628143\n",
      "elapsed: 994.4727458953857\n",
      "elapsed: 1149.362936258316\n",
      "elapsed: 967.6179296970367\n",
      "elapsed: 1041.6633930206299\n",
      "elapsed: 1240.0505802631378\n",
      "elapsed: 1201.236732006073\n",
      "elapsed: 1234.3408823013306\n",
      "elapsed: 1312.3141160011292\n",
      "elapsed: 1289.723163843155\n",
      "elapsed: 1304.8417263031006\n",
      "elapsed: 1269.6038711071014\n",
      "elapsed: 1298.6964440345764\n",
      "elapsed: 1237.6799895763397\n",
      "elapsed: 1252.7490816116333\n",
      "elapsed: 1293.4907474517822\n",
      "elapsed: 1270.7929813861847\n",
      "elapsed: 1377.7298312187195\n",
      "elapsed: 1246.2023344039917\n",
      "elapsed: 1086.2915189266205\n",
      "elapsed: 1452.5626385211945\n",
      "elapsed: 1331.057825088501\n",
      "elapsed: 1529.020557641983\n",
      "elapsed: 1112.3623776435852\n",
      "elapsed: 1113.7145392894745\n",
      "elapsed: 1038.7944169044495\n",
      "elapsed: 1383.2786333560944\n",
      "elapsed: 1318.519068479538\n",
      "elapsed: 1473.3103291988373\n",
      "elapsed: 1314.6585409641266\n",
      "elapsed: 1177.946308851242\n",
      "elapsed: 1214.0089809894562\n",
      "elapsed: 1504.7819378376007\n",
      "elapsed: 1239.6123659610748\n",
      "elapsed: 1902.8189496994019\n",
      "elapsed: 1930.8751139640808\n",
      "elapsed: 1864.6367914676666\n",
      "elapsed: 2276.7817175388336\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    pool = multiprocessing.Pool(processes=40)\n",
    "    result_list = pool.map(do_resort, files[6700:])\n",
    "    print(result_list)\n",
    "\n",
    "# 4250:4300\n",
    "# 4250:4255\n",
    "# sonst alle bis 5500\n",
    "# for i in range(len(files)):\n",
    "#     do_resort(files[i])\n",
    "#     if i > 2: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid60/scratch/federico/MERGED_DATABASE_OCTOBER2020_sensor/0-20000-0-22165_CEUAS_merged_v0.nc']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[4253:4254]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/raid60/scratch/federico/MERGED_DATABASE_OCTOBER2020_sensor/0-20000-0-22165_CEUAS_merged_v0.nc'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[4253]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allvars = data.observations_table.observed_variable[()]\n",
    "# allvars.sort()\n",
    "# allvars = np.unique(allvars)\n",
    "# # recordindices einzeln sortieren - fÃ¼r alle variablen eigene listen erstellen - dazu recordindices erstellen - alle daten hintereinander legen - len aller vorhergehender rec_in zu den nachkommenden addieren.\n",
    "# #\n",
    "# ri = data.recordindex[()]\n",
    "# rt = data.recordtimestamp[()]\n",
    "# keys = data.observations_table.keys()\n",
    "# recordindices = [[] for i in range(len(np.unique(allvars)))]\n",
    "# recordtimestamps = [[] for i in range(len(np.unique(allvars)))]\n",
    "\n",
    "# variables = [[] for i in range(len(np.unique(allvars)))]\n",
    "# values = [[] for i in range(len(np.unique(allvars)))]\n",
    "# zcoords = [[] for i in range(len(np.unique(allvars)))]\n",
    "\n",
    "# for i in range(len(ri)):\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         break\n",
    "        \n",
    "#     a = data.observations_table.observed_variable[()][start:end]\n",
    "#     b = data.observations_table.observation_value[()][start:end]\n",
    "#     c = data.observations_table.z_coordinate[()][start:end]\n",
    "\n",
    "#     sa, sb = zip(*sorted(zip(a, b)))\n",
    "#     sa, sc = zip(*sorted(zip(a, c)))\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 variables[j].append(sa[k])\n",
    "#                 values[j].append(sb[k])\n",
    "#                 zcoords[j].append(sc[k])\n",
    "#         recordindices[j].append(len(variables[j]))\n",
    "#         recordtimestamps[j].append(rt[i])\n",
    "        \n",
    "# lenadd = 0\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(variables[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allvars = data.observations_table.observed_variable[()]\n",
    "# allvars.sort()\n",
    "# allvars = np.unique(allvars)\n",
    "# #\n",
    "# ri = data.recordindex[()]\n",
    "# rt = data.recordtimestamp[()]\n",
    "# keys = data.observations_table.keys()[:-1]\n",
    "# recordindices = [[] for i in range(len(np.unique(allvars)))]\n",
    "# recordtimestamps = [[] for i in range(len(np.unique(allvars)))]\n",
    "\n",
    "# # output variables (from observations_table)\n",
    "# ov = []\n",
    "# for o in keys:\n",
    "#     ov.append([[] for i in range(len(np.unique(allvars)))])\n",
    "    \n",
    "# for i in range(len(ri)):\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         break\n",
    "        \n",
    "#     a = data.observations_table.observed_variable[()][start:end]\n",
    "#     helpvar = []\n",
    "#     for o in range(len(keys)):\n",
    "#         b = data.observations_table[keys[o]][()][start:end]\n",
    "#         try: # all varibales with different shape won't work here\n",
    "#             sa, sb = zip(*sorted(zip(a, b)))\n",
    "#             helpvar.append(sb)\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 for m in range(len(helpvar)):\n",
    "#                     try: # all varibales with different shape won't work here\n",
    "#                         ov[m][j].append(helpvar[m][k])\n",
    "#                     except:\n",
    "#                         pass\n",
    "#         recordindices[j].append(len(ov[0][j]))\n",
    "#         recordtimestamps[j].append(rt[i])\n",
    "        \n",
    "# lenadd = 0\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(variables[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# #\n",
    "# allvars = data.observations_table.observed_variable[()]\n",
    "# allvars.sort()\n",
    "# allvars = np.unique(allvars)\n",
    "# #\n",
    "# ri = data.recordindex[()]\n",
    "# print(len(ri))\n",
    "# # rt = data.recordtimestamp[()]\n",
    "# keys = data.observations_table.keys()[:-1]\n",
    "# # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "# keys.pop(43)\n",
    "# keys.pop(42)\n",
    "# keys.pop(41)\n",
    "# recordindices = [[] for i in range(len(allvars))]\n",
    "# recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "# # output variables (from observations_table)\n",
    "# ov = []\n",
    "# for o in keys:\n",
    "#     ov.append([[] for i in range(len(np.unique(allvars)))])    \n",
    "    \n",
    "# print('resort:start')\n",
    "# for i in range(len(ri)):\n",
    "#     print(i)\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         start = ri[i]\n",
    "#         end = len(data.observations_table.observed_variable[()])\n",
    "        \n",
    "#     a = data.observations_table.observed_variable[()][start:end]\n",
    "#     helpvar = []\n",
    "#     for o in range(len(keys)):\n",
    "#         if keys[o] == 'observation_id' or keys[o] == 'report_id' or keys[o] == 'sensor_id' or keys[o] == 'source_id':\n",
    "#             b = []\n",
    "#             for n in data.observations_table[keys[o]][()][start:end]:\n",
    "#                 c = ''\n",
    "#                 for bb in n:\n",
    "#                     c = c + bb.decode()\n",
    "#                 b.append(c)\n",
    "#         else:\n",
    "#             b = data.observations_table[keys[o]][()][start:end]\n",
    "#         sa, sb = zip(*sorted(zip(a, b)))\n",
    "#         helpvar.append(sb)\n",
    "\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 for m in range(len(helpvar)):\n",
    "#                     ov[m][j].append(helpvar[m][k])\n",
    "#         recordindices[j].append(len(ov[0][j]))\n",
    "# #         recordtimestamps[j].append(rt[i])\n",
    "\n",
    "# print('resort:done')\n",
    "        \n",
    "# #\n",
    "# # setting record_indices to the right value -> stacking them\n",
    "# #\n",
    "# lenadd = 0.\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(ov[0][i])\n",
    "\n",
    "# #\n",
    "# # shaping record_indices -> setting every missing value to nan\n",
    "# #\n",
    "# old = -1\n",
    "# for i in range(len(recordindices)):\n",
    "#     for j in range(len(recordindices[i])):\n",
    "#         if recordindices[i][j] != 0 and recordindices[i][j] == recordindices[i][j-1]:\n",
    "#             old = recordindices[i][j]\n",
    "#             recordindices[i][j] = np.nan\n",
    "#         elif recordindices[i][j] == old:\n",
    "#             recordindices[i][j] = np.nan\n",
    "\n",
    "# #\n",
    "# # recordtimestamps are only necessary once\n",
    "# #\n",
    "# recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "# # \n",
    "# # stacking all output variables\n",
    "# #\n",
    "# out = []\n",
    "# for j in ov:\n",
    "#     finalvar = []\n",
    "#     for i in j:\n",
    "#         finalvar = finalvar + i\n",
    "#     out.append(finalvar)\n",
    "    \n",
    "# #\n",
    "# # restoring byte arrays:\n",
    "# #\n",
    "# for o in range(len(keys)):\n",
    "#     if keys[o] == 'observation_id' or keys[o] == 'report_id' or keys[o] == 'sensor_id' or keys[o] == 'source_id':\n",
    "#         b = []\n",
    "#         for n in range(len(out[o])):\n",
    "#             c = []\n",
    "#             for bb in out[o][n]:\n",
    "#                 c.append(bb.encode())\n",
    "#             out[o][n] = c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allvars = data.observations_table.observed_variable[()]\n",
    "# allvars.sort()\n",
    "# allvars = np.unique(allvars)\n",
    "# #\n",
    "# ri = data.recordindex[()]\n",
    "# print('recordindex: ', len(ri))\n",
    "# # rt = data.recordtimestamp[()]\n",
    "# keys = data.observations_table.keys()[:-1]\n",
    "# # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "# keys.pop(43)\n",
    "# keys.pop(42)\n",
    "# keys.pop(41)\n",
    "# recordindices = [[] for i in range(len(allvars))]\n",
    "# recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "# # output variables (from observations_table)\n",
    "# ov = []\n",
    "# for o in keys:\n",
    "#     ov.append([[] for i in range(len(np.unique(allvars)))])\n",
    "\n",
    "# #\n",
    "# # load data into memory and decode byte arrays\n",
    "# #\n",
    "# print('loading data')\n",
    "# obsv = data.observations_table.observed_variable[:]\n",
    "# ov_vars = []\n",
    "# for o in range(len(keys)):\n",
    "#     ov_vars.append(data.observations_table[keys[o]][:])\n",
    "#     if keys[o] == 'observation_id' or keys[o] == 'report_id' or keys[o] == 'sensor_id' or keys[o] == 'source_id':\n",
    "#         b = []\n",
    "#         for n in ov_vars[o]:\n",
    "#             c = ''\n",
    "#             for bb in n:\n",
    "#                 c = c + bb.decode()\n",
    "#             b.append(c)\n",
    "#         ov_vars[o] = b\n",
    "\n",
    "# #\n",
    "# # resorting the data\n",
    "# #\n",
    "# print('resort:start')\n",
    "# for i in range(len(ri)):\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         start = ri[i]\n",
    "#         end = len(data.observations_table.observed_variable[()])\n",
    "        \n",
    "#     a = obsv[start:end]\n",
    "#     helpvar = []\n",
    "#     for o in range(len(keys)):\n",
    "#         b = ov_vars[o][start:end]\n",
    "#         sa, sb = zip(*sorted(zip(a, b)))\n",
    "#         helpvar.append(sb)\n",
    "\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 for m in range(len(helpvar)):\n",
    "#                     ov[m][j].append(helpvar[m][k])\n",
    "#         recordindices[j].append(len(ov[0][j]))\n",
    "# #         recordtimestamps[j].append(rt[i])\n",
    "\n",
    "# print('resort:done')\n",
    "        \n",
    "# #\n",
    "# # setting record_indices to the right value -> stacking them\n",
    "# #\n",
    "# lenadd = 0.\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(ov[0][i])\n",
    "\n",
    "# #\n",
    "# # shaping record_indices -> setting every missing value to nan\n",
    "# #\n",
    "# print('shaping record_indices')\n",
    "# old = -1\n",
    "# for i in range(len(recordindices)):\n",
    "#     for j in range(len(recordindices[i])):\n",
    "#         if recordindices[i][j] != 0 and recordindices[i][j] == recordindices[i][j-1]:\n",
    "#             old = recordindices[i][j]\n",
    "#             recordindices[i][j] = np.nan\n",
    "#         elif recordindices[i][j] == old:\n",
    "#             recordindices[i][j] = np.nan\n",
    "\n",
    "# #\n",
    "# # recordtimestamps are only necessary once\n",
    "# #\n",
    "# recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "# # \n",
    "# # stacking all output variables\n",
    "# #\n",
    "# print('stacking output variables')\n",
    "# out = []\n",
    "# for j in ov:\n",
    "#     finalvar = []\n",
    "#     for i in j:\n",
    "#         finalvar = finalvar + i\n",
    "#     out.append(finalvar)\n",
    "    \n",
    "# #\n",
    "# # restoring byte arrays:\n",
    "# # this takes very long -> find faster option\n",
    "# #\n",
    "# print('restoring byte arrays')\n",
    "# bytelist = [25, 34, 38, 39]\n",
    "# for o in bytelist:\n",
    "#     originallen = len(data.observations_table[keys[o]][()][0])\n",
    "#     for n in range(len(out[o])):\n",
    "#         c = [elem.encode() for elem in out[o][n]]\n",
    "#         # problem: if the string was [b'x', b'y', b''] in the first place, it get converted to 'xy' and then back to [b'x', b'y']\n",
    "#         # add empty byte strings until the data is as long as before:\n",
    "#         while(len(c) < originallen):\n",
    "#             c.append(str('').encode())\n",
    "#         out[o][n] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with eua.CDMDataset(files[0]) as data:\n",
    "#     allvars = data.observations_table.observed_variable[()]\n",
    "#     allvars.sort()\n",
    "#     allvars = np.unique(allvars)\n",
    "#     #\n",
    "#     ri = data.recordindex[()]\n",
    "#     print('recordindex: ', len(ri))\n",
    "#     keys = data.observations_table.keys()[:-1]\n",
    "#     # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "#     keys.pop(43)\n",
    "#     keys.pop(42)\n",
    "#     keys.pop(41)\n",
    "#     recordindices = [[] for i in range(len(allvars))]\n",
    "#     recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "#     # output variables (from observations_table)\n",
    "#     ov = []\n",
    "#     for o in keys:\n",
    "#         ov.append([[] for i in range(len(np.unique(allvars)))])\n",
    "\n",
    "#     #\n",
    "#     # load data into memory and decode byte arrays\n",
    "#     #\n",
    "#     print('loading data')\n",
    "#     obsv = data.observations_table.observed_variable[:]\n",
    "#     ov_vars = []\n",
    "#     for o in range(len(keys)):\n",
    "#         ov_vars.append(data.observations_table[keys[o]][:])\n",
    "\n",
    "# #\n",
    "# # resorting the data\n",
    "# #\n",
    "# print('resort:start')\n",
    "# for i in range(len(ri)):\n",
    "#     try:\n",
    "#         start = ri[i]\n",
    "#         end = ri[i+1]\n",
    "#     except:\n",
    "#         start = ri[i]\n",
    "#         end = len(obsv)\n",
    "        \n",
    "#     a = obsv[start:end]\n",
    "#     sa, sortindex = zip(*sorted(zip(a, range(len(a)))))\n",
    "#     sortindex = np.array(sortindex)\n",
    "#     helpvar = []\n",
    "#     for o in range(len(keys)):\n",
    "#         b = ov_vars[o][start:end]\n",
    "#         helpvar.append(b[sortindex])\n",
    "\n",
    "#     for j in range(len(allvars)):\n",
    "#         for k in range(len(sa)):\n",
    "#             if sa[k] == allvars[j]:\n",
    "#                 for m in range(len(helpvar)):\n",
    "#                     ov[m][j].append(helpvar[m][k])\n",
    "#         recordindices[j].append(len(ov[0][j]))\n",
    "\n",
    "# print('resort:done')\n",
    "        \n",
    "# #\n",
    "# # setting record_indices to the right value -> stacking them\n",
    "# #\n",
    "# lenadd = 0.\n",
    "# for i in range(len(recordindices)):\n",
    "#     recordindices[i] = np.asarray(np.append([0], recordindices[i][:-1])) + lenadd\n",
    "#     lenadd += len(ov[0][i])\n",
    "\n",
    "# #\n",
    "# # shaping record_indices -> setting every missing value to nan\n",
    "# #\n",
    "# print('shaping record_indices')\n",
    "# old = -1\n",
    "# for i in range(len(recordindices)):\n",
    "#     for j in range(len(recordindices[i])):\n",
    "#         if i != 0 or j != 0:\n",
    "#             if recordindices[i][j] == recordindices[i][j-1]:\n",
    "#                 old = recordindices[i][j]\n",
    "#                 recordindices[i][j] = np.nan\n",
    "#             elif recordindices[i][j] == old:\n",
    "#                 recordindices[i][j] = np.nan\n",
    "\n",
    "# #\n",
    "# # recordtimestamps are only necessary once\n",
    "# #\n",
    "# recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "# # \n",
    "# # stacking all output variables\n",
    "# #\n",
    "# print('stacking output variables')\n",
    "# out = []\n",
    "# for k in ov:\n",
    "#     out.append([j for i in k for j in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_resort(fn):\n",
    "    \n",
    "#     data = eua.CDMDataset(fn)\n",
    "\n",
    "#     allvars = data.observations_table.observed_variable[()]\n",
    "#     allvars.sort()\n",
    "#     allvars = numpy.unique(allvars)\n",
    "#     #\n",
    "#     ri = data.recordindex[()]\n",
    "#     print('recordindex: ', len(ri))\n",
    "#     rt = data.recordtimestamp[()]\n",
    "#     keys = data.observations_table.keys()[:-1]\n",
    "#     fbkeys = data.era5fb.keys()[:]\n",
    "#     # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "#     keys.pop(43)\n",
    "#     keys.pop(42)\n",
    "#     keys.pop(41)\n",
    "#     recordindices = [[] for i in range(len(allvars))]\n",
    "#     recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "#     # output variables (from observations_table)\n",
    "#     ov = []\n",
    "#     for o in keys:\n",
    "#         ov.append([[] for i in range(len(numpy.unique(allvars)))])\n",
    "\n",
    "#     #\n",
    "#     # load data into memory and decode byte arrays\n",
    "#     #\n",
    "#     print('loading data')\n",
    "#     obsv = data.observations_table.observed_variable[:]\n",
    "#     ov_vars = []\n",
    "#     for o in range(len(keys)):\n",
    "#         ov_vars.append(data.observations_table[keys[o]][:])\n",
    "\n",
    "#     #\n",
    "#     # resorting the data\n",
    "#     #\n",
    "#     print('resort:start')\n",
    "#     @njit\n",
    "#     def make_vrindex(vridx,ridx,idx):\n",
    "#         l=0\n",
    "#         for i in range(1,len(idx)): # to set the recordindices\n",
    "#             if ridx[i]>ridx[i-1]:\n",
    "#                 vridx[ridx[i-1]:ridx[i]]=l # next record after l\n",
    "#                 l=i\n",
    "#         vridx[ridx[i]:]=len(idx) # next record for the last element is the len of the data\n",
    "\n",
    "\n",
    "#     tt=time.time()\n",
    "\n",
    "#     ridxall=np.zeros(ov_vars[0].shape[0],dtype=np.int64) # reverse index - index of the record index\n",
    "#     j=-1\n",
    "#     for j in range(len(ri)-1):\n",
    "#         ridxall[ri[j]:ri[j+1]]=j\n",
    "#     j+=1\n",
    "#     ridxall[ri[j]:]=j # for the last elemenet\n",
    "\n",
    "#     ridx=[]\n",
    "#     vridx=[]\n",
    "#     absidx=[]\n",
    "#     abscount=0\n",
    "#     for j in range(len(allvars)):\n",
    "#         idx=np.where(ov_vars[keys.index('observed_variable')]==allvars[j])[0] # index of all elements form certain variable j\n",
    "#         print(j,len(idx),',',end='')\n",
    "#         vridx.append(np.zeros(ri.shape[0],dtype=np.int64)) # all zeros in lenght of record index\n",
    "#         ridx=ridxall[idx] # ridxall where variable is j\n",
    "#         make_vrindex(vridx[-1],ridx,idx)\n",
    "#         vridx[-1]+=abscount # abscount for stacking the recordindex\n",
    "\n",
    "#         absidx.append(copy.copy(idx)) # why copy? - to make sure it's not just the ref. - maybe ok without the cp\n",
    "#         abscount+=len(idx)\n",
    "\n",
    "#     print('')\n",
    "#     absidx=np.concatenate(absidx)\n",
    "#     for o in range(len(keys)):\n",
    "#         ov_vars[o]=ov_vars[o][absidx]\n",
    "#         print(o,end='')\n",
    "#     #\n",
    "#     # recordtimestamps are only necessary once\n",
    "#     #\n",
    "#     recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "\n",
    "#     out = ov_vars\n",
    "#     # targetfile has to be a copy of the original file\n",
    "#     targetfile = '/raid60/scratch/uli/resorted_files_20201109/'+fn.split('/')[-1]# 0-20000-0-63894_CEUAS_merged_v0.nc'\n",
    "#     if os.path.isfile(targetfile):\n",
    "#         mode='r+'\n",
    "#     else:\n",
    "#         mode='w'\n",
    "        \n",
    "#     print()\n",
    "#     print('writing '+targetfile)\n",
    "#     #\n",
    "#     # writing data into observations_table\n",
    "#     #\n",
    "#     with h5py.File(targetfile, mode) as file:\n",
    "#         for i in range(len(keys)):\n",
    "#             try:\n",
    "#                 del file['observations_table'][keys[i]]\n",
    "#             except:\n",
    "#                 pass\n",
    "#     for i in range(len(keys)):\n",
    "#         if keys[i] == 'index':\n",
    "#             pass\n",
    "#         elif keys[i] == 'observation_id' or keys[i] == 'report_id' or keys[i] == 'sensor_id' or keys[i] == 'source_id':\n",
    "# #             slen = len(out[i][0])\n",
    "#             alldict = {keys[i]:np.asarray(out[i], dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({keys[i]:out[i]})\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])  \n",
    "#         #\n",
    "#         # writing the recordindices and recordtimestamp.\n",
    "#         #       \n",
    "#     recordindices=vridx\n",
    "#     with h5py.File(targetfile, 'r+') as file:\n",
    "#         for i in range(len(recordindices)):\n",
    "#             try:\n",
    "#                 del file['recordindices'][str(allvars[i])]\n",
    "#             except:\n",
    "#                 pass\n",
    "#     for i in range(len(recordindices)):\n",
    "#         testvar = pandas.DataFrame({str(allvars[i]):recordindices[i]})\n",
    "#         write_dict_h5(targetfile, testvar, 'recordindices', {str(allvars[i]): { 'compression': None } }, [str(allvars[i])]) \n",
    "\n",
    "#     with h5py.File(targetfile, 'r+') as file:\n",
    "#         try:\n",
    "#             del file['recordindex']\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             del file['recordtimestamp']\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             del file['recordindices']['recordtimestamp']\n",
    "#         except:\n",
    "#             pass\n",
    "#     write_dict_h5(targetfile, {'recordtimestamp':rt}, 'recordindices', {'recordtimestamp': { 'compression': None } }, ['recordtimestamp']) \n",
    "        \n",
    "#     print('elapsed:',time.time()-tt)\n",
    "#     print(end='')\n",
    "\n",
    "\n",
    "\n",
    "# # In[15]:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_resort(fn):\n",
    "    \n",
    "#     data = eua.CDMDataset(fn)\n",
    "\n",
    "#     allvars = data.observations_table.observed_variable[()]\n",
    "#     allvars.sort()\n",
    "#     allvars = numpy.unique(allvars)\n",
    "#     #\n",
    "#     ri = data.recordindex[()]\n",
    "#     print('recordindex: ', len(ri))\n",
    "#     rt = data.recordtimestamp[()]\n",
    "#     keys = data.observations_table.keys()[:-1]\n",
    "#     fbkeys = data.era5fb.keys()[:-1]\n",
    "#     # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "#     pops = []\n",
    "#     for i in range(len(keys)):\n",
    "#         if 'string' in keys[i]:\n",
    "#             pops.append(keys[i])\n",
    "#     for i in pops: keys.remove(i)\n",
    "#     pops = []\n",
    "#     for i in range(len(fbkeys)):\n",
    "#         if 'string' in fbkeys[i]:\n",
    "#             pops.append(fbkeys[i])\n",
    "#     for i in pops: fbkeys.remove(i)\n",
    "        \n",
    "#     recordindices = [[] for i in range(len(allvars))]\n",
    "#     recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "#     # output variables (from observations_table)\n",
    "#     ov = []\n",
    "#     for o in keys:\n",
    "#         ov.append([[] for i in range(len(allvars))])\n",
    "#     fb = []\n",
    "#     for o in fbkeys:\n",
    "#         fb.append([[] for i in range(len(allvars))])\n",
    "\n",
    "#     #\n",
    "#     # load data into memory and decode byte arrays\n",
    "#     #\n",
    "#     print('loading data')\n",
    "#     obsv = data.observations_table.observed_variable[:]\n",
    "#     ov_vars = []\n",
    "#     for o in range(len(keys)):\n",
    "#         ov_vars.append(data.observations_table[keys[o]][:])\n",
    "#     fb_vars = []\n",
    "#     for o in range(len(fbkeys)):\n",
    "#         fb_vars.append(data.era5fb[fbkeys[o]][:])\n",
    "\n",
    "#     #\n",
    "#     # resorting the data\n",
    "#     #\n",
    "#     print('resort:start')\n",
    "#     @njit\n",
    "#     def make_vrindex(vridx,ridx,idx):\n",
    "#         l=0\n",
    "#         for i in range(1,len(idx)): # to set the recordindices\n",
    "#             if ridx[i]>ridx[i-1]:\n",
    "#                 vridx[ridx[i-1]:ridx[i]]=l # next record after l\n",
    "#                 l=i\n",
    "#         vridx[ridx[i]:]=len(idx) # next record for the last element is the len of the data\n",
    "\n",
    "\n",
    "#     tt=time.time()\n",
    "\n",
    "#     ridxall=np.zeros(ov_vars[0].shape[0],dtype=np.int64) # reverse index - index of the record index\n",
    "#     j=-1\n",
    "#     for j in range(len(ri)-1):\n",
    "#         ridxall[ri[j]:ri[j+1]]=j\n",
    "#     j+=1\n",
    "#     ridxall[ri[j]:]=j # for the last elemenet\n",
    "\n",
    "#     ridx=[]\n",
    "#     vridx=[]\n",
    "#     absidx=[]\n",
    "#     abscount=0\n",
    "#     for j in range(len(allvars)):\n",
    "#         idx=np.where(ov_vars[keys.index('observed_variable')]==allvars[j])[0] # index of all elements form certain variable j\n",
    "#         print(j,len(idx),',',end='')\n",
    "#         vridx.append(np.zeros(ri.shape[0],dtype=np.int64)) # all zeros in lenght of record index\n",
    "#         ridx=ridxall[idx] # ridxall where variable is j\n",
    "#         make_vrindex(vridx[-1],ridx,idx)\n",
    "#         vridx[-1]+=abscount # abscount for stacking the recordindex\n",
    "\n",
    "#         absidx.append(copy.copy(idx)) # why copy? - to make sure it's not just the ref. - maybe ok without the cp\n",
    "#         abscount+=len(idx)\n",
    "\n",
    "#     print('')\n",
    "#     absidx=np.concatenate(absidx)\n",
    "#     for o in range(len(keys)):\n",
    "#         ov_vars[o]=ov_vars[o][absidx]\n",
    "#         print(o,end='')\n",
    "        \n",
    "#     for o in range(len(fbkeys)):\n",
    "#         fb_vars[o]=fb_vars[o][absidx]\n",
    "#         print(o,end='')\n",
    "\n",
    "#     #\n",
    "#     # recordtimestamps are only necessary once\n",
    "#     #\n",
    "#     recordtimestamps = recordtimestamps[0]\n",
    "\n",
    "\n",
    "#     out = ov_vars\n",
    "#     # targetfile has to be a copy of the original file\n",
    "#     targetfile = '/raid60/scratch/uli/resorted_files_20201109/'+fn.split('/')[-1]# 0-20000-0-63894_CEUAS_merged_v0.nc'\n",
    "#     if os.path.isfile(targetfile):\n",
    "#         mode='r+'\n",
    "#     else:\n",
    "#         mode='w'\n",
    "        \n",
    "#     print()\n",
    "#     print('writing '+targetfile)\n",
    "#     #\n",
    "#     # writing data into observations_table\n",
    "#     #\n",
    "#     with h5py.File(targetfile, mode) as file:\n",
    "#         for i in range(len(keys)):\n",
    "#             try:\n",
    "#                 del file['observations_table'][keys[i]]\n",
    "#             except:\n",
    "#                 pass\n",
    "#         for i in range(len(fbkeys)):\n",
    "#             try:\n",
    "#                 del file['era5fb'][fbkeys[i]]\n",
    "#             except:\n",
    "#                 pass\n",
    "#     for i in range(len(keys)):\n",
    "#         if keys[i] == 'index':\n",
    "#             pass\n",
    "#         elif keys[i] == 'observation_id' or keys[i] == 'report_id' or keys[i] == 'sensor_id' or keys[i] == 'source_id':\n",
    "# #             slen = len(out[i][0])\n",
    "#             alldict = {keys[i]:np.asarray(out[i], dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({keys[i]:out[i]})\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])  \n",
    "            \n",
    "#     for i in range(len(fbkeys)):\n",
    "#         if fbkeys[i] == 'index' or fbkeys[i] == 'string6' or fbkeys[i] == 'string7' or fbkeys[i] == 'string10':\n",
    "#             pass\n",
    "#         elif fbkeys[i] == 'expver' or fbkeys[i] == 'source@hdr' or fbkeys[i] == 'source_id' or fbkeys[i] == 'statid@hdr':\n",
    "# #             slen = len(out[i][0])\n",
    "#             alldict = {fbkeys[i]:np.asarray(fb_vars[i], dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({fbkeys[i]:fb_vars[i]})\n",
    "#             write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]]) \n",
    "#     #\n",
    "#     # writing the recordindices and recordtimestamp.\n",
    "#     #       \n",
    "#     recordindices=vridx\n",
    "#     with h5py.File(targetfile, 'r+') as file:\n",
    "#         for i in range(len(recordindices)):\n",
    "#             try:\n",
    "#                 del file['recordindices'][str(allvars[i])]\n",
    "#             except:\n",
    "#                 pass\n",
    "#     for i in range(len(recordindices)):\n",
    "#         testvar = pandas.DataFrame({str(allvars[i]):recordindices[i]})\n",
    "#         write_dict_h5(targetfile, testvar, 'recordindices', {str(allvars[i]): { 'compression': None } }, [str(allvars[i])]) \n",
    "\n",
    "#     with h5py.File(targetfile, 'r+') as file:\n",
    "#         try:\n",
    "#             del file['recordindex']\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             del file['recordtimestamp']\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             del file['recordindices']['recordtimestamp']\n",
    "#         except:\n",
    "#             pass\n",
    "#     write_dict_h5(targetfile, {'recordtimestamp':rt}, 'recordindices', {'recordtimestamp': { 'compression': None } }, ['recordtimestamp']) \n",
    "        \n",
    "#     print('elapsed:',time.time()-tt)\n",
    "#     print(end='')\n",
    "\n",
    "\n",
    "\n",
    "# # In[15]:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_resort(fn):\n",
    "#     targetfile = '/raid60/scratch/uli/resorted_files/'+fn.split('/')[-1]  \n",
    "    \n",
    "#     with h5py.File(fn, 'r') as file:\n",
    "#         with h5py.File(targetfile, 'w') as newfile:\n",
    "#             newfile.copy(file['header_table'],newfile)\n",
    "#             newfile.copy(file['sensor_configuration'],newfile)\n",
    "#             newfile.copy(file['crs'],newfile)\n",
    "#             newfile.copy(file['observed_variable'],newfile)\n",
    "#             newfile.copy(file['source_configuration'],newfile)\n",
    "#             newfile.copy(file['station_configuration'],newfile)\n",
    "#             newfile.copy(file['station_type'],newfile)\n",
    "#             newfile.copy(file['station_configuration_codes'],newfile)\n",
    "#             newfile.copy(file['units'],newfile)\n",
    "#             newfile.copy(file['z_coordinate_type'],newfile)\n",
    "            \n",
    "#             newfile.create_dataset('dateindex', data=file['dateindex'][:]) \n",
    "\n",
    "    \n",
    "#     data =  eua.CDMDataset(fn)\n",
    "#     allvars = data.observations_table.observed_variable[()]\n",
    "#     allvars.sort()\n",
    "#     allvars = numpy.unique(allvars)\n",
    "#     #\n",
    "#     ri = data.recordindex[()]\n",
    "# #     print('recordindex: ', len(ri))\n",
    "#     rt = data.recordtimestamp[()]\n",
    "#     keys = data.observations_table.keys()[:-1]\n",
    "#     fbkeys = data.era5fb.keys()[:-1]\n",
    "#     # dropping all keys, where dimensions won't work - just help variabels for dimensions\n",
    "#     pops = []\n",
    "#     for i in range(len(keys)):\n",
    "#         if 'string' in keys[i]:\n",
    "#             pops.append(keys[i])\n",
    "#     for i in pops: keys.remove(i)\n",
    "#     pops = []\n",
    "#     for i in range(len(fbkeys)):\n",
    "#         if 'string' in fbkeys[i]:\n",
    "#             pops.append(fbkeys[i])\n",
    "#     for i in pops: fbkeys.remove(i)\n",
    "\n",
    "#     recordindices = [[] for i in range(len(allvars))]\n",
    "#     recordtimestamps = [[] for i in range(len(allvars))]\n",
    "\n",
    "#     # output variables (from observations_table)\n",
    "#     ov = []\n",
    "#     for o in keys:\n",
    "#         ov.append([[] for i in range(len(allvars))])\n",
    "#     fb = []\n",
    "#     for o in fbkeys:\n",
    "#         fb.append([[] for i in range(len(allvars))])\n",
    "#     #\n",
    "#     # loading the observed_variables\n",
    "#     #\n",
    "#     obsv = data.observations_table.observed_variable[:]\n",
    "#     #\n",
    "#     # resorting the data\n",
    "#     #\n",
    "# #     print('resort:start')\n",
    "#     @njit\n",
    "#     def make_vrindex(vridx,ridx,idx):\n",
    "#         l=0\n",
    "#         for i in range(1,len(idx)): # to set the recordindices\n",
    "#             if ridx[i]>ridx[i-1]:\n",
    "#                 vridx[ridx[i-1]:ridx[i]]=l # next record after l\n",
    "#                 l=i\n",
    "#         vridx[ridx[i]:]=len(idx) # next record for the last element is the len of the data\n",
    "\n",
    "\n",
    "#     tt=time.time()\n",
    "\n",
    "#     ridxall=np.zeros(obsv.shape[0],dtype=np.int64) # reverse index - index of the record index\n",
    "#     j=-1\n",
    "#     for j in range(len(ri)-1):\n",
    "#         ridxall[ri[j]:ri[j+1]]=j\n",
    "#     j+=1\n",
    "#     ridxall[ri[j]:]=j # for the last elemenet\n",
    "#     ridx=[]\n",
    "#     vridx=[]\n",
    "#     absidx=[]\n",
    "#     abscount=0\n",
    "#     for j in range(len(allvars)):\n",
    "#         idx=np.where(obsv==allvars[j])[0] # index of all elements form certain variable j\n",
    "# #         print(j,len(idx),',',end='')\n",
    "#         vridx.append(np.zeros(ri.shape[0],dtype=np.int64)) # all zeros in lenght of record index\n",
    "#         ridx=ridxall[idx] # ridxall where variable is j\n",
    "#         make_vrindex(vridx[-1],ridx,idx)\n",
    "#         vridx[-1]+=abscount # abscount for stacking the recordindex\n",
    "\n",
    "#         absidx.append(copy.copy(idx)) # why copy? - to make sure it's not just the ref. - maybe ok without the cp\n",
    "#         abscount+=len(idx)\n",
    "\n",
    "# #     print('')\n",
    "#     #\n",
    "#     # finishing the sorting \n",
    "#     #\n",
    "#     absidx=np.concatenate(absidx)\n",
    "#     #\n",
    "#     # recordtimestamps are only necessary once\n",
    "#     #\n",
    "#     recordtimestamps = recordtimestamps[0]\n",
    "#     #\n",
    "#     # targetfile has to be a copy of the original file\n",
    "#     #\n",
    "#     targetfile = '/raid60/scratch/uli/resorted_files/'+fn.split('/')[-1]# 0-20000-0-63894_CEUAS_merged_v0.nc'\n",
    "#     if os.path.isfile(targetfile):\n",
    "#         mode='r+'\n",
    "#     else:\n",
    "#         mode='w'\n",
    "# #     print()\n",
    "# #     print('writing '+targetfile)\n",
    "\n",
    "#     #\n",
    "#     # writing data into observations_table\n",
    "#     #\n",
    "# #     with h5py.File(targetfile, mode) as file:\n",
    "# #         for i in range(len(keys)):\n",
    "# #             try:\n",
    "# #                 del file['observations_table'][keys[i]]\n",
    "# #             except:\n",
    "# #                 pass\n",
    "# #         for i in range(len(fbkeys)):\n",
    "# #             try:\n",
    "# #                 del file['era5fb'][fbkeys[i]]\n",
    "# #             except:\n",
    "# #                 pass\n",
    "#     for i in range(len(keys)):\n",
    "#         ov_vars = data.observations_table[keys[i]][:]\n",
    "#         ov_vars = ov_vars[absidx]\n",
    "#         if keys[i] == 'index':\n",
    "#             pass\n",
    "#         elif keys[i] == 'observation_id' or keys[i] == 'report_id' or keys[i] == 'sensor_id' or keys[i] == 'source_id':\n",
    "#             alldict = {keys[i]:np.asarray(ov_vars, dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({keys[i]:ov_vars})\n",
    "#             write_dict_h5(targetfile, alldict, 'observations_table', {keys[i]: { 'compression': 'gzip' } }, [keys[i]])  \n",
    "\n",
    "#     for i in range(len(fbkeys)):\n",
    "#         fb_vars = data.era5fb[fbkeys[i]][:]\n",
    "#         fb_vars = fb_vars[absidx]\n",
    "#         if fbkeys[i] == 'index' or fbkeys[i] == 'string6' or fbkeys[i] == 'string7' or fbkeys[i] == 'string10':\n",
    "#             pass\n",
    "#         elif fbkeys[i] == 'expver' or fbkeys[i] == 'source@hdr' or fbkeys[i] == 'source_id' or fbkeys[i] == 'statid@hdr':\n",
    "#             alldict = {fbkeys[i]:np.asarray(fb_vars, dtype='S1')}\n",
    "#             write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]])\n",
    "#         else:\n",
    "#             alldict = pandas.DataFrame({fbkeys[i]:fb_vars})\n",
    "#             write_dict_h5(targetfile, alldict, 'era5fb', {fbkeys[i]: { 'compression': 'gzip' } }, [fbkeys[i]]) \n",
    "#     #\n",
    "#     # writing the recordindices and recordtimestamp.\n",
    "#     #       \n",
    "#     recordindices=vridx\n",
    "# #     with h5py.File(targetfile, 'w') as file:\n",
    "# #         for i in range(len(recordindices)):\n",
    "# #             try:\n",
    "# #                 del file['recordindices'][str(allvars[i])]\n",
    "# #             except:\n",
    "# #                 pass\n",
    "#     for i in range(len(recordindices)):\n",
    "#         testvar = pandas.DataFrame({str(allvars[i]):recordindices[i]})\n",
    "#         write_dict_h5(targetfile, testvar, 'recordindices', {str(allvars[i]): { 'compression': None } }, [str(allvars[i])]) \n",
    "\n",
    "# #     with h5py.File(targetfile, 'w') as file:\n",
    "# #         try:\n",
    "# #             del file['recordindex']\n",
    "# #         except:\n",
    "# #             pass\n",
    "# #         try:\n",
    "# #             del file['recordtimestamp']\n",
    "# #         except:\n",
    "# #             pass\n",
    "# #         try:\n",
    "# #             del file['recordindices']['recordtimestamp']\n",
    "# #         except:\n",
    "# #             pass\n",
    "#     write_dict_h5(targetfile, {'recordtimestamp':rt}, 'recordindices', {'recordtimestamp': { 'compression': None } }, ['recordtimestamp'])\n",
    "# #     with h5py.File(targetfile, 'w') as newfile:\n",
    "# #         for i in data.header_table.keys():\n",
    "# #             alldict = pandas.DataFrame({i:data.header_table[i]})\n",
    "# #             write_dict_h5(targetfile, alldict, 'header_table', {i: { 'compression': 'gzip' } }, [i])\n",
    "\n",
    "#     print('elapsed:',time.time()-tt)\n",
    "# #     print(end='')\n",
    "\n",
    "\n",
    "\n",
    "# # In[15]:\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
