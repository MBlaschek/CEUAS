{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d02ff84-164b-4d00-9da9-223d71d2ba61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: ecCodes 2.21.0 or higher is recommended. You are running version 2.17.0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env\n",
    "# coding: utf-8\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob, sys\n",
    "import math\n",
    "\n",
    "sys.path.append(os.getcwd()+'/../cds-backend/code/')\n",
    "os.environ['PYTHONPATH'] = os.getcwd()+'/../cds-backend/code/'\n",
    "import cds_eua4 as eua\n",
    "import trajectory as trj\n",
    "from harvest_convert_to_netCDF import write_dict_h5\n",
    "\n",
    "import ray\n",
    "# ray.init(num_cpus=30)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aaf6595-a9c3-4bfe-b002-620a577a51e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ray.remote\n",
    "def write_trj(stat):\n",
    "    test_counter = 0\n",
    "    # try:\n",
    "    # check if output already exists:\n",
    "    targetfile = '/mnt/users/staff/uvoggenberger/scratch/converted_v11/trajectory_files_20230426/trajectory_'+str(stat.split('/')[-1])\n",
    "    checkfile = glob.glob(targetfile)\n",
    "    # if target file already exists\n",
    "    if len(checkfile) > 0:\n",
    "        # if input is older than target\n",
    "        if os.path.getmtime(stat) < os.path.getmtime(targetfile):\n",
    "            return 1\n",
    "\n",
    "\n",
    "\n",
    "    # read converted file and open it\n",
    "    file = eua.CDMDataset(filename = stat)\n",
    "\n",
    "    # check if all variables are available:\n",
    "    # if (not ('126' in file.recordindices.keys())) or (not ('139' in file.recordindices.keys())) or (not ('140' in file.recordindices.keys())):\n",
    "        # return 2\n",
    "\n",
    "#     try:\n",
    "#         igra_file = glob.glob('/scratch/das/federico/COP2_HARVEST_APRIL2022/igra2/*'+stat+'*')[0]\n",
    "#         i_file = eua.CDMDataset(filename = igra_file)\n",
    "#         igra_file_avail = True\n",
    "#     except:\n",
    "#         igra_file_avail = False\n",
    "#     print('igra_file_avail:', igra_file_avail)\n",
    "\n",
    "    # creating fillable output variables\n",
    "    try:\n",
    "        statlen = len(file.observations_table.observed_variable[:])\n",
    "        latd = np.full(statlen, np.nan)\n",
    "        lond = np.full(statlen, np.nan)\n",
    "        timed = np.full(statlen, np.nan)\n",
    "        ttime = np.full(statlen, np.nan)\n",
    "\n",
    "        slat = file.observations_table.latitude[0]\n",
    "        slon = file.observations_table.longitude[0]\n",
    "    except:\n",
    "        return 4\n",
    "    if ('126' in file.recordindices.keys()) and ('139' in file.recordindices.keys()) and ('140' in file.recordindices.keys()):\n",
    "        print('ascents: ', len(file.header_table.report_id[:])-1)\n",
    "        for i in range(len(file.header_table.report_id[:])-1):\n",
    "            var_recidx = {}\n",
    "            try:\n",
    "                for j in file.recordindices.keys():\n",
    "                    if j not in ['index', 'recordtimestamp']:\n",
    "                        var_recidx[j]=[file.recordindices[j][i], file.recordindices[j][i+1]]\n",
    "            except:\n",
    "                return 5\n",
    "\n",
    "            t_idx_s = var_recidx['126'][0]\n",
    "            t_idx_e = var_recidx['126'][1]\n",
    "            u_idx_s = var_recidx['139'][0]\n",
    "            u_idx_e = var_recidx['139'][1]\n",
    "            v_idx_s = var_recidx['140'][0]\n",
    "            v_idx_e = var_recidx['140'][1]\n",
    "\n",
    "            # -----------\n",
    "\n",
    "            if (u_idx_s == u_idx_e) or (u_idx_s == u_idx_e) or (t_idx_s == t_idx_e):\n",
    "                # replace with nan filling\n",
    "                continue\n",
    "\n",
    "            # check for z_coordinate_type\n",
    "            z_coordinate_type = file.observations_table.z_coordinate_type[t_idx_s:t_idx_e]\n",
    "            if len(np.where(z_coordinate_type == 1)[0]) < 3:\n",
    "                continue\n",
    "\n",
    "            # create variables needed for dataframe\n",
    "            repid = file.header_table.report_id[i]\n",
    "            u = file.observations_table.observation_value[u_idx_s:u_idx_e]\n",
    "            v = file.observations_table.observation_value[v_idx_s:v_idx_e]\n",
    "            t = file.observations_table.observation_value[t_idx_s:t_idx_e]\n",
    "            z_coordinate_t = file.observations_table.z_coordinate[t_idx_s:t_idx_e]\n",
    "            z_coordinate_u = file.observations_table.z_coordinate[u_idx_s:u_idx_e]\n",
    "            z_coordinate_v = file.observations_table.z_coordinate[v_idx_s:v_idx_e]\n",
    "\n",
    "            # check for right z_coordinate sorting\n",
    "            # if z_coordinate_t[0] < z_coordinate_t[-1]:\n",
    "            #     continue\n",
    "            checksorting = (lambda zc: np.all(zc[:-1] >= zc[1:]))\n",
    "            if not checksorting(np.array(z_coordinate_t)):\n",
    "                continue\n",
    "\n",
    "            # find shortest array\n",
    "            z_coords = [z_coordinate_t, z_coordinate_u, z_coordinate_v]\n",
    "            shortest_zc = 0\n",
    "\n",
    "            for k in range(len(z_coords)-1):\n",
    "                if len(z_coords[k+1]) < len(z_coords[shortest_zc]):\n",
    "                    shortest_zc = k+1\n",
    "\n",
    "            # if shortest array < 3 -> skip\n",
    "            if len(z_coords[shortest_zc]) < 3:\n",
    "                # replace with nan filling\n",
    "                continue\n",
    "\n",
    "            # -----------\n",
    "\n",
    "            # check if it's neccessary to interpolate\n",
    "            check = False\n",
    "            if (len(np.array(z_coordinate_t)) == len(np.array(z_coordinate_u))):\n",
    "                if not (np.array(z_coordinate_t) == np.array(z_coordinate_u)).all():\n",
    "                    check = True\n",
    "            else:\n",
    "                check = True\n",
    "\n",
    "            if check:\n",
    "                u_new = []\n",
    "                v_new = []\n",
    "\n",
    "                for k in range(len(z_coordinate_t)):\n",
    "                    if z_coordinate_u[0] <= z_coordinate_t[k] <= z_coordinate_u[-1]:\n",
    "                        u_new.append(np.interp(z_coordinate_t[k], z_coordinate_u, u))\n",
    "                        v_new.append(np.interp(z_coordinate_t[k], z_coordinate_v, v))\n",
    "                    else:\n",
    "                        u_new.append(np.nan)\n",
    "                        v_new.append(np.nan)\n",
    "\n",
    "                u = np.array(u_new)\n",
    "                v = np.array(v_new)\n",
    "\n",
    "            # done collecting data for calculation\n",
    "            input_df = pd.DataFrame({'t':t, 'u':u, 'v':v, 'p':z_coordinate_t, 'idx':np.array(range(t_idx_s, t_idx_e))})\n",
    "\n",
    "            # clean input array and check for outliers:\n",
    "            input_df.drop(input_df[input_df.t < 172].index, inplace=True)\n",
    "            input_df.drop(input_df[input_df.t > 372].index, inplace=True)\n",
    "            input_df.drop(input_df[input_df.u > 150].index, inplace=True)\n",
    "            input_df.drop(input_df[input_df.v > 150].index, inplace=True)\n",
    "\n",
    "            # flip for ascending order\n",
    "            input_df = input_df.dropna().iloc[::-1].reset_index()\n",
    "\n",
    "            # last check if not to little levels # 3 levels is minimum -> maybe more should be needed\n",
    "            # if changed - also change for shortest array!\n",
    "            if len(input_df) < 3:\n",
    "                continue\n",
    "\n",
    "            # check for lowest level pressure - is it too high above the ground?\n",
    "            # whats the lowest levels pressure in Pa?\n",
    "            p_lowest_level = input_df.p.iloc[0] # Pa\n",
    "            t_lowest_level = input_df.t.iloc[0] # K\n",
    "            # what is the acepted pressure range around that, for a given station height?\n",
    "            # delta_p = g/(R*T) * p_0 * delta_h\n",
    "            # delta_h = delta_p * R * T / (g * p_0)\n",
    "            R = 287 # J/(kg K)\n",
    "            g = 9.80 # m/sÂ²\n",
    "\n",
    "            # low_msl_pressure = 99000 # Pa\n",
    "            # mean_msl_pressure = 101300 # Pa\n",
    "            high_msl_pressure = 103000 # Pa\n",
    "\n",
    "            # z_low = (low_msl_pressure - p_lowest_level) * R * t_lowest_level / g / low_msl_pressure\n",
    "            # z_mean = (mean_msl_pressure - p_lowest_level) * R * t_lowest_level / g / mean_msl_pressure\n",
    "            z_high = (high_msl_pressure - p_lowest_level) * R * t_lowest_level / g / high_msl_pressure\n",
    "\n",
    "            # print('maximal height above ground', z_high, p_lowest_level)\n",
    "\n",
    "            station_z = file.observations_table.station_elevation[input_df.idx.iloc[0]]\n",
    "            # if the first observation is more than 1500 m above ground, the ascent is invalid (we check for high and low pressure)\n",
    "            if np.logical_and(((z_high - station_z) > 1500), ((z_high - station_z) > 1500)):\n",
    "                continue\n",
    "\n",
    "\n",
    "            # check if best possible time is selected # skipped for now\n",
    "            date_time = file.observations_table.date_time[t_idx_s] \n",
    "\n",
    "            # if dt format is needed convert with:\n",
    "            # dt_date = pd.to_datetime(date_time, unit='s', origin='1900-01-01')\n",
    "            '''\n",
    "            if not (file.era5fb.reportype[t_idx_s] == 16045):\n",
    "                if igra_file_avail:\n",
    "                    #if not already igra data -> select igra datetime\n",
    "                    if int(repid[0]) != 3:\n",
    "                        dups = file.header_table.duplicates[i]\n",
    "                        dups = dups[dups != b'']\n",
    "                        dups = dups[dups != b',']\n",
    "                        #iterate through all duplicates\n",
    "                        for j in range(0,int((len(dups)/11))):\n",
    "                            #if there is an igra duplicate:\n",
    "                            if int((dups[(j*11):((j+1)*11)])[0]) == 3:\n",
    "                                save_id = 0\n",
    "                                deci = 1\n",
    "                                a = ((dups[(j*11):((j+1)*11)]))[1:]\n",
    "                                for o in np.flip(a):\n",
    "                                    save_id += int(o)* deci\n",
    "                                    deci = deci*10\n",
    "                                date_time = i_file.recordtimestamp[save_id]  \n",
    "            '''\n",
    "\n",
    "            # calculate trajectory\n",
    "            phys_model = trj.trajectory(lat=slat, lon=slon, temperature=np.array(input_df.t), u=np.array(input_df.u), v=np.array(input_df.v), pressure=np.array(input_df.p))\n",
    "            if phys_model == (None, None, None, None, None):\n",
    "                continue\n",
    "\n",
    "            # helper knows where to write the data\n",
    "            helper = list(input_df.idx)\n",
    "\n",
    "            # filling output variables with calculated data\n",
    "            latd[helper] = np.array(phys_model[0])\n",
    "            lond[helper] = np.array(phys_model[1])\n",
    "            timed[helper] = np.array(phys_model[4])\n",
    "            ttime[helper] = np.array(phys_model[4])+date_time\n",
    "\n",
    "            # # stopper for some tests:\n",
    "            # if test_counter > 100:\n",
    "            #     break\n",
    "            # else:\n",
    "            #     test_counter += 1\n",
    "\n",
    "\n",
    "    file.close()\n",
    "\n",
    "\n",
    "    # writing to homogenisations only file\n",
    "\n",
    "\n",
    "    mode='w'\n",
    "    group = 'advanced_homogenisation'\n",
    "\n",
    "    i = 'latitude_displacement'\n",
    "    ov_vars = latd\n",
    "    alldict = pd.DataFrame({i:ov_vars})\n",
    "    write_dict_h5(targetfile, alldict, group, {i: { 'compression': 'gzip' } }, [i]) \n",
    "\n",
    "    i = 'longitude_displacement'\n",
    "    ov_vars = lond\n",
    "    alldict = pd.DataFrame({i:ov_vars})\n",
    "    write_dict_h5(targetfile, alldict, group, {i: { 'compression': 'gzip' } }, [i]) \n",
    "\n",
    "    i = 'time_since_launch'\n",
    "    ov_vars = timed\n",
    "    alldict = pd.DataFrame({i:ov_vars})\n",
    "    write_dict_h5(targetfile, alldict, group, {i: { 'compression': 'gzip' } }, [i]) \n",
    "\n",
    "    i = 'true_time'\n",
    "    ov_vars = ttime\n",
    "    alldict = pd.DataFrame({i:ov_vars})\n",
    "    write_dict_h5(targetfile, alldict, group, {i: { 'compression': 'gzip' } }, [i]) \n",
    "\n",
    "\n",
    "    # writing to input file\n",
    "\n",
    "#         try:\n",
    "#             mode='r+'\n",
    "#             group = 'advanced_homogenisation'\n",
    "\n",
    "#             i = 'latitude_displacement'\n",
    "#             ov_vars = latd\n",
    "#             alldict = pd.DataFrame({i:ov_vars})\n",
    "#             write_dict_h5(stat, alldict, group, {i: { 'compression': 'gzip' } }, [i]) \n",
    "\n",
    "#             i = 'longitude_displacement'\n",
    "#             ov_vars = lond\n",
    "#             alldict = pd.DataFrame({i:ov_vars})\n",
    "#             write_dict_h5(stat, alldict, group, {i: { 'compression': 'gzip' } }, [i]) \n",
    "\n",
    "#             i = 'time_since_launch'\n",
    "#             ov_vars = timed\n",
    "#             alldict = pd.DataFrame({i:ov_vars})\n",
    "#             write_dict_h5(stat, alldict, group, {i: { 'compression': 'gzip' } }, [i]) \n",
    "\n",
    "#             i = 'true_time'\n",
    "#             ov_vars = ttime\n",
    "#             alldict = pd.DataFrame({i:ov_vars})\n",
    "#             write_dict_h5(stat, alldict, group, {i: { 'compression': 'gzip' } }, [i]) \n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "\n",
    "\n",
    "    return 0 \n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     return stat\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "159c6aad-866a-4e8d-b887-ab26a4130719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/users/scratch/leo/scratch/converted_v11/long/0-20001-0-10739_CEUAS_merged_v1.nc\n",
      "ascents:  94491\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "file = glob.glob('/mnt/users/scratch/leo/scratch/converted_v11/long/*20001-0-10739*.nc')[0]\n",
    "print(file)\n",
    "results = write_trj(file)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8645dc-58d0-4422-a582-7e0c363b06b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
